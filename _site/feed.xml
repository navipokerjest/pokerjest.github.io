<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-08-07T14:34:35+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">wu-kan</title><subtitle>SYSU超算17级在读&lt;br/&gt;
水野爱&lt;br/&gt;
HPC&lt;br/&gt;
田宫例四驱车&lt;br/&gt;
&lt;a href=&quot;mailto:i@wu-kan.cn&quot;&gt;
  &lt;i class=&quot;fas fa-envelope&quot;&gt;&lt;/i&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/wu-kan&quot;&gt;
  &lt;i class=&quot;fab fa-github&quot;&gt;&lt;/i&gt;
&lt;/a&gt;
&lt;a href=&quot;https://codeforces.com/profile/WuK&quot;&gt;
  &lt;i class=&quot;fas fa-chart-bar&quot;&gt;&lt;/i&gt;
&lt;/a&gt;
&lt;a href=&quot;https://vjudge.net/user/WuK&quot;&gt;
  &lt;i class=&quot;fas fa-smile&quot;&gt;&lt;/i&gt;
&lt;/a&gt;
&lt;a href=&quot;https://www.zhihu.com/people/wu.kan/activities&quot;&gt;
  &lt;i class=&quot;fab fa-zhihu&quot;&gt;&lt;/i&gt;
&lt;/a&gt;
&lt;iframe
  src=&quot;https://music.163.com/outchain/player?type=0&amp;id=155059595&amp;auto=0&amp;height=32&quot;
  width=100%
  height=52
  frameborder=&quot;no&quot;
  border=&quot;0&quot;
  marginwidth=&quot;0&quot;
  marginheight=&quot;0&quot;
&gt;&lt;/iframe&gt;
</subtitle><entry><title type="html">我的ASC决赛复盘：冒险、失误与翻车</title><link href="http://localhost:4000/2021/05/19/%E6%88%91%E7%9A%84ASC%E5%86%B3%E8%B5%9B%E5%A4%8D%E7%9B%98-%E5%86%92%E9%99%A9-%E5%A4%B1%E8%AF%AF%E4%B8%8E%E7%BF%BB%E8%BD%A6/" rel="alternate" type="text/html" title="我的ASC决赛复盘：冒险、失误与翻车" /><published>2021-05-19T00:00:00+08:00</published><updated>2021-05-19T00:00:00+08:00</updated><id>http://localhost:4000/2021/05/19/%E6%88%91%E7%9A%84ASC%E5%86%B3%E8%B5%9B%E5%A4%8D%E7%9B%98:%E5%86%92%E9%99%A9%E3%80%81%E5%A4%B1%E8%AF%AF%E4%B8%8E%E7%BF%BB%E8%BD%A6</id><content type="html" xml:base="http://localhost:4000/2021/05/19/%E6%88%91%E7%9A%84ASC%E5%86%B3%E8%B5%9B%E5%A4%8D%E7%9B%98-%E5%86%92%E9%99%A9-%E5%A4%B1%E8%AF%AF%E4%B8%8E%E7%BF%BB%E8%BD%A6/">&lt;p&gt;忙完了毕业论文答辩之后，终于有时间能够静下心来写这样一篇文章。虽然比赛结果还行，但是回顾整个决赛过程，几乎没有一个环节可以说做得非常好的。&lt;/p&gt;

&lt;h2 id=&quot;5-月-8-日装机-day-1&quot;&gt;5 月 8 日，装机 Day 1&lt;/h2&gt;

&lt;p&gt;5 月 8 日一早，一行人抱着我们的设备进入赛场，包括了提前做好的几个版本的系统盘（Centos7.6、Centos8.0 以及 ASC19 决赛用过的系统盘镜像）、若干企业级 PCIE SSD、A100 显卡、插线版、螺丝刀及三盒搭配的刀头。八台浪潮 5280M6 已经摆放在我们的机架上，一切都蓄势待发。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/20/1tvAo2I9FK6sVLP.jpg&quot; alt=&quot;赛场.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们第一件事就是进 BIOS 看机器配置，但是迎面而来的&lt;a href=&quot;https://ark.intel.com/content/www/cn/zh/ark/products/212282/intel-xeon-platinum-8358-processor-48m-cache-2-60-ghz.html&quot;&gt;志强铂金 8358&lt;/a&gt;让我们立觉不妙（以往决赛提供的都是金牌处理器）。果然，官网上查到的信息印证了我们的直觉。我们现场算了一下，单 Socket 双精度算力接近 3Tflops，已经抵上小半张 V100 显卡了。于此同时，更为恐怖的是高达 250W 的 TDP，这意味着我们赛前设计的六机八卡方案几乎绝对不可行，即使使用四机八卡的配置也要疯狂压住功耗。&lt;/p&gt;

&lt;p&gt;打开机箱后的迎面而来的 raid 卡则更让我们心里一凉，这意味着我们提前准备的系统并不能直接插盘启动。虽然不抱希望，我们还是把一块装好了我们自己系统盘的硬盘托架插进了机器，经过了一翻折腾，果然启动不了。这时候接到赛会通知，我和轩轩要去后台领取机器配置表、硬件安装说明、团队应用赛题等一些东西。说明书上说这个服务器最多可以接三张显卡，但是赛会只提供了一个双卡的示例。&lt;/p&gt;

&lt;p&gt;等到我出来的时候，组委会开始清理场上人员，只要是戴了观摩队员牌子的人一律给你请到场下（之前并没有说装机环节观摩队不能在场上）。不过队员们似乎已经找到绕过 raid 卡进入系统的方法，赛前我们觉得最容易出现问题的 IB 驱动也很给力的起起来了，目前来看除了 raid 卡让我们吓出冷汗之外装机进度还算顺利，只留我们六个带着参赛队员牌的人完全没有问题。到中午的时候，我们起了六台机器，并在六号机上运行我们的&lt;a href=&quot;https://github.com/SYSU-SCC/softwares-all-in-one&quot;&gt;一键部署脚本&lt;/a&gt;；我就在边上改几台机器的路由和 IPMI 接口的 IP。&lt;/p&gt;

&lt;p&gt;截至目前为止，一切的一切还在按照我们赛前的时间线进行：中午之前起起来所有的机器并开始部署软件环境，下午就可以进行节点体检，顺利的话还能跑一些 PRESTO 和 QuEST 的测试。节点体检的结果显示，三号机四号机的 CPU 体质比一号机二号机好一些，不过在合理误差内，没有大问题。三点钟左右的时候跑完了&lt;a href=&quot;https://wu-kan.cn/_posts/2021-03-07-osu-micro-benchmarks/&quot;&gt;osu&lt;/a&gt;，我们的 IB 网也没有问题。终于轮到我来测一下 QuEST 赛题的情况：初赛 random 算例单卡 8.24s、单机双卡 72s、双机四卡 50s。由于来比赛之前已经预期 PCIe4.0 带宽远低于我初赛阶段用于优化的 NVLink，此时我并没有意识到这个运行时间有什么问题，就是觉得是因为跨 PCIe 通信开销过大。于是机时让给 PRESTO、AI 赛题负责的同学。尤其是 PRESTO，在这么强的 CPU 上的性能我们需要重新估计一下。&lt;/p&gt;

&lt;p&gt;五点半的时候，三号机四号机的显卡空了出来，我想试一下跨节点通信对我的多卡 QuEST 的影响，便在上面各起了一个进程，每个进程使用一张卡。测试的结果让人大跌眼镜：11s，这意味着跨节点通信比节点内通信还要快得多！我马上把这一发现和大家说了。终端输入一个 &lt;code&gt;nvidia-smi topo -m&lt;/code&gt;看一下，两张卡是接在不同 CPU 上的，我猜测跨 CPU 的 GPU P2P 通信走了内存中转。为验证这一现象不是节点本身的问题，我们把正在运行的 AI 赛题咔掉，在每个节点上都运行了程序，结果都是这样。&lt;/p&gt;

&lt;p&gt;怎么办？我们让家振去重测一下结点间的带宽和显卡的 P2P 带宽（用 NVIDIA 自己的 sample 代码），与此同时我把四号机拆了下来，重新安装两张显卡。不得不说，非赛会推荐的显卡安装真的非常难走线，要把显卡前面的横杆拆掉；一张显卡的电源线还要以一个非常妖娆的姿势从卡的下面扭上来（因为主板供电接口在这张卡的正下方）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/20/kU91NjAsuETCz7B.jpg&quot; alt=&quot;妖娆&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图，浪潮推荐的显卡安装方式是上面两个槽，如果我想装在图上两个红圈的地方，就要把原来上面的 IB 卡调整位置，而且还要注意显卡电源供电接口的位置。六点三十分，我改过显卡安装位置的四号机重新点亮，测试得 random 算例一机两卡 28s。看来跨 socket 可能是影响显卡 P2P 速率的原因，但是仍然比跨节点通信要慢，还有其他原因。&lt;/p&gt;

&lt;p&gt;六点四十，我们没有搞清楚节点内通信慢的原因（家振测试的结果也支持我的发现），不过又想了一种新的改装方案：既然我们已经把两张显卡挂在同一个 CPU 上了，不如再把 IB 网卡也挂在同一个 CPU 上，这样我们后面跑 Linpack 的时候就可以直接把另外一个 CPU 压到一个很低的功耗。但是，原先我对四号机的改装方法中是把两张显卡接在 CPU0 上，由于机箱的结构限制，没有办法再把 IB 网卡接在 CPU0 上。经过对这个机器主板和接线的仔细观察，我们发现中间一个 PCIe 扩展卡背后伸出四根很粗壮的线，分别走两边连到了两个 CPU Socket 附近，大胆猜测这四根线就是 PCIe 到 Socket 的连接线。换句话说，我们只要调整这四根线到 CPU 的连接位置，应该就可以按照我们想的方案一样，把两张显卡和 IB 网卡接到 CPU1 上了！但是此时遇到了新的问题：这四根线的长度是定制的，两长两短，刚好能够接到原先的 CPU 上；我们想改接的话原来的线不是过长就是过短了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/20/bFT9HEa3tPgXSOu.jpg&quot; alt=&quot;改装&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最后的解决方案是，从两个 CPU Socket 上的内存插槽中间走线（需要拆卸、调整一些内存条的位置）。即使找到了这样一个巧妙的位置，理线工作仍然是个地狱：既要找到一块足够大的地方放下过长的线，又不能挡住 CPU 的风道。七点二十分，我们重新改线的二号机点亮了。我们松了一口气，能点亮至少说明把四根线修改接口之后不会影响系统的基本功能。但是进了系统之后我们再次血压拉满：其中一张 A100 显卡工作在 PCIe x8 模式下，而非原先的 PCIe x16 模式！这说明要么我们接线没接紧，要么接线的插口还有什么别的限制，或是这个 CPU 本身就不能同时带动三个 x16 状态下的设备。&lt;/p&gt;

&lt;p&gt;我们再次把二号机从机架上拉出来，观察接线有没有什么别的问题。我们摇了一下我们的接线，应当都是接紧的；主板上这几个接口附近也没有什么明显的标识。我大着胆子把 CPU1 的两根接线交换了一下，七点四十分我们重新进入二号机的系统，显卡和网卡终于工作在正常的模式下，所有人都长喘出一口气，改装成功了！&lt;/p&gt;

&lt;p&gt;但是紧接着的一个事实让我们的血压再次拉满（这已经是今天不知道第多少次了）：八点钟赛会就要全场下电了。我们观察了一下全场的队伍，基本上这个时候都已经完成了对各自机器的组装和调整；入口处的一个学校似乎组了 1TB 内存的胖节点，他们拆下来的占位条堆成了一座小山。如果说我们在午餐前就点亮了所有机器的系统（很可能是场上进度最快的学校），那我们现在的处境很可能是全场最糟的：还有至少三台机器需要重新改装内部走线，并且节点间的带宽也需要重新测试；计划中原定的 HPL、HPCG、AI 赛题都没有测试。&lt;/p&gt;

&lt;p&gt;为了最大化节省明天白天的时间用于测试，我们决定在断电之后把所有机器的硬件改装做完。七点五十五分，我们把所有机器关机，并把二号机之外的其他机器机架上拆下来。八点整，我们的“星河姬”玉体横陈，也吸引了很多别的队伍围观；闻声赶来的赛会希望我们尽快离开，但是我们则希望能把机器装好（因为确实没有规则说下电之后不能调整机器或是留在赛场）。结果是，我们吃了一发赛会的警告，只好把衣衫不整的星河姬先抱到我们的座位后方，并把她的隐私部位——四张已经拆下来的 A100 显卡带走。&lt;/p&gt;

&lt;p&gt;总之第一天的结尾，我们落到一个非常狼狈的境地：所有的机器都需要重新装，很多赛题都没有测试，而且我们甚至还要手动把四张总价二十六万的显卡带回酒店。而且，晚上对机器的改装太急，所有的人都没有吃晚饭，还有几个队员没有搞清楚情况的。大家去金拱门匆匆点了些东西，然后带到我的房间，一边啃汉堡一边开总结会。&lt;/p&gt;

&lt;h2 id=&quot;5-月-9-日装机-day-2&quot;&gt;5 月 9 日，装机 Day 2&lt;/h2&gt;

&lt;p&gt;第二天一早，大家再次进入赛场，手上土豪金色的 A100 帮我们吸引了很多注意。但是我们已经顾不上太多，今天我们的压力比昨天还要大得多。&lt;/p&gt;

&lt;p&gt;由于我已经改了两台机器，对改装的流程比较熟悉，昨晚我们决定赛场上分成两组流水线作业：我带景润、正曦三个人改装一号机、三号机、四号机；轩轩带家振、炜乐先把软件盘重新装在昨天已经改好的二号机上运行一些测试，再把我们这边改好的机器上架检查。&lt;/p&gt;

&lt;p&gt;昨晚的骚动影响还没有褪去，有很多学校的人跑到我们这看我们在干什么，于是我们三个人把机器搬到了台后改装，不得不说是个很正确的决定。由于夜里已经在脑海中无数次演练过早上的装机环节，早上大家的配合非常流畅，十点钟的时候四台机器就已经全部改装完毕并成功进入系统。唯一有点失手的是在装三号机的时候，有两根线非常难走，以至于我和景润的手都被锋利的侧板划了口子（观摩队的同学带了创口贴，血也万幸没有流到主板上，我们把侧板上的血用纸仔细擦干净了）。轩轩那边也验证了，显卡的 PCIe 模式确实是因为接线顺序。&lt;/p&gt;

&lt;p&gt;终于可以喘口气，于是我跑去休息室吃了一些蛋糕，然后去看台上看别的队伍的情况（其他队员继续做昨天被打断的测试）。我们的正对面就是大清&lt;del&gt;中 门 对 狙&lt;/del&gt;，他们昨天下午就装好了机器，似乎已经做了很多的测试，感觉相当稳。他们也拆了几台机器下来，似乎在对 IB 网卡做什么调整（赛后和他们交流，说是在测试一台机器带三张显卡的可能性）。非常恐怖的是，他们装机这两天的功耗一直非常稳的压在 3000w 附近，这让我们猜测他们有什么“自动驾驶”的手段（赛后被否认了）。我们也关注了其他几个“传统强队”的情况，大部分都和我们一样打算使用四机八卡的配置。&lt;/p&gt;

&lt;p&gt;下午，我提前把最后的机器配置表交到赛会，似乎也是第一个交的学校。不过问题并不大，今年这颗铂金 CPU 下，应该也没有必要对自己队伍配置进行过多保密。交完配置表回来的几分钟，刚好队员们正在跑的一项测试跑完了，那我就来重新跑一下 QuEST 吧！跑的结果让我再次血压拉满，甚至有些无奈：一机两卡 random 算例在每个节点上都是 67s，退回到了最开始的水平！然而距离装机环节的结束只剩不到五个小时了，我们不能再拿后面的时间去冒险了；况且我们的 HPL 和 HPCG 都没有跑，即使找到了解决方案也必须优先腾出时间去试 HPL 和 HPCG 的最佳参数。我回到看台上，和其他人紧急讨论这个意外情况，其他队员继续去试 HPL 和 HPCG 的参数。最后决定，我们不再对机器硬件进行调整。我连回天河，希望通过软件上调整进程与显卡、节点的绑定，最大化避免节点内部两个进程的通信；光南则在我们在学校的另外两台 KNL 服务器做实验，看看是否能骗过 MPI，所有的通信都走 IB 网卡过一下。&lt;/p&gt;

&lt;p&gt;六点钟的时候，万能的群友不知道从哪里搞到了 5280M6 的文档，发现我们改过线的那张 PCIe 扩展卡上，还有两根很细的线，分别被标注成 CLK1 和 CLK2。难道我们把两张卡挂在 CPU1 上，却读了 CPU0 的时钟信号？想想不大可能，这么严重的问题应该完全点亮不了系统才对。但是我们来不及多想，马上把四号机关机，以最快的速度调整了这两个线的接口（因为此时离断电只剩不到两个小时了，昨晚的经历让我们不敢再去试赛会的底线）。四号机点亮之后结果还是一样，虽然同节点内部通信的问题没有改善，但是至少我们没有犯时钟接错的大问题。&lt;/p&gt;

&lt;p&gt;经历之前的风波之后，我们用于练习“开车”的时间已经非常有限。七点三十五分，我们终于完全测试好了第二天一早要跑的 HPL，预期结果在 78Tflops 附近。但是此时时间已经不够我们再去练习一次完整的 HPCG（需要大于 1800s），我们只好先跑一个十五分钟的。七点五十七分，我们算是勉强完成了所有赛题的测试，非常极限的在断电前关掉了我们的机器。&lt;/p&gt;

&lt;p&gt;&lt;del&gt;不过今天赛会管的显然不如昨天严，断电之后还有队伍在对机器大作调整，甚至把机架的侧盖整个拆了下来。说好的警告呢…&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;另外装机的两天我们全程拍了一个 vlog，待整理素材上传~&lt;/p&gt;

&lt;h2 id=&quot;5-月-10-日比赛-day-1&quot;&gt;5 月 10 日，比赛 Day 1&lt;/h2&gt;

&lt;p&gt;前一天晚上，赛会通知明天上场还是六个人。但是，赛前的 rulebook 上已经写了在正式比赛的两天，场上只能有五名队员，遇到问题的时候才能和 advisor 交流。于是我要对接的志愿者重新和赛会确认一下。一早传来消息，果然只能五个人。不过我们赛前已经对这种情况有预案，我的任务更多在于场上队员和场下以及留在超算中心的其他人之间的沟通。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;顺带提一下我的 advisor 身份。在年初提交决赛名单的时候正好原来的指导老师杜总离职了，我们和新的指导老师广爷和黄聃老师商量了一下，今年决赛还是由老队员带队去决赛现场；而作为赛队日常训练时名义上的队长，我做这次的 advisor。当然，也有一定的风险，正式比赛环节 advisor 和其他老队员很可能只能留在看台上。不过我自己倒是没有意见，毕竟&lt;a href=&quot;https://www.bilibili.com/bangumi/play/ep399475?t=934&quot;&gt;水野爱也有不上场的时候&lt;/a&gt;；况且我也已经大四了，让大三的同学多一些场上经验，对我们下一年的决赛也有好处（希望这里没有立下明年进不了决赛的 flag）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;早上八点钟，五个队员进入赛场，把所有机器的风扇功率开到最大，准备开跑 Linpack；我们其他几个老队员坐在正后方的看台上。白白写了一个非常有意思的 bot，从赛会官方的监控网页爬取各个队伍的功耗数据，并在爆功耗的时候推送到 Lark 上。&lt;/p&gt;

&lt;p&gt;&lt;del&gt;安利一下超算队内部沟通用的 Lark（飞书），是我们使用体验最好的协作工具。我在字节实习的时候就发现它和 Grafana 等监控软件联通很方便，同时飞书文档也比我们之前用的腾讯文档好用很多，可以直接插代码块，很适合赛场上大家同步进度；我们的答辩 ppt 也是在飞书文档上做的。&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/20/9NyKHWrB3EGis2Q.jpg&quot; alt=&quot;image _2_.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第一次跑 Linpack ，我们就很幸运地跑出了 79.04 Tflops 的好成绩，比我们前一天晚上的结果还要好一点，而且没有爆功耗。于是我们决定直接提交这次的结果，以便更早拿到其他赛题。手机也在疯狂推送其他学校爆功耗的消息，看来别的学校都已经开始跑 Linpack 了。我也顺手查了一下 Top500 榜单，我们的结果已经超过 2004 年世界第一的超算“蓝色基因”原型机了。&lt;/p&gt;

&lt;p&gt;按照之前的安排，拿到其他赛题后（按照规则，要提交 Linpack 的结果之后才可以拿到今天其他赛题的数据）先解压 PRESTO 赛题的数据，然后负责 PRESTO 赛题的轩轩研究决赛算例，并调整之前优化的代码；其他人跑 HPCG 和团队应用。拿到赛题之后，我们发现 PRESTO 赛题提供的从“中国天眼”上扒下来的数据光压缩包都有足足 53GB。我们已经猜到决赛会有很大的算例，不过这么大的还在我们意料之外；尤其是我们提前在 Centos 上装的解压工具 tar 还是单线程的，光解压就不知道要等到猴年马月。于是场上的同学先去跑 HPCG，看台上的同学去找有没有多线程的解压工具。最后经过一番 Google，我们现场学了 lbzip2 的用法。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/20/LQah3YZgb2F6Vu9.png&quot; alt=&quot;爆啦&quot; /&gt;&lt;/p&gt;

&lt;p&gt;十点多的时候，我们早已跑完 HPCG，但是手机上突然推送：Sun Yat-sen University 爆啦！这让在看台上的我们吓了一跳。原来是队员们在用 pip 安装团队应用的某个依赖的时候使用全核编译了，一不留神就爆了功耗。&lt;del&gt;250 瓦的 CPU，真的就是火龙啊&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;十一点四十的时候，赛委会那边开始有人走动，要公布早上跑 Linpack 的成绩了！因为今年决赛大家的机器配置都差不多，因此 Linpack 的排名还有很大悬念。一开始看到赛会写某个学校的成绩是 92Tflops，我们是场上的第二名，大家都高呼可惜。被超过这么多，我们猜测他们是组了四机十二卡的配置，单纯为了冲这一项的跑分来的（如果把显卡功耗压到 120w 的话确实有可能做到）。不过看那个学校的位置上完全看不出开心的迹象；也有别的学校同学去赛会那里去争论（以我在看台上的视角，几乎要吵起来了）。原来是小数点点错了，我们是第一！顿时微信群里老人家的红包横飞，我也激动地发了一条票圈。&lt;/p&gt;

&lt;p&gt;稍微冷静下来一些之后，我们也关注了别的学校的成绩。第二名大清跑出了接近 78Tflops 的成绩，我们实际上也只是险胜；这还是我们对机器改装、把其中一块 CPU 的功率从 250W 暴力压到了 60W（此时频率甚至只有 100+MHz）下的结果，看来他们还有着非常强的控功耗的技术。&lt;/p&gt;

&lt;p&gt;紧接着我又意识到，赛前的安排出了大问题：PRESTO 这边只有轩轩在负责，第一天剩下的压力全在轩轩身上了。赛前我们有向赛会发邮件咨询过 PRESTO 软件版本的问题，赛会这样回复：“关于 PRESTO 和 Python 版本没有限制，参赛队可以选择性能最优版本。”然而事实上最后能跑通的只有 PRESTO-3 版本，而我们之前的优化工作都是在 PRESTO-2 上做的，需要现场改很多代码。&lt;/p&gt;

&lt;p&gt;到了结束的时候，我们的 PRESTO 赛题跑了 10/12 个点，只完整处理完了第一组数据，已经是非常全力的发挥了。除了 HPL 之外，今天的成绩要明早才能公布。散场的时候，很多学校都来我们这边看我们的机器，想知道我们是不是对机器做了什么改造；不过我们的接线从外面也并看不出来，所以问题不大。&lt;/p&gt;

&lt;h2 id=&quot;5-月-11-日比赛-day-2&quot;&gt;5 月 11 日，比赛 Day 2&lt;/h2&gt;

&lt;p&gt;比赛第二天的任务非常重，个个都不是好啃的骨头。按照之前的计划，我们应该上来就把最吃显卡算力的 AI 赛题赛题跑掉；在这段时间，我和景润连回天河，分别把赛会提供的魔改版 QuEST（针对决赛增加了几个魔改的电路门）加进我们俩之前写的 GPU/CPU 版本；然后剩下的人 All in 神秘应用，毕竟 ASC19 的时候我们就是靠着神秘应用逆风翻盘的，平时的训练也主要针对神秘应用进行训练。&lt;/p&gt;

&lt;p&gt;然而比赛就是比赛，永远不会按照你预先写的剧本进行。一早我们拿到 AI 赛题的模型，发现赛会居然提供的是 ALBERT-xxlarge（这次决赛和以往不同，主办方说必须使用提供的 Dataset 和 Pretrained Model，导致决赛发挥余地有限。同时，赛会给的 Baseline 用的是 BERT-Large，我们下意识地以为现场也会是 BERT-Large，准备的时候就一直调试 BERT-Large 模型）。虽然之前也有准备 ALBERT 的代码，但是因为没怎么跑过，现场跑出来的结果就和瞎猜一样，现场 debug 也发现不了自己的训练脚本问题出在哪。&lt;/p&gt;

&lt;p&gt;QuEST 赛题这边的问题也一样。赛会在我问比赛版本的时候有意无意的回避了 QuEST 赛题的版本问题，一拿到代码果然是在 QuEST-3 上的魔改，然而初赛阶段却规定必须用 QuEST-2 去跑！不过我也有准备，提前把 QuEST-2 和 QuEST-3 的主要更新点搞明白了，赛场上我要做的就是把 QuEST-3 接口中的 Unitary 矩阵重新转换成 QuEST-2 中的存储格式，这样就能接入我写的基于 QuEST-2 的多 GPU 后端了（还要补全 QuEST-3 新增的一些 API），如果对这个软件很熟悉的话难度其实没有很高，主要是心要细，改的时候不能出问题。&lt;/p&gt;

&lt;p&gt;赛会提供的算例也是非常坑，一共有一个 34 量子比特的算例，三个 35 量子比特的算例，还有一个 22 量子比特的算例，但是有上万个量子门，搞人心态。赛前我看到一台机器的默认配置内存是 512GB，便断定赛会不会出超过 34 量子比特的算例，否则如果不做内存压缩的话连单节点和双节点的 baseline 都跑不了（需要 1TB 内存）！事实证明真的不能把赛会当成好人，他们真的就是抱着让所有人都跑不了的心态出的题。因此我做过内存压缩的 GPU 版本只能跑算例一（34 量子比特）和算例四（22 量子比特），剩下三个算例要丢到 CPU 上去跑。&lt;/p&gt;

&lt;p&gt;赛会也公布了前一天的得分情况，大清在除了 Linpack 的所有赛题上都拿到了第一，而我们在 Linpack 上也并没有和他们拉开差距，总体得分比他们少了五六分，恐怖如斯。&lt;/p&gt;

&lt;p&gt;上午十点钟，原以为工作量最大的 QuEST GPU 版本居然是我们今天第一次得分。由于之前测得节点内部通信的问题，我们的进程到节点是一个非常古怪的“12213443”的绑定方法，在这种绑定方法下大部分通信流量都是走 IB 网。算例一必须八卡去跑，跑了 33 秒；算例四则在两卡下能跑到 13 秒。CPU 版本那边则出了大问题：因为没有写内存压缩，我们必须在全部四个节点上才能跑 35 比特的算例。然而跑了十分钟我们的监控根本没有看到通信的流量！无奈我们只好掏出赛会的魔改代码，希望能跑 baseline，但是用 CPU 想跑出一个算例我估算也要大半个小时，场上队员还要保持精力去控制机器的功耗，一不留神就会爆功耗。&lt;/p&gt;

&lt;p&gt;我们现在就是非常后悔，如果我们能够提前把内存加到每个节点 768GB 内存（因为我们对机器的改装需要走内存条走线，不能把内存插满到 1TB），也不用像这样被动，每次跑都要全部节点了。神秘应用那边的环境遇到了非常大的困难，有的软件版本依赖过高也跑不了，过低也不行，我们只好每个人（在天河上）试不同的版本。&lt;/p&gt;

&lt;p&gt;到下午大概一点多的时候，我们绝望地发现，除了之前跑通的 QuEST 的两个算例，我们在所有赛题上都进入了绝境。我们决定直接跑 AI 赛题的 baseline，中间还要穿插编译神秘应用的依赖。然而赛会提供的 baseline 也有大大小小的问题，模型始终不收敛。&lt;/p&gt;

&lt;p&gt;一直到下午快四点，我们孤注一掷，决定只用三分之一的数据集训练十分钟，至少要有一个能交的结果。不过 ALBERT 模型还是非常强大的，这么搞居然也能在 Dev 数据集达到 80% 的准确度。&lt;/p&gt;

&lt;p&gt;中间轩轩去抽了个签，手很臭，居然抽到明天第一个答辩。鉴于场上队员已经濒临崩溃的心理状态，我们决定 AI 赛题之后去跑神秘应用，给大家一个喘息的时间（或者说放弃 QuEST 的 CPU 版本了）。结果证明我们再次犯了错。虽然我们成功跑起了神秘应用，但是从应用输出的 log 来看，在我们的机器上要跑完一个算例至少要十二个小时（赛后复盘的时候发现这个应用的瓶颈主要在 IO 上，而我们这次没有组 raid，机器的 IO 性能拉了，临时组内存盘也是杯水车薪）。五点多的时候终于能跑 QuEST，然而直到比赛结束我们也没能再跑出一个算例来。&lt;/p&gt;

&lt;p&gt;在看台上的压力已经如此紧迫，场上队员的压力更是不用多说。当比赛正式结束，我下到我们的机位时候，有的队员的眼睛都已经红了。所有的人都没吃中饭和晚饭，情绪也很低落，然而我们还要把所有的机器都复原，交给浪潮的人验收之后才能离场。&lt;/p&gt;

&lt;p&gt;拆机前我和“星河一号”最后拍了一张照，照片上完全看不出来当时的心境。不管怎样，属于我的 ASC 已经正式画上句号了（答辩环节我虽然可以在场但是也不能过多发言），而且我们至少还破了个记录，我还作为领队被采访上了一次推送，即使有遗憾，但也并不是毫无亮点。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/19/gyjD5wJ1bOt28rH.jpg&quot; alt=&quot;XingHe.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;大家把机器装得差不多的时候，我打起精神，跑去和别的学校社交，毕竟比赛就是比赛而已；我和北航的同学正好在同一个组实习，挺好的面基机会不要错过了。我们聊了各种赛题的情况，还介绍了我们对机器的详细改装方法。最后我也去大清那边问了他们 QuEST 赛题的情况，因为赛前就听说他们这题做的非常好，果然比我们快了三倍多。&lt;/p&gt;

&lt;h2 id=&quot;5-月-12-日答辩--颁奖&quot;&gt;5 月 12 日，答辩 + 颁奖&lt;/h2&gt;

&lt;p&gt;由于前一天轩轩手特别臭，我们很不幸的抽中了一号签去答辩。我们规定的答辩时间是七点四十五，而按照比赛的规则，PPT 需要提前四十五分钟送到会场。六点四十分，大家的早饭稀稀拉拉还没有吃完，我就先坐上了去会场的小巴，把我们的 PPT 先送过去。队员们上了七点十五的那一班车，七点二十五的时候人就都齐了。七点四十分，我们六个人进入答辩室。&lt;/p&gt;

&lt;p&gt;&lt;del&gt;再次吐槽一下让人有点晕的规则：装机的时候可以六个人，正式比赛只能五个人，答辩又要六个人&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;刚进到答辩室，就看到屏幕上的专家列表里赫然有 Jack Dongarra 老人家。这让我超级激动，因为我的&lt;a href=&quot;https://wu-kan.cn/_posts/2021-03-14-HPL-AI/&quot;&gt;毕业设计&lt;/a&gt;就是在做 Linpack 相关的工作，其中有相当大的工作都引用了这位大佬的！我拿出手机，拍下了这一（对我来说）历史性的一幕。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/19/OZYBolwK9h8jQxG.jpg&quot; alt=&quot;jack&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因为已经和黄聃老师一起做过预演，大家很顺溜的完成了英文展示和答辩。我还记得 Jack 问我们，“Did you modified the HPL codes?”轩轩回答：“No, we used the binary from NVIDIA.”展示和问问题的时间都恰到好处，不过结束离场的时候有点点小尴尬，我就拍了一两下手，然后现场的专家们就和我们一起鼓掌，答辩室内充满了快活的空气。&lt;/p&gt;

&lt;p&gt;出了答辩室，意味着长达五天的赛程终于画上终点了。所有的人都疲惫不堪（身体上、精神上），大家都回酒店，补觉到中午十二点，然后一起在海底捞大吃一顿。工作日海底捞人不多，于是我们享受了非常周到的服务。&lt;/p&gt;

&lt;p&gt;颁奖典礼在下午三点开始，两点五十分的时候对接的志愿者把领奖的顺序发给了我们。我们竟然是一等奖最后一个上台领奖的，那么说明我们竟然拿了第三！本来大家已经很悲观，觉得今年表现这么差，搞不好要击穿中大参赛历史（12 年~17 年都是第四，18 年第六，19 年第三）的下限了，没想到居然追平了去年的纪录。一等奖名单里还缺了大清和坐在我们前面的暨大，看来最后的赢家就要在他们之间产生了。昨天和大清那边交流过，他们赛场上放弃了整道 AI 题，因此虽然大清整体实力非常强，但是也不是无懈可击的。不过怎么说呢，虽然我们也被宣传成“传统强队”，但真正的强队有且只有一支，中大只能算个“奖杯守门员”罢了！&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/20/TqoOzEWlj4ws5nu.jpg&quot; alt=&quot;守门员&quot; /&gt;&lt;/p&gt;

&lt;p&gt;终于开始颁奖典礼，主持人介绍到场嘉宾的时候，我们发现卢总竟然也来了颁奖仪式；到我们上台领奖的时候，卢总还和我招了一下手（上台的同学里只有我之前见过卢总）。终于轮到冠亚军的归属，主持人故意卖了个关子，然后才报出亚军是清华。全场掌声雷动，不过很大一部分都是给我们前面的暨大的同学的。当然，有一说一，清华的实力还是非常强，虽然我们的名次只差了一名，赛场上还是能够明显感受到两边的差距（&lt;del&gt;甚至把我们的队员虐哭了&lt;/del&gt;），并且他们在除了 Linpack 和 AI 之外的所有题目都拿到了第一，恐怖如斯！&lt;/p&gt;

&lt;p&gt;亚军的颁奖是卢总来的，卢总的咖位很高，是我们太菜了（笑）。不过赛后我们也和卢总一起拍了合照。再之后，我们作为 Linpack 奖得主，和冠亚军一起接受“东方媒体”的采访。作为队里公认会“说话”的人，我再次被大家推出来。除了媒体群访之外，还有一个专访。记得被问了这样一个问题，认为我们国家超算领域处于什么现状，我们未来要为超算领域怎么样？我回答问题的时候满脑子群友的格局表情包，其结果就是站在国家角度发表了一通讲话。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/20/jJAnYdLXWy9KFvO.jpg&quot; alt=&quot;格局&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;del&gt;赛后我们很光荣的上了新闻联播，但幸好这一段被剪掉了，不然美国对广州超算中心的制裁估计永远不会结束了（笑）&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/20/GVZWMgtsQm53yCh.png&quot; alt=&quot;离谱&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;del&gt;特朗普或者拜登不看深圳卫视的吧&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;del&gt;奖也写错了&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;晚上我们和暨大的队员们一起在“南山必胜客”（深圳南山区的一家比萨 🍕 店）约饭，顺便交流各个赛题的情况。暨大这次的表现真的是非常稳，所有赛题都认认真真去跑了，QuEST 即使是 CPU 版本也跑了很多算例；而且他们也不像我们一样有经验丰富的老队员带队。我们都心服口服。&lt;/p&gt;

&lt;h2 id=&quot;后记&quot;&gt;后记&lt;/h2&gt;

&lt;p&gt;再次回看一下整场比赛，我们无论是从赛题优化、预先准备还是现场发挥都出现了很严重的问题。今年是我们离奖杯最近的一年，也是我们离奖杯最远的一年；期待以后的队员能满怀敬意地对奖杯发起挑战，让我能看到中大捧杯的那一天。最后还是以&lt;a href=&quot;https://www.bilibili.com/bangumi/play/ep399475?t=934&quot;&gt;爱酱不上场的这首歌&lt;/a&gt;结尾一下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;to be brave そう胸の flame&lt;/em&gt;&lt;/p&gt;

  &lt;p&gt;眼前有剑 心中有火焰&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;后后记&quot;&gt;后后记&lt;/h2&gt;

&lt;p&gt;今天（5 月 19 日）我们收到了大清快递过来的礼物（老队员前几届比赛就已经认识了），是他们设计的一些贴纸&lt;del&gt;到处都是梗&lt;/del&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/19/DRnAyoFMe9jXJlk.jpg&quot; alt=&quot;tuna&quot; /&gt;&lt;/p&gt;

&lt;p&gt;还有一个大家非常喜欢的、印了很多常用终端指令的方块；下一届新队员面试的时候就直接对着这个问就好了 hh。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/19/r8HOkyhuTAs1EiG.jpg&quot; alt=&quot;cube&quot; /&gt;&lt;/p&gt;

&lt;p&gt;非常感谢~&lt;/p&gt;</content><author><name></name></author><category term="ASC" /><summary type="html">忙完了毕业论文答辩之后，终于有时间能够静下心来写这样一篇文章。虽然比赛结果还行，但是回顾整个决赛过程，几乎没有一个环节可以说做得非常好的。</summary></entry><entry><title type="html">量子线路模拟器QuEST在多GPU平台上的性能优化</title><link href="http://localhost:4000/2021/05/11/%E9%87%8F%E5%AD%90%E7%BA%BF%E8%B7%AF%E6%A8%A1%E6%8B%9F%E5%99%A8QuEST%E5%9C%A8%E5%A4%9AGPU%E5%B9%B3%E5%8F%B0%E4%B8%8A%E7%9A%84%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/" rel="alternate" type="text/html" title="量子线路模拟器QuEST在多GPU平台上的性能优化" /><published>2021-05-11T00:00:00+08:00</published><updated>2021-05-11T00:00:00+08:00</updated><id>http://localhost:4000/2021/05/11/%E9%87%8F%E5%AD%90%E7%BA%BF%E8%B7%AF%E6%A8%A1%E6%8B%9F%E5%99%A8QuEST%E5%9C%A8%E5%A4%9AGPU%E5%B9%B3%E5%8F%B0%E4%B8%8A%E7%9A%84%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96</id><content type="html" xml:base="http://localhost:4000/2021/05/11/%E9%87%8F%E5%AD%90%E7%BA%BF%E8%B7%AF%E6%A8%A1%E6%8B%9F%E5%99%A8QuEST%E5%9C%A8%E5%A4%9AGPU%E5%B9%B3%E5%8F%B0%E4%B8%8A%E7%9A%84%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/">&lt;p&gt;&lt;a href=&quot;https://github.com/SYSU-SCC/SYSuEST&quot;&gt;代码地址&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;自己负责的 ASC20 的赛题的 GPU 部分，不过最后比赛表现不是很理想。随便写点东西吧。&lt;/p&gt;

&lt;h2 id=&quot;实现工作简述&quot;&gt;实现工作简述&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;量子电路优化器
    &lt;ul&gt;
      &lt;li&gt;将 targetQubit 相同或小于 12 的连续电路合并&lt;/li&gt;
      &lt;li&gt;合并概率计算&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;使用两个通用门代替其他门&lt;/li&gt;
  &lt;li&gt;基于 CUDA-aware MPI 的多卡实现&lt;/li&gt;
  &lt;li&gt;使用 15 个 stream 和 16 个显存 Buffer 实现计算和通信重叠&lt;/li&gt;
  &lt;li&gt;手写的显存分配器
    &lt;ul&gt;
      &lt;li&gt;预先申请 92.5%的显存栈&lt;/li&gt;
      &lt;li&gt;运行时直接移动栈顶指针分配所需空间&lt;/li&gt;
      &lt;li&gt;运行 34 量子比特时相较于原版 QuEST 多进程实现节省 200+GB 空间&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;优化 GPU 利用效率
    &lt;ul&gt;
      &lt;li&gt;使用 shared memory 存 targetQubit &amp;lt; 12 的电路&lt;/li&gt;
      &lt;li&gt;使用 constant memory 存合并的电路参数（可以保存约 600 个）&lt;/li&gt;
      &lt;li&gt;使用 cuBLAS 代替原来的向量规约和点积操作&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;决赛结果&quot;&gt;决赛结果&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;GPU(s)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;4&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;8&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;main_HamExp&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15.130841s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;13.011804s&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18.062945s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15.508143s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;main_InvQFT&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;33.724309s&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;random&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8.241649s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10.510134s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13.1650710s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9.797779s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GHZ&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.148370s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.057206s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.641490s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.313547s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GHZ_QFT_N(N=29)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.560380s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.545665s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.840117s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.653872s&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;被大清和北航按着锤了，他们 &lt;code&gt;main_InvQFT&lt;/code&gt; 算例在四机八卡上分别 9s 和 22s…他们的显存分配比我写的更好，我这边只跑出其中两个算例，他们几乎全部跑完了。和北航的同学交流了一下，显存不够的时候怎么办？对方回答：拉回内存上呀…也去问了一下大清的队长，他们是直接重构了 QuEST 中状态向量的分布。华科的同学说大清把通信改成多对多了，我想了想很有道理，因为我这边确实很多进程之间是没有通信的，把通信流量均分成 AlltoAll 可能确实更高效。&lt;/p&gt;

&lt;p&gt;感觉自己优化的时候老是局限于从某些细节扣出零点几秒，还非常缺乏直接从算法层面重构整个软件的能力。&lt;/p&gt;

&lt;p&gt;&lt;del&gt;辛 亥 革 命 失 败&lt;/del&gt;&lt;/p&gt;

&lt;h3 id=&quot;运行环境&quot;&gt;运行环境&lt;/h3&gt;

&lt;p&gt;我们的决赛平台“星河一号”包含四台浪潮 5280M6 服务器，每个节点上有两张 NVIDIA A100 PCIe。我们发现同一个节点间显卡 P2P 通信速率远低于跨节点的两张 GPU 通信速率，为此使用 NVIDIA 提供的 CUDA P2P Sample 测试的结果支持这一观点。慢的比例远大于 PCIe4.0 和 NVLink 之间的差距，猜测这里显卡的 P2P 走了内存。为此我们调整了机器走线，使得两张显卡均挂在一个 CPU 上，使结果快了一倍，但是仍远低于跨节点速率。&lt;/p&gt;

&lt;p&gt;因此，我们运行时按这样分配进程到节点的映射：12213443，使得节点内的两个进程在算法上尽量不要有通信。然而，我们的代码在决赛机器上的八卡性能仍然略低于单卡性能。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ nvidia-smi
Tue May 11 12:08:26 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  A100-PCIE-40GB      On   | 00000000:CA:00.0 Off |                    0 |
| N/A   30C    P0    36W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  A100-PCIE-40GB      On   | 00000000:E3:00.0 Off |                    0 |
| N/A   31C    P0    36W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
$ nvidia-smi topo -m
        GPU0    GPU1    mlx5_0  CPU Affinity    NUMA Affinity
GPU0     X      NODE    NODE    32-63   1
GPU1    NODE     X      NODE    32-63   1
mlx5_0  NODE    NODE     X

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;初赛结果&quot;&gt;初赛结果&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;GPU(s)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;4&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;random&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9.62s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;7.38s&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GHZ&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.98s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;0.79s&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GHZ_QFT_N(N=29)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.75s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.47s&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.38s&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;实际上我们在提交初赛成绩的时候有所保留（以防被赛会过度宣传），只交了一份去年的代码，只写了多机多卡，没有做其他优化。random 算例和 GHZ 算例分别 8.53s、7.45s。很显然这么做的并不止我们，而且别人保留的更多…&lt;/p&gt;

&lt;h3 id=&quot;运行环境-1&quot;&gt;运行环境&lt;/h3&gt;

&lt;p&gt;在超算中心 TH-2K 的 V100 节点上运行的，节点上有四张 V100 16GB，用 NVLink 以一种很坑的方式连起来了，不同显卡间 NVLink 的数量并不一样多。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ nvidia-smi
Wed May  5 18:58:36 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:8A:00.0 Off |                    0 |
| N/A   39C    P0    56W / 300W |     10MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:8B:00.0 Off |                    0 |
| N/A   36C    P0    61W / 300W |     10MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:B3:00.0 Off |                    0 |
| N/A   36C    P0    62W / 300W |     10MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:B4:00.0 Off |                    0 |
| N/A   39C    P0    60W / 300W |     10MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
$ nvidia-smi topo -m
        GPU0    GPU1    GPU2    GPU3    mlx5_0  mlx5_1  mlx5_2  mlx5_3  CPU Affinity
GPU0     X      NV2     NV1     NODE    PIX     PIX     NODE    NODE    14-27
GPU1    NV2      X      NODE    NV2     PIX     PIX     NODE    NODE    14-27
GPU2    NV1     NODE     X      NV2     NODE    NODE    PIX     PIX     14-27
GPU3    NODE    NV2     NV2      X      NODE    NODE    PIX     PIX     14-27
mlx5_0  PIX     PIX     NODE    NODE     X      PIX     NODE    NODE
mlx5_1  PIX     PIX     NODE    NODE    PIX      X      NODE    NODE
mlx5_2  NODE    NODE    PIX     PIX     NODE    NODE     X      PIX
mlx5_3  NODE    NODE    PIX     PIX     NODE    NODE    PIX      X

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing a single PCIe switch
  NV#  = Connection traversing a bonded set of # NVLinks
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;彩蛋&quot;&gt;彩蛋&lt;/h2&gt;

&lt;p&gt;只改动&lt;code&gt;statevec_pauliYKernel&lt;/code&gt;中的两行代码后（见&lt;code&gt;statevec_pauliYKernel_WuK&lt;/code&gt;），random 算例在单张 v100 32G 上就快了 0.8s。&lt;/p&gt;

&lt;p&gt;不过很可惜，再后来重构的代码中我把所有的量子门使用两个通用门表示了，这个小彩蛋并没有用上。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;// nvcc pauliYKernel.cu -ptx -arch=compute_70 -code=sm_70
typedef double qreal;

typedef struct
{

    char *buffer;   // generated QASM string
    int bufferSize; // maximum number of chars before overflow
    int bufferFill; // number of chars currently in buffer
    int isLogging;  // whether gates are being added to buffer

} QASMLogger;

/** Represents an array of complex numbers grouped into an array of
 * real components and an array of coressponding complex components.
 *
 * @ingroup type
 * @author Ania Brown
 */
typedef struct ComplexArray
{
    qreal *real;
    qreal *imag;
} ComplexArray;

/// \endcond

/** Represents a system of qubits.
 * Qubits are zero-based
 *
 * @ingroup type
 * @author Ania Brown
 * @author Tyson Jones (density matrix)
 */
typedef struct Qureg
{
    //! Whether this instance is a density-state representation
    int isDensityMatrix;
    //! The number of qubits represented in either the state-vector or density matrix
    int numQubitsRepresented;
    //! Number of qubits in the state-vector - this is double the number represented for mixed states
    int numQubitsInStateVec;
    //! Number of probability amplitudes held in stateVec by this process
    //! In the non-MPI version, this is the total number of amplitudes
    long long int numAmpsPerChunk;
    //! Total number of amplitudes, which are possibly distributed among machines
    long long int numAmpsTotal;
    //! The position of the chunk of the state vector held by this process in the full state vector
    int chunkId;
    //! Number of chunks the state vector is broken up into -- the number of MPI processes used
    int numChunks;

    //! Computational state amplitudes - a subset thereof in the MPI version
    ComplexArray stateVec;
    //! Temporary storage for a chunk of the state vector received from another process in the MPI version
    ComplexArray pairStateVec;
    ComplexArray pairDeviceStateVec;

    //! Storage for wavefunction amplitudes in the GPU version
    ComplexArray deviceStateVec;
    //! Storage for reduction of probabilities on GPU
    qreal *firstLevelReduction, *secondLevelReduction;
    int *lastPairRank;
    void *deviceStateVecRealHandle, *deviceStateVecImagHandle, *pairDeviceStateVecRealHandle, *pairDeviceStateVecImagHandle;
    //! Storage for generated QASM output
    QASMLogger *qasmLog;

} Qureg;
__global__ void statevec_pauliYKernel(Qureg qureg, const int targetQubit, const int conjFac)
{

    long long int sizeHalfBlock = 1LL &amp;lt;&amp;lt; targetQubit;
    long long int sizeBlock = 2LL * sizeHalfBlock;
    long long int numTasks = qureg.numAmpsPerChunk &amp;gt;&amp;gt; 1;
    long long int thisTask = blockIdx.x * blockDim.x + threadIdx.x;
    if (thisTask &amp;gt;= numTasks)
        return;

    long long int thisBlock = thisTask / sizeHalfBlock;
    long long int indexUp = thisBlock * sizeBlock + thisTask % sizeHalfBlock;
    long long int indexLo = indexUp + sizeHalfBlock;
    qreal stateRealUp, stateImagUp;

    qreal *stateVecReal = qureg.deviceStateVec.real;
    qreal *stateVecImag = qureg.deviceStateVec.imag;
    stateRealUp = stateVecReal[indexUp];
    stateImagUp = stateVecImag[indexUp];

    // update under +-{{0, -i}, {i, 0}}
    stateVecReal[indexUp] = conjFac * stateVecImag[indexLo];
    stateVecImag[indexUp] = conjFac * -stateVecReal[indexLo];
    stateVecReal[indexLo] = conjFac * -stateImagUp;
    stateVecImag[indexLo] = conjFac * stateRealUp;
}
__global__ void statevec_pauliYKernel_WuK(Qureg qureg, const int targetQubit, const int conjFac)
{

    long long int sizeHalfBlock = 1LL &amp;lt;&amp;lt; targetQubit;
    long long int sizeBlock = 2LL * sizeHalfBlock;
    long long int numTasks = qureg.numAmpsPerChunk &amp;gt;&amp;gt; 1;
    long long int thisTask = blockIdx.x * blockDim.x + threadIdx.x;
    if (thisTask &amp;gt;= numTasks)
        return;

    long long int thisBlock = thisTask / sizeHalfBlock;
    long long int indexUp = thisBlock * sizeBlock + thisTask % sizeHalfBlock;
    long long int indexLo = indexUp + sizeHalfBlock;
    qreal stateRealUp, stateImagUp;

    qreal *stateVecReal = qureg.deviceStateVec.real;
    qreal *stateVecImag = qureg.deviceStateVec.imag;
    stateRealUp = stateVecReal[indexUp];
    stateImagUp = stateVecImag[indexUp];

    const qreal
        stateRealLo = stateVecReal[indexLo],
        stateImagLo = stateVecImag[indexLo];

    // update under +-{{0, -i}, {i, 0}}
    stateVecReal[indexUp] = conjFac * stateImagLo;
    stateVecImag[indexUp] = conjFac * -stateRealLo;
    stateVecReal[indexLo] = conjFac * -stateImagUp;
    stateVecImag[indexLo] = conjFac * stateRealUp;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;终端运行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;nvcc pauliYKernel.cu -ptx -arch=compute_70 -code=sm_70
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;得到对应的 ptx 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-26907403
// Cuda compilation tools, release 10.1, V10.1.243
// Based on LLVM 3.4svn
//

.version 6.4
.target sm_70
.address_size 64

	// .globl	_Z21statevec_pauliYKernel5Quregii

.visible .entry _Z21statevec_pauliYKernel5Quregii(
	.param .align 8 .b8 _Z21statevec_pauliYKernel5Quregii_param_0[168],
	.param .u32 _Z21statevec_pauliYKernel5Quregii_param_1,
	.param .u32 _Z21statevec_pauliYKernel5Quregii_param_2
)
{
	.reg .pred 	%p&amp;lt;3&amp;gt;;
	.reg .b32 	%r&amp;lt;11&amp;gt;;
	.reg .f64 	%fd&amp;lt;12&amp;gt;;
	.reg .b64 	%rd&amp;lt;29&amp;gt;;


	mov.b64	%rd8, _Z21statevec_pauliYKernel5Quregii_param_0;
	ld.param.u32 	%r2, [_Z21statevec_pauliYKernel5Quregii_param_1];
	ld.param.u32 	%r1, [_Z21statevec_pauliYKernel5Quregii_param_2];
	mov.u64 	%rd3, %rd8;
	cvt.u64.u32	%rd1, %r2;
	mov.u64 	%rd9, 1;
	shl.b64 	%rd2, %rd9, %r2;
	ld.param.u64 	%rd10, [_Z21statevec_pauliYKernel5Quregii_param_0+16];
	shr.s64 	%rd11, %rd10, 1;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	cvt.u64.u32	%rd4, %r6;
	setp.ge.s64	%p1, %rd4, %rd11;
	@%p1 bra 	BB0_5;

	and.b64  	%rd12, %rd2, -4294967296;
	setp.eq.s64	%p2, %rd12, 0;
	@%p2 bra 	BB0_3;

	rem.s64 	%rd28, %rd4, %rd2;
	bra.uni 	BB0_4;

BB0_3:
	cvt.u32.u64	%r7, %rd2;
	cvt.u32.u64	%r8, %rd4;
	rem.u32 	%r9, %r8, %r7;
	cvt.u64.u32	%rd28, %r9;

BB0_4:
	cvt.u32.u64	%r10, %rd1;
	shr.u64 	%rd13, %rd4, %r10;
	mul.lo.s64 	%rd14, %rd2, %rd13;
	shl.b64 	%rd15, %rd14, 1;
	add.s64 	%rd16, %rd28, %rd15;
	add.s64 	%rd17, %rd16, %rd2;
	ld.param.u64 	%rd18, [%rd3+88];
	cvta.to.global.u64 	%rd19, %rd18;
	ld.param.u64 	%rd20, [%rd3+96];
	cvta.to.global.u64 	%rd21, %rd20;
	shl.b64 	%rd22, %rd16, 3;
	add.s64 	%rd23, %rd19, %rd22;
	ld.global.f64 	%fd1, [%rd23];
	add.s64 	%rd24, %rd21, %rd22;
	ld.global.f64 	%fd2, [%rd24];
	shl.b64 	%rd25, %rd17, 3;
	add.s64 	%rd26, %rd21, %rd25;
	ld.global.f64 	%fd3, [%rd26];
	cvt.rn.f64.s32	%fd4, %r1;
	mul.f64 	%fd5, %fd4, %fd3;
	st.global.f64 	[%rd23], %fd5;
	add.s64 	%rd27, %rd19, %rd25;
	ld.global.f64 	%fd6, [%rd27];
	mul.f64 	%fd7, %fd4, %fd6;
	neg.f64 	%fd8, %fd7;
	st.global.f64 	[%rd24], %fd8;
	mul.f64 	%fd9, %fd4, %fd2;
	neg.f64 	%fd10, %fd9;
	st.global.f64 	[%rd27], %fd10;
	mul.f64 	%fd11, %fd4, %fd1;
	st.global.f64 	[%rd26], %fd11;

BB0_5:
	ret;
}

	// .globl	_Z25statevec_pauliYKernel_WuK5Quregii
.visible .entry _Z25statevec_pauliYKernel_WuK5Quregii(
	.param .align 8 .b8 _Z25statevec_pauliYKernel_WuK5Quregii_param_0[168],
	.param .u32 _Z25statevec_pauliYKernel_WuK5Quregii_param_1,
	.param .u32 _Z25statevec_pauliYKernel_WuK5Quregii_param_2
)
{
	.reg .pred 	%p&amp;lt;3&amp;gt;;
	.reg .b32 	%r&amp;lt;11&amp;gt;;
	.reg .f64 	%fd&amp;lt;12&amp;gt;;
	.reg .b64 	%rd&amp;lt;29&amp;gt;;


	mov.b64	%rd8, _Z25statevec_pauliYKernel_WuK5Quregii_param_0;
	ld.param.u32 	%r2, [_Z25statevec_pauliYKernel_WuK5Quregii_param_1];
	ld.param.u32 	%r1, [_Z25statevec_pauliYKernel_WuK5Quregii_param_2];
	mov.u64 	%rd3, %rd8;
	cvt.u64.u32	%rd1, %r2;
	mov.u64 	%rd9, 1;
	shl.b64 	%rd2, %rd9, %r2;
	ld.param.u64 	%rd10, [_Z25statevec_pauliYKernel_WuK5Quregii_param_0+16];
	shr.s64 	%rd11, %rd10, 1;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r6, %r3, %r4, %r5;
	cvt.u64.u32	%rd4, %r6;
	setp.ge.s64	%p1, %rd4, %rd11;
	@%p1 bra 	BB1_5;

	and.b64  	%rd12, %rd2, -4294967296;
	setp.eq.s64	%p2, %rd12, 0;
	@%p2 bra 	BB1_3;

	rem.s64 	%rd28, %rd4, %rd2;
	bra.uni 	BB1_4;

BB1_3:
	cvt.u32.u64	%r7, %rd2;
	cvt.u32.u64	%r8, %rd4;
	rem.u32 	%r9, %r8, %r7;
	cvt.u64.u32	%rd28, %r9;

BB1_4:
	cvt.u32.u64	%r10, %rd1;
	shr.u64 	%rd13, %rd4, %r10;
	mul.lo.s64 	%rd14, %rd2, %rd13;
	shl.b64 	%rd15, %rd14, 1;
	add.s64 	%rd16, %rd28, %rd15;
	add.s64 	%rd17, %rd16, %rd2;
	ld.param.u64 	%rd18, [%rd3+88];
	cvta.to.global.u64 	%rd19, %rd18;
	ld.param.u64 	%rd20, [%rd3+96];
	cvta.to.global.u64 	%rd21, %rd20;
	shl.b64 	%rd22, %rd16, 3;
	add.s64 	%rd23, %rd19, %rd22;
	ld.global.f64 	%fd1, [%rd23];
	add.s64 	%rd24, %rd21, %rd22;
	ld.global.f64 	%fd2, [%rd24];
	shl.b64 	%rd25, %rd17, 3;
	add.s64 	%rd26, %rd19, %rd25;
	ld.global.f64 	%fd3, [%rd26];
	add.s64 	%rd27, %rd21, %rd25;
	cvt.rn.f64.s32	%fd4, %r1;
	ld.global.f64 	%fd5, [%rd27];
	mul.f64 	%fd6, %fd4, %fd5;
	st.global.f64 	[%rd23], %fd6;
	mul.f64 	%fd7, %fd4, %fd3;
	neg.f64 	%fd8, %fd7;
	st.global.f64 	[%rd24], %fd8;
	mul.f64 	%fd9, %fd4, %fd2;
	neg.f64 	%fd10, %fd9;
	st.global.f64 	[%rd26], %fd10;
	mul.f64 	%fd11, %fd4, %fd1;
	st.global.f64 	[%rd27], %fd11;

BB1_5:
	ret;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以发现，这样进行一下 naive 的修改之后，部分访存的代码由&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;	add.s64 	%rd26, %rd21, %rd25;
	ld.global.f64 	%fd3, [%rd26];
	cvt.rn.f64.s32	%fd4, %r1;
	mul.f64 	%fd5, %fd4, %fd3;
	st.global.f64 	[%rd23], %fd5;
	add.s64 	%rd27, %rd19, %rd25;
	ld.global.f64 	%fd6, [%rd27];
	mul.f64 	%fd7, %fd4, %fd6;
	neg.f64 	%fd8, %fd7;
	st.global.f64 	[%rd24], %fd8;
	mul.f64 	%fd9, %fd4, %fd2;
	neg.f64 	%fd10, %fd9;
	st.global.f64 	[%rd27], %fd10;
	mul.f64 	%fd11, %fd4, %fd1;
	st.global.f64 	[%rd26], %fd11;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;变成了&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;	add.s64 	%rd26, %rd19, %rd25;
	ld.global.f64 	%fd3, [%rd26];
	add.s64 	%rd27, %rd21, %rd25;
	cvt.rn.f64.s32	%fd4, %r1;
	ld.global.f64 	%fd5, [%rd27];
	mul.f64 	%fd6, %fd4, %fd5;
	st.global.f64 	[%rd23], %fd6;
	mul.f64 	%fd7, %fd4, %fd3;
	neg.f64 	%fd8, %fd7;
	st.global.f64 	[%rd24], %fd8;
	mul.f64 	%fd9, %fd4, %fd2;
	neg.f64 	%fd10, %fd9;
	st.global.f64 	[%rd26], %fd10;
	mul.f64 	%fd11, %fd4, %fd1;
	st.global.f64 	[%rd27], %fd11;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，核函数代码对 global 内存的读写顺序改变了，原先 ld.global 与 st.global 指令间存在一定的交错执行关系，经过调整之后统一先读后写，这使得其更容易在运行的时候合并访存。可以使用 nvprof 这个工具验证优化的有效性。statevec_pauliYKernel 在算例一中有 43 个 Calls，其运行时间的 Avg/Min/Max 分别为 63.251ms/57.993ms/84.206ms ，总运行时间占算例一总时间的 14.91%（2.71978s/18.688107s）。经过修改之后，其运行时间的 Avg/Min/Max 分别为 44.101ms/42.064ms/53.005ms ，总运行时间占算例一总时间的 10.87%（1.89634s/17.922671s）。从结果上来看，只改动了原 CUDA 代码的两行就给 GPU 单卡的代码带来了近 1s 的提速，还是非常有效的。&lt;/p&gt;

&lt;p&gt;发现这个问题的过程也很有参考意义：在 nvprof 得到的结果中，这个核函数运行时间的 Avg（63.251ms）远大于相同计算量的 statevec_pauliXKernel（43.516ms），且这个核函数的 Max 远大于 Avg，说明运行时间的波动较大，存在优化空间。在逐行比较 statevec_pauliYKernel 和 statevec_pauliXKernel 后，唯一有区别的几乎就是，原作者为了节省两个变量，直接将访存代码放进了计算的表达式中；调整之后果然取得了优化。最后通过类似的手段又在别的地方挤出 0.4s。这也提示我们一个比较优秀的写 CUDA 代码的习惯：尽量不要在同一个表达式中同时出现读写 global memory 的操作；同时对 global memory 的读写应该尽量分开，各自聚集在一起。&lt;/p&gt;</content><author><name></name></author><summary type="html">代码地址</summary></entry><entry><title type="html">华为昇腾910使用初探</title><link href="http://localhost:4000/2021/03/21/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE910%E4%BD%BF%E7%94%A8%E5%88%9D%E6%8E%A2/" rel="alternate" type="text/html" title="华为昇腾910使用初探" /><published>2021-03-21T00:00:00+08:00</published><updated>2021-03-21T00:00:00+08:00</updated><id>http://localhost:4000/2021/03/21/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE910%E4%BD%BF%E7%94%A8%E5%88%9D%E6%8E%A2</id><content type="html" xml:base="http://localhost:4000/2021/03/21/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE910%E4%BD%BF%E7%94%A8%E5%88%9D%E6%8E%A2/">&lt;p&gt;在本文中，我使用华为 &lt;a href=&quot;https://e.huawei.com/cn/products/cloud-computing-dc/atlas/atlas-800-training-9000&quot;&gt;Atlas 800-9000&lt;/a&gt; 训练服务器上的 &lt;a href=&quot;https://e.huawei.com/cn/products/cloud-computing-dc/atlas/ascend-910&quot;&gt;Ascend 910&lt;/a&gt; AI 加速卡上跑了一个 MatMul 算子，并对调用 AscendCL 接口的地方分别计时，从而大致了解使用 NPU 启动一次计算的流程和各部分耗时情况。&lt;/p&gt;

&lt;h2 id=&quot;实验环境&quot;&gt;实验环境&lt;/h2&gt;

&lt;p&gt;可以看到，机器的配置符合官网上的说明，单机有八张昇腾 910A 加速卡，而单卡空转待机功率接近七十瓦。Ascend 910 似乎有 A、B、Pro 等若干后缀小版本，而它们之间的差异似乎在软件层面是不可见的。&lt;/p&gt;

&lt;p&gt;此外在 npu 面板中可以看到，其中有两个占用：Memory-Usage 和 HBM-Usage。初次看到的时候让我迷惑了一下，后查阅文档得知，Atlas 系列的加速卡上本身就有运行一个 device OS，这使得它可以工作在 EP 模式和 RC 模式下。所谓 EP 模式和我们平时的 GPU 比较类似，由 host 机的 CPU 调度 AICore，Device 上的 CPU 几乎处于不可见状态；而 RC 模式下可以直接通过 Device 上的 AI CPU 启动程序，并且（从代码示例）来看 AI CPU 可以直接对 HBM 进行寻址，无需再走 PCI-E 总线，感觉和 nvidia 之前出的 DPU 有些像，大约可以应用于边缘计算的场景。因此 HBM 接近于我印象中的显存，而另一者就不是今天主要关注的对象了。&lt;/p&gt;

&lt;p&gt;使用自带的算力测试工具 &lt;a href=&quot;https://support.huawei.com/enterprise/zh/doc/EDOC1100164862/bc368639&quot;&gt;&lt;code&gt;ascend-dmi -f&lt;/code&gt;&lt;/a&gt;跑一下，单卡在默认参数下跑出 262 TFLOPS，相对于&lt;a href=&quot;https://e.huawei.com/cn/products/cloud-computing-dc/atlas/ascend-910&quot;&gt;官网&lt;/a&gt;上宣称的 320 TFLOPS 理论值来说可以接受。注意到此时功耗约 240W，离最大功耗 310W 还有一段距离，感觉还有提升的空间。&lt;/p&gt;

&lt;p&gt;服务器的 CPU 也值得单独提一下，共有四块鲲鹏 920，是（我）少见的用于服务器的 arm CPU。另外在官网上看到鲲鹏 920 似乎有 32/48/64 多个版本的配置，而我手上的是 48 核版。就使用体验来看很让我惊喜，arm 在服务端上也可以有 Intel Xeon Gold 同一量级的性能。不过在 &lt;code&gt;/proc/cpuinfo&lt;/code&gt; 中缺失了 CPU 型号相关的信息，有些可惜。&lt;/p&gt;

&lt;p&gt;最后这台服务器实际上是有其他人在用的，（小小吐槽一下）&lt;code&gt;/usr&lt;/code&gt;目录下的软件管理比较混乱，使用时经常报错。因此接下来除驱动外我统一使用自己安装在用户目录下的软件环境。CANN 我统一使用自己下载的 &lt;a href=&quot;https://www.huaweicloud.com/ascend/cann-download&quot;&gt;20.1.rc1_linux-aarch64 社区版&lt;/a&gt;，因为接下来使用到的部分接口（&lt;code&gt;aclopCompileAndExecute&lt;/code&gt;）似乎在新版本的文档中去掉了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ npu-smi info
+-------------------------------------------------------------------------------------------+
| npu-smi 1.7.6                        Version: 20.1.0                                      |
+-------------------+-----------------+-----------------------------------------------------+
| NPU     Name      | Health          | Power(W)        Temp(C)                             |
| Chip              | Bus-Id          | AICore(%)       Memory-Usage(MB)     HBM-Usage(MB)  |
+===================+=================+=====================================================+
| 0       910A      | OK              | 68.4            40                                  |
| 0                 | 0000:C1:00.0    | 0               459  / 15307         0    / 32255   |
+===================+=================+=====================================================+
| 1       910A      | OK              | 65.5            35                                  |
| 0                 | 0000:81:00.0    | 0               784  / 15689         0    / 32255   |
+===================+=================+=====================================================+
| 2       910A      | OK              | 66.8            34                                  |
| 0                 | 0000:41:00.0    | 0               1098 / 15689         0    / 32255   |
+===================+=================+=====================================================+
| 3       910A      | OK              | 66.9            40                                  |
| 0                 | 0000:01:00.0    | 0               2496 / 15601         0    / 32255   |
+===================+=================+=====================================================+
| 4       910A      | OK              | 66.7            39                                  |
| 0                 | 0000:C2:00.0    | 0               153  / 15307         0    / 32255   |
+===================+=================+=====================================================+
| 5       910A      | OK              | 65.0            34                                  |
| 0                 | 0000:82:00.0    | 0               313  / 15689         0    / 32255   |
+===================+=================+=====================================================+
| 6       910A      | OK              | 67.5            34                                  |
| 0                 | 0000:42:00.0    | 0               784  / 15689         0    / 32255   |
+===================+=================+=====================================================+
| 7       910A      | OK              | 66.3            40                                  |
| 0                 | 0000:02:00.0    | 0               3276 / 15601         0    / 32255   |
+===================+=================+=====================================================+
$ ascend-dmi -f
-----------------------------------------------------------------------------------------
    Device        Execute Times        Duration(ms)        TFLOPS@FP16        Power(W)
-----------------------------------------------------------------------------------------
    0             192,000,000          1536                262.144            239.716
-----------------------------------------------------------------------------------------
$ cat /proc/cpuinfo | grep processor | wc -l
192
$ cat /proc/cpuinfo | head -n 10
processor       : 0
BogoMIPS        : 200.00
Features        : fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop
CPU implementer : 0x48
CPU architecture: 8
CPU variant     : 0x1
CPU part        : 0xd01
CPU revision    : 0

processor       : 1
$ cat /proc/meminfo
MemTotal:       803585472 kB
MemFree:        578641344 kB
MemAvailable:   704245568 kB
Buffers:            3136 kB
Cached:         206471232 kB
SwapCached:            0 kB
Active:         12392256 kB
Inactive:       194916672 kB
Active(anon):    3918336 kB
Inactive(anon):  1231232 kB
Active(file):    8473920 kB
Inactive(file): 193685440 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:      39063488 kB
SwapFree:       39063488 kB
Dirty:               384 kB
Writeback:             0 kB
AnonPages:        835008 kB
Mapped:           132096 kB
Shmem:           4315072 kB
Slab:            9605440 kB
SReclaimable:    6719360 kB
SUnreclaim:      2886080 kB
KernelStack:      138560 kB
PageTables:        17152 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    440856192 kB
Committed_AS:    5693952 kB
VmallocTotal:   133009637312 kB
VmallocUsed:           0 kB
VmallocChunk:          0 kB
HardwareCorrupted:     0 kB
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:     524288 kB
$ uname -a
Linux atlas01 4.14.0-115.el7a.0.1.aarch64 #1 SMP Sun Nov 25 20:54:21 UTC 2018 aarch64 aarch64 aarch64 GNU/Linux
$ spack debug report
* **Spack:** 0.16.1
* **Python:** 3.7.5
* **Platform:** linux-centos7-aarch64
$ spack unload -a
$ spack load gcc@7.5.0
$ spack find --loaded
==&amp;gt; 6 installed packages
-- linux-centos7-aarch64 / gcc@4.8.5 ----------------------------
gcc@7.5.0  gmp@6.1.2  isl@0.18  mpc@1.1.0  mpfr@3.1.6  zlib@1.2.11
$ which $CXX
~/spack-0.16.1/opt/spack/linux-centos7-aarch64/gcc-4.8.5/gcc-7.5.0-4euuqqu5srdpm6hxg4kuwhoyjc6crp2t/bin/g++
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;实验结果及分析&quot;&gt;实验结果及分析&lt;/h2&gt;

&lt;p&gt;几个简单总结：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;创建上下文、申请内存、释放内存、释放上下文、去初始化的开销都很大，需要尽量避免&lt;/li&gt;
  &lt;li&gt;算子首次编译需要 12s 左右的时间，后续运行无需重新编译&lt;/li&gt;
  &lt;li&gt;Memset 开销远低于 Malloc，应当尽量多复用内存&lt;/li&gt;
  &lt;li&gt;FP16MatMul 算子运行时间已经低于将矩阵从内存拷贝到设备的开销！&lt;/li&gt;
  &lt;li&gt;GEMM 算子不支持 FP32&lt;/li&gt;
  &lt;li&gt;GEMM 算子和使用 FP32 的 MatMul 算子性能很低，鸡肋&lt;/li&gt;
  &lt;li&gt;切换上下文开销很低
    &lt;ul&gt;
      &lt;li&gt;由于创建释放上下文的开销仍然很高，必要时仍要使用 stream&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;经过热身的 MatMul 算子可以在 &lt;code&gt;{m, n, k} == {8192, 8192, 8192}&lt;/code&gt; 问题规模的 FP16 矩阵乘法上跑出接近 200 TFLOPS 的算力，远优于同期发布的 &lt;a href=&quot;https://www.nvidia.cn/data-center/v100/&quot;&gt;NVIDIA V100S&lt;/a&gt; 只有 130TFLOPS 的 FP16 理论算力（使用 Tensor Core）。
    &lt;ul&gt;
      &lt;li&gt;当然，一年后发布的 &lt;a href=&quot;https://www.nvidia.cn/data-center/a100/&quot;&gt;NVIDIA A100&lt;/a&gt; 使用 Tensor Core 的理论 FP16 算力达到 312TFLOPS，已经处于同一水平了。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./MatMul 8192 8192 8192
aclInit: 4.422000 ms, 0.000000e+00 FLOPS
aclrtCreateContext: 351.026000 ms, 0.000000e+00 FLOPS
aclrtSetCurrentContext: 0.006000 ms, 0.000000e+00 FLOPS
aclrtMalloc: 198.336000 ms, 0.000000e+00 FLOPS
aclrtMallocHost: 26.796000 ms, 0.000000e+00 FLOPS
aclrtMemsetAsync: 77.729000 ms, 0.000000e+00 FLOPS
aclrtMemcpyAsyncHtoD: 36.260000 ms, 0.000000e+00 FLOPS
Cast(aclFloat16): 0.498000 ms, 0.000000e+00 FLOPS
Cast(float): 0.484000 ms, 0.000000e+00 FLOPS
MatMulV2(aclFloat16): 5.613000 ms, 1.958866e+14 FLOPS
MatMulV2(float): 1154.355000 ms, 9.524900e+11 FLOPS
MatMul(aclFloat16): 5.610000 ms, 1.959914e+14 FLOPS
MatMul(float): 1154.473000 ms, 9.523927e+11 FLOPS
GEMM(aclFloat16): 202.152000 ms, 5.439034e+12 FLOPS
aclrtMemcpyAsyncDtoH: 11.689000 ms, 0.000000e+00 FLOPS
aclrtFree: 59.107000 ms, 0.000000e+00 FLOPS
aclrtFreeHost: 64.095000 ms, 0.000000e+00 FLOPS
aclrtDestroyContext: 186.624000 ms, 0.000000e+00 FLOPS
aclFinalize: 349.656000 ms, 0.000000e+00 FLOPS
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;实验代码-matmulcc&quot;&gt;实验代码 &lt;code&gt;MatMul.cc&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;华为官方提供了一个 &lt;a href=&quot;https://support.huawei.com/enterprise/zh/doc/EDOC1100180745/efaaf049&quot;&gt;gemm 算子示例&lt;/a&gt;，提供了封装后的 GEMM 算子使用方法。但是我在实验中发现，这个封装的 &lt;code&gt;aclblasGemmEx&lt;/code&gt; 接口仍然存在诸多问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;使用这个封装后的接口时，居然还需要在程序运行前确定所有可能会遇到的矩阵规模和存储格式，再用 &lt;a href=&quot;https://support.huawei.com/enterprise/zh/doc/EDOC1100164868/a3cf4cee&quot;&gt;ATC&lt;/a&gt; 工具将算子提前编译成二进制文件！诚然，这样固定问题规模的设计，有利于对不同的输入做编译期的优化，但是使用起来无疑是非常不方便的。&lt;/li&gt;
  &lt;li&gt;截至目前， 接口输入输出矩阵的 LD 只支持 &lt;code&gt;-1&lt;/code&gt;，由输入矩阵的格式推断。这和上面固定问题规模的问题一起，导致不能像 &lt;a href=&quot;https://docs.nvidia.com/cuda/cublas/index.html&quot;&gt;cuBLAS&lt;/a&gt; 一样做到即开即用，使用前基本都需要重新对数据进行打包。&lt;/li&gt;
  &lt;li&gt;GEMM 算子的性能非常低，在官方示例的基础上各种调整，也最多只能在 &lt;code&gt;{m, n, k} == {20000, 20000, 20000}&lt;/code&gt; 的问题规模上得到不到 10TFLOPS 的算力。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;关于第三点，我找到 Ascend910 对应的&lt;a href=&quot;https://support.huawei.com/enterprise/zh/doc/EDOC1100164829/f96da97d&quot;&gt;达芬奇架构设计图&lt;/a&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/22/zTLpgZy3SDJmeCn.png&quot; alt=&quot;达芬奇架构&quot; /&gt;&lt;/p&gt;

&lt;p&gt;再结合&lt;a href=&quot;https://support.huawei.com/enterprise/zh/doc/EDOC1100164829/9153d09&quot;&gt;文档里&lt;/a&gt;可知，矩阵运算和向量运算属于两条流水队列（共有五条）的，猜测 GEMM 算子内部混合使用了两个运算单元且流水调度做的比较差。而因为文档里的&lt;a href=&quot;https://support.huawei.com/enterprise/zh/doc/EDOC1100164829/1ed5847a&quot;&gt;这句话&lt;/a&gt;说，华为没有开放 Cube 单元和 Vector 单元混合编程的接口，因此没有继续探索手动控制流水线这条不归路了。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;当前仅支持用户开发 Vector 算子，由于开发高性能 Cube 算子难度较大，暂由华为交付。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;因此本例中，相较于 GEMM 算子，我运行的是 &lt;a href=&quot;https://support.huawei.com/enterprise/zh/doc/EDOC1100164838&quot;&gt;CANN V100R020C10 算子清单 (训练) 01&lt;/a&gt; 中查到的 MatMul 算子，从而真正发挥 NPU 的算力。&lt;/p&gt;

&lt;p&gt;此外我使用了 20.1.rc1 版本中 fwkacllib 库提供的 &lt;a href=&quot;https://support.huawei.com/enterprise/zh/doc/EDOC1100164875/4dfa293&quot;&gt;&lt;code&gt;aclopCompileAndExecute&lt;/code&gt;&lt;/a&gt; 接口，它可以在运行期异步编译并执行指定的算子。这个接口更适合那些首次启动性能不那么重要的场合（也可以在启动时做热身操作），比如端上的应用。需要注意的是，在&lt;a href=&quot;https://support.huawei.com/enterprise/zh/doc/EDOC1100180745/426cffd9&quot;&gt;后续版本的文档中&lt;/a&gt;移除了这个接口，这个特性可能仍处于开发阶段。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;/*

export DDK_PATH=~/Ascend/ascend-toolkit/20.1.rc1/arm64-linux

$CXX -std=c++11 -I$DDK_PATH/fwkacllib/include -L$DDK_PATH/fwkacllib/lib64 \
-lascendcl -lacl_op_compiler -o MatMul MatMul.cc

./MatMul 8192 8192 8192

*/

#include &amp;lt;acl/acl.h&amp;gt;
#include &amp;lt;acl/acl_op_compiler.h&amp;gt;
#include &amp;lt;functional&amp;gt;
#include &amp;lt;string&amp;gt;
#include &amp;lt;sys/time.h&amp;gt;

#define ACLCHECK(cmd)                                                          \
  do {                                                                         \
    int e = cmd;                                                               \
    if (e != ACL_ERROR_NONE) {                                                 \
      fprintf(stderr, &quot;%d&quot;, e);                                                \
      exit(1);                                                                 \
    }                                                                          \
  } while (0)

void WuK_Timer(const char *tag, float flo, const std::function&amp;lt;void()&amp;gt; &amp;amp;kernel,
               int test_time = 1) {
  double min_time = 9e99;
  while (test_time--) {
    struct timeval start, end;
    gettimeofday(&amp;amp;start, NULL);
    kernel();
    gettimeofday(&amp;amp;end, NULL);
    min_time = std::min(min_time, (end.tv_sec - start.tv_sec) * 1e3 +
                                      (end.tv_usec - start.tv_usec) * 1e-3);
  }
  fprintf(stdout, &quot;%s: %f ms, %e FLOPS\n&quot;, tag, min_time, flo * 1e3 / min_time);
}

int main(int argc, char **argv) {

  int64_t m = std::stoi(argv[1]), n = std::stoi(argv[2]),
          k = std::stoi(argv[3]), dimA[] = {m, k}, dimB[] = {k, n},
          dimC[] = {m, n}, sAsize = (m * k * sizeof(float) + 63) / 32 * 32,
          sBsize = (k * n * sizeof(float) + 63) / 32 * 32,
          sCsize = (m * n * sizeof(float) + 63) / 32 * 32,
          sALPHAsize = (sizeof(float) + 63) / 32 * 32,
          sBETAsize = (sizeof(float) + 63) / 32 * 32,
          hAsize = (m * k * sizeof(aclFloat16) + 63) / 32 * 32,
          hBsize = (k * n * sizeof(aclFloat16) + 63) / 32 * 32,
          hCsize = (m * k * sizeof(aclFloat16) + 63) / 32 * 32,
          hALPHAsize = (sizeof(aclFloat16) + 63) / 32 * 32,
          hBETAsize = (sizeof(aclFloat16) + 63) / 32 * 32;

  aclTensorDesc
      *sAdesc = aclCreateTensorDesc(ACL_FLOAT, 2, dimA, ACL_FORMAT_ND),
      *sBdesc = aclCreateTensorDesc(ACL_FLOAT, 2, dimB, ACL_FORMAT_ND),
      *sCdesc = aclCreateTensorDesc(ACL_FLOAT, 2, dimC, ACL_FORMAT_ND),
      *sALPHAdesc = aclCreateTensorDesc(ACL_FLOAT, 0, dimC, ACL_FORMAT_ND),
      *sBETAdesc = aclCreateTensorDesc(ACL_FLOAT, 0, dimC, ACL_FORMAT_ND),
      *sDesc[] = {sAdesc, sBdesc, sCdesc, sALPHAdesc, sBETAdesc},
      *hAdesc = aclCreateTensorDesc(ACL_FLOAT16, 2, dimA, ACL_FORMAT_ND),
      *hBdesc = aclCreateTensorDesc(ACL_FLOAT16, 2, dimB, ACL_FORMAT_ND),
      *hCdesc = aclCreateTensorDesc(ACL_FLOAT16, 2, dimC, ACL_FORMAT_ND),
      *hALPHAdesc = aclCreateTensorDesc(ACL_FLOAT16, 0, dimC, ACL_FORMAT_ND),
      *hBETAdesc = aclCreateTensorDesc(ACL_FLOAT16, 0, dimC, ACL_FORMAT_ND),
      *hDesc[] = {hAdesc, hBdesc, hCdesc, hALPHAdesc, hBETAdesc};

  aclopAttr *attrMatMul = aclopCreateAttr(), *attrGEMM = aclopCreateAttr(),
            *attrCastFp16 = aclopCreateAttr(),
            *attrCastFp32 = aclopCreateAttr();

  ACLCHECK(aclopSetAttrBool(attrMatMul, &quot;transpose_x1&quot;, 0));
  ACLCHECK(aclopSetAttrBool(attrMatMul, &quot;transpose_x2&quot;, 0));
  ACLCHECK(aclopSetAttrBool(attrGEMM, &quot;transpose_a&quot;, 0));
  ACLCHECK(aclopSetAttrBool(attrGEMM, &quot;transpose_b&quot;, 0));
  ACLCHECK(aclopSetAttrInt(attrCastFp16, &quot;dst_type&quot;, ACL_FLOAT16));
  ACLCHECK(aclopSetAttrInt(attrCastFp32, &quot;dst_type&quot;, ACL_FLOAT));

  WuK_Timer(&quot;aclInit&quot;, 0, [&amp;amp;] { ACLCHECK(aclInit(NULL)); });

  aclrtContext HPLAI_ACL_BLASPP_CONTEXT;

  WuK_Timer(&quot;aclrtCreateContext&quot;, 0, [&amp;amp;] {
    uint32_t deviceId = 0, count = 0;
    aclrtGetDeviceCount(&amp;amp;count);
    ACLCHECK(aclrtCreateContext(&amp;amp;HPLAI_ACL_BLASPP_CONTEXT, deviceId % count));
  });

  WuK_Timer(&quot;aclrtSetCurrentContext&quot;, 0, [&amp;amp;] {
    ACLCHECK(aclrtSetCurrentContext(HPLAI_ACL_BLASPP_CONTEXT));
  });

  void *HPLAI_ACL_BLASPP_HOST_BUFFER, *HPLAI_ACL_BLASPP_DEVICE_BUFFER;
  int64_t HPLAI_ACL_BLASPP_HOST_BUFFER_SIZE =
              sAsize + sBsize + sCsize + sALPHAsize + sBETAsize,
          HPLAI_ACL_BLASPP_DEVICE_BUFFER_SIZE =
              HPLAI_ACL_BLASPP_HOST_BUFFER_SIZE + hAsize + hBsize + hCsize +
              hALPHAsize + hBETAsize;

  WuK_Timer(&quot;aclrtMalloc&quot;, 0, [&amp;amp;] {
    ACLCHECK(aclrtMalloc(&amp;amp;HPLAI_ACL_BLASPP_DEVICE_BUFFER,
                         HPLAI_ACL_BLASPP_DEVICE_BUFFER_SIZE,
                         ACL_MEM_MALLOC_HUGE_FIRST));
  });

  WuK_Timer(&quot;aclrtMallocHost&quot;, 0, [&amp;amp;] {
    ACLCHECK(aclrtMallocHost(&amp;amp;HPLAI_ACL_BLASPP_HOST_BUFFER,
                             HPLAI_ACL_BLASPP_HOST_BUFFER_SIZE));
  });

  char *sAdevice = (char *)HPLAI_ACL_BLASPP_DEVICE_BUFFER,
       *sBdevice = sAdevice + sAsize, *sCdevice = sBdevice + sBsize,
       *sALPHAdevice = sCdevice + sCsize,
       *sBETAdevice = sALPHAdevice + sALPHAsize,
       *hAdevice = sBETAdevice + sBETAsize, *hBdevice = hAdevice + hAsize,
       *hCdevice = hBdevice + hBsize, *hALPHAdevice = hCdevice + hCsize,
       *hBETAdevice = hALPHAdevice + hALPHAsize,
       *sAhost = (char *)HPLAI_ACL_BLASPP_HOST_BUFFER,
       *sBhost = sAhost + sAsize, *sChost = sBhost + sBsize,
       *sALPHAhost = sChost + sCsize, *sBETAhost = sALPHAhost + sALPHAsize;

  WuK_Timer(&quot;aclrtMemsetAsync&quot;, 0, [&amp;amp;] {
    ACLCHECK(aclrtMemsetAsync(HPLAI_ACL_BLASPP_HOST_BUFFER,
                              HPLAI_ACL_BLASPP_HOST_BUFFER_SIZE, 0,
                              HPLAI_ACL_BLASPP_HOST_BUFFER_SIZE, NULL));
    ACLCHECK(aclrtSynchronizeStream(NULL));
  });

  WuK_Timer(&quot;aclrtMemcpyAsyncHtoD&quot;, 0, [&amp;amp;] {
    ACLCHECK(aclrtMemcpyAsync(
        HPLAI_ACL_BLASPP_DEVICE_BUFFER, HPLAI_ACL_BLASPP_DEVICE_BUFFER_SIZE,
        HPLAI_ACL_BLASPP_HOST_BUFFER, HPLAI_ACL_BLASPP_HOST_BUFFER_SIZE,
        ACL_MEMCPY_HOST_TO_DEVICE, NULL));
    ACLCHECK(aclrtSynchronizeStream(NULL));
  });

  aclDataBuffer *sAdata = aclCreateDataBuffer(sAdevice, sAsize),
                *sBdata = aclCreateDataBuffer(sBdevice, sBsize),
                *sCdata = aclCreateDataBuffer(sCdevice, sCsize),
                *sALPHAdata = aclCreateDataBuffer(sALPHAdevice, sALPHAsize),
                *sBETAdata = aclCreateDataBuffer(sBETAdevice, sBETAsize),
                *sData[] = {sAdata, sBdata, sCdata, sALPHAdata, sBETAdata},
                *hAdata = aclCreateDataBuffer(hAdevice, hAsize),
                *hBdata = aclCreateDataBuffer(hBdevice, hBsize),
                *hCdata = aclCreateDataBuffer(hCdevice, hCsize),
                *hALPHAdata = aclCreateDataBuffer(hALPHAdevice, hALPHAsize),
                *hBETAdata = aclCreateDataBuffer(hBETAdevice, hBETAsize),
                *hData[] = {hAdata, hBdata, hCdata, hALPHAdata, hBETAdata};

  WuK_Timer(
      &quot;Cast(aclFloat16)&quot;, 0,
      [&amp;amp;] {
        ACLCHECK(aclopCompileAndExecute(&quot;Cast&quot;, 1, sDesc, sData, 1, hDesc,
                                        hData, attrCastFp16, ACL_ENGINE_SYS,
                                        ACL_COMPILE_SYS, NULL, NULL));
        ACLCHECK(aclrtSynchronizeStream(NULL));
      },
      10);
  WuK_Timer(
      &quot;Cast(float)&quot;, 0,
      [&amp;amp;] {
        ACLCHECK(aclopCompileAndExecute(
            &quot;Cast&quot;, 1, hDesc + 2, hData + 2, 1, sDesc + 2, sData + 2,
            attrCastFp32, ACL_ENGINE_SYS, ACL_COMPILE_SYS, NULL, NULL));
        ACLCHECK(aclrtSynchronizeStream(NULL));
      },
      10);

  WuK_Timer(
      &quot;MatMulV2(aclFloat16)&quot;, 2.0 * m * n * k,
      [&amp;amp;] {
        ACLCHECK(aclopCompileAndExecute(
            &quot;MatMulV2&quot;, 2, hDesc, hData, 1, hDesc + 2, hData + 2, attrMatMul,
            ACL_ENGINE_SYS, ACL_COMPILE_SYS, NULL, NULL));
        ACLCHECK(aclrtSynchronizeStream(NULL));
      },
      10);

  WuK_Timer(
      &quot;MatMulV2(float)&quot;, 2.0 * m * n * k,
      [&amp;amp;] {
        ACLCHECK(aclopCompileAndExecute(
            &quot;MatMulV2&quot;, 2, sDesc, sData, 1, sDesc + 2, sData + 2, attrMatMul,
            ACL_ENGINE_SYS, ACL_COMPILE_SYS, NULL, NULL));
        ACLCHECK(aclrtSynchronizeStream(NULL));
      },
      10);

  WuK_Timer(
      &quot;MatMul(aclFloat16)&quot;, 2.0 * m * n * k,
      [&amp;amp;] {
        ACLCHECK(aclopCompileAndExecute(&quot;MatMul&quot;, 2, hDesc, hData, 1, hDesc + 2,
                                        hData + 2, attrMatMul, ACL_ENGINE_SYS,
                                        ACL_COMPILE_SYS, NULL, NULL));
        ACLCHECK(aclrtSynchronizeStream(NULL));
      },
      10);

  WuK_Timer(
      &quot;MatMul(float)&quot;, 2.0 * m * n * k,
      [&amp;amp;] {
        ACLCHECK(aclopCompileAndExecute(&quot;MatMul&quot;, 2, sDesc, sData, 1, sDesc + 2,
                                        sData + 2, attrMatMul, ACL_ENGINE_SYS,
                                        ACL_COMPILE_SYS, NULL, NULL));
        ACLCHECK(aclrtSynchronizeStream(NULL));
      },
      10);

  WuK_Timer(
      &quot;GEMM(aclFloat16)&quot;, 2.0 * m * n * k,
      [&amp;amp;] {
        ACLCHECK(aclopCompileAndExecute(&quot;GEMM&quot;, 5, hDesc, hData, 1, hDesc + 2,
                                        hData + 2, attrGEMM, ACL_ENGINE_SYS,
                                        ACL_COMPILE_SYS, NULL, NULL));
        ACLCHECK(aclrtSynchronizeStream(NULL));
      },
      10);

  WuK_Timer(&quot;aclrtMemcpyAsyncDtoH&quot;, 0, [&amp;amp;] {
    ACLCHECK(aclrtMemcpyAsync(sChost, sCsize, sCdevice, sCsize,
                              ACL_MEMCPY_DEVICE_TO_HOST, NULL));
    ACLCHECK(aclrtSynchronizeStream(NULL));
  });

  ACLCHECK(aclDestroyDataBuffer(sAdata));
  ACLCHECK(aclDestroyDataBuffer(sBdata));
  ACLCHECK(aclDestroyDataBuffer(sCdata));
  ACLCHECK(aclDestroyDataBuffer(sALPHAdata));
  ACLCHECK(aclDestroyDataBuffer(sBETAdata));
  ACLCHECK(aclDestroyDataBuffer(hAdata));
  ACLCHECK(aclDestroyDataBuffer(hBdata));
  ACLCHECK(aclDestroyDataBuffer(hCdata));
  ACLCHECK(aclDestroyDataBuffer(hALPHAdata));
  ACLCHECK(aclDestroyDataBuffer(hBETAdata));
  aclopDestroyAttr(attrCastFp32);
  aclopDestroyAttr(attrCastFp16);
  aclopDestroyAttr(attrGEMM);
  aclopDestroyAttr(attrMatMul);

  WuK_Timer(&quot;aclrtFree&quot;, 0,
            [&amp;amp;] { ACLCHECK(aclrtFree(HPLAI_ACL_BLASPP_DEVICE_BUFFER)); });
  WuK_Timer(&quot;aclrtFreeHost&quot;, 0,
            [&amp;amp;] { ACLCHECK(aclrtFreeHost(HPLAI_ACL_BLASPP_HOST_BUFFER)); });

  WuK_Timer(&quot;aclrtDestroyContext&quot;, 0,
            [&amp;amp;] { ACLCHECK(aclrtDestroyContext(HPLAI_ACL_BLASPP_CONTEXT)); });

  WuK_Timer(&quot;aclFinalize&quot;, 0, [&amp;amp;] { ACLCHECK(aclFinalize()); });
}
&lt;/code&gt;&lt;/pre&gt;</content><author><name></name></author><summary type="html">在本文中，我使用华为 Atlas 800-9000 训练服务器上的 Ascend 910 AI 加速卡上跑了一个 MatMul 算子，并对调用 AscendCL 接口的地方分别计时，从而大致了解使用 NPU 启动一次计算的流程和各部分耗时情况。</summary></entry><entry><title type="html">HPL-AI</title><link href="http://localhost:4000/2021/03/14/HPL-AI/" rel="alternate" type="text/html" title="HPL-AI" /><published>2021-03-14T00:00:00+08:00</published><updated>2021-03-14T00:00:00+08:00</updated><id>http://localhost:4000/2021/03/14/HPL-AI</id><content type="html" xml:base="http://localhost:4000/2021/03/14/HPL-AI/">&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/05/23/8QcJtLTwF1Upn46.png&quot; alt=&quot;grade&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;REFERENCE&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wu-kan/HPL-AI&quot;&gt;code&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wu-kan.github.io/sysu-thesis/main.pdf&quot;&gt;论文（中文）&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wu-kan.github.io/sysu-thesis/presentation/pre.pdf&quot;&gt;展示（中文）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;docs&quot;&gt;DOCS&lt;/h2&gt;

&lt;p&gt;This project is the graduation design for my bachelor’s degree. The complete document will be uploaded later.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">又谈CUDA上的Reduce操作</title><link href="http://localhost:4000/2021/03/13/%E5%8F%88%E8%B0%88CUDA%E4%B8%8A%E7%9A%84Reduce%E6%93%8D%E4%BD%9C/" rel="alternate" type="text/html" title="又谈CUDA上的Reduce操作" /><published>2021-03-13T00:00:00+08:00</published><updated>2021-03-13T00:00:00+08:00</updated><id>http://localhost:4000/2021/03/13/%E5%8F%88%E8%B0%88CUDA%E4%B8%8A%E7%9A%84Reduce%E6%93%8D%E4%BD%9C</id><content type="html" xml:base="http://localhost:4000/2021/03/13/%E5%8F%88%E8%B0%88CUDA%E4%B8%8A%E7%9A%84Reduce%E6%93%8D%E4%BD%9C/">&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;五个 &lt;code&gt;shfl.sync.bfly.b32&lt;/code&gt; 和 &lt;code&gt;add&lt;/code&gt; 被换成了 &lt;code&gt;redux.sync.add.s32&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;几乎没卵用，也许在 ptx 到 sass 汇编的后端编译过程中自动被优化了？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;del&gt;很好，又水了一篇 blog&lt;/del&gt;&lt;/p&gt;

&lt;h2 id=&quot;源代码&quot;&gt;源代码&lt;/h2&gt;

&lt;h3 id=&quot;reduce_addsh&quot;&gt;&lt;code&gt;reduce_add.sh&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash

spack unload -a
spack load gcc@9.3.0
spack load cuda@11.0.2

spack find --loaded
cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c
nvidia-smi

nvcc -gencode=arch=compute_80,code=sm_80 -run reduce_add.cu
nvcc -gencode=arch=compute_80,code=sm_80 -run reduce_add.cu -DUSE_AMPERE_REDUCE
nvcc -gencode=arch=compute_80,code=sm_80 -ptx -o reduce_add.0.ptx reduce_add.cu
nvcc -gencode=arch=compute_80,code=sm_80 -ptx -o reduce_add.1.ptx reduce_add.cu -DUSE_AMPERE_REDUCE
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;reduce_addlog&quot;&gt;&lt;code&gt;reduce_add.log&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;-- linux-debian9-zen / gcc@6.3.0 --------------------------------
gcc@9.3.0
gmp@6.1.2
isl@0.20
mpc@1.1.0
mpfr@3.1.6
zlib@1.2.11

-- linux-debian9-zen2 / gcc@9.3.0 -------------------------------
cuda@11.0.2
libiconv@1.16
libxml2@2.9.10
xz@5.2.5
zlib@1.2.11
    128  AMD EPYC 7542 32-Core Processor
Tue Mar  9 08:01:10 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  A100-SXM4-40GB      Off  | 00000000:0F:00.0 Off |                    0 |
| N/A   36C    P0    61W / 400W |      0MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  A100-SXM4-40GB      Off  | 00000000:15:00.0 Off |                   On |
| N/A   34C    P0    47W / 400W |      0MiB / 40537MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   2  A100-SXM4-40GB      Off  | 00000000:51:00.0 Off |                   On |
| N/A   31C    P0    43W / 400W |      0MiB / 40537MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   3  A100-SXM4-40GB      Off  | 00000000:54:00.0 Off |                   On |
| N/A   32C    P0    46W / 400W |      0MiB / 40537MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   4  A100-SXM4-40GB      Off  | 00000000:8D:00.0 Off |                   On |
| N/A   31C    P0    46W / 400W |      0MiB / 40537MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   5  A100-SXM4-40GB      Off  | 00000000:92:00.0 Off |                   On |
| N/A   30C    P0    43W / 400W |      0MiB / 40537MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   6  A100-SXM4-40GB      Off  | 00000000:D6:00.0 Off |                   On |
| N/A   30C    P0    43W / 400W |      0MiB / 40537MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+
|   7  A100-SXM4-40GB      Off  | 00000000:DA:00.0 Off |                   On |
| N/A   32C    P0    45W / 400W |      3MiB / 40537MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| MIG devices:                                                                |
+------------------+----------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |
|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|
|                  |                      |        ECC|                       |
|==================+======================+===========+=======================|
|  7   13   0   0  |      3MiB /  4864MiB | 14      0 |  1   0    0    0    0 |
|                  |      0MiB /  8191MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
wuk::host_reduce_add_strided_batched: 27.999231 ms, 3.834898e+10 FLOPS.
wuk::host_reduce_add_strided_batched: 28.010496 ms, 3.833355e+10 FLOPS.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;reduce_addcu&quot;&gt;&lt;code&gt;reduce_add.cu&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;实验代码。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;lt;cstdio&amp;gt;
#include &amp;lt;cuda.h&amp;gt;
#include &amp;lt;cuda_runtime.h&amp;gt;
#include &amp;lt;thrust/device_vector.h&amp;gt;

namespace wuk
{

    template &amp;lt;
        typename Tcompute,
        int WARPSIZE = 32,
        typename Tmask = unsigned&amp;gt;
    static __device__ __forceinline__ Tcompute
    warp_allreduce_add(
        Tcompute val,
        const Tmask FINAL_MASK = 0xffffffff)
    {

#if CUDA_VERSION &amp;lt; 9000

#pragma unroll
        for (int offset = WARPSIZE &amp;gt;&amp;gt; 1; offset &amp;gt; 0; offset &amp;gt;&amp;gt;= 1)
            val += __shfl_xor(val, FINAL_MASK, offset);

#else

#pragma unroll
        for (int offset = WARPSIZE &amp;gt;&amp;gt; 1; offset &amp;gt; 0; offset &amp;gt;&amp;gt;= 1)
            val += __shfl_xor_sync(FINAL_MASK, val, offset, WARPSIZE);

#endif

        return val;
    }

#ifdef USE_AMPERE_REDUCE

    template &amp;lt;&amp;gt;
    __device__ __forceinline__ int
    warp_allreduce_add&amp;lt;
        int,
        32,
        unsigned&amp;gt;(
        int val,
        const unsigned FINAL_MASK)
    {
        return __reduce_add_sync(FINAL_MASK, val);
    }

    template &amp;lt;&amp;gt;
    __device__ __forceinline__ unsigned
    warp_allreduce_add&amp;lt;
        unsigned,
        32,
        unsigned&amp;gt;(
        unsigned val,
        const unsigned FINAL_MASK)
    {
        return __reduce_add_sync(FINAL_MASK, val);
    }

#endif

    template &amp;lt;
        typename Tin,
        typename Tcompute,
        int WARPSIZE = 32&amp;gt;
    static __device__ __forceinline__ Tcompute
    thread_reduce_add(
        int n,
        Tin *x,
        int incx,
        int offset)
    {
        Tcompute val = (Tcompute)0;
        for (int i = offset; i &amp;lt; n; i += incx)
            val += (Tcompute)x[i];
        return val;
    }

    template &amp;lt;
        typename Tin,
        typename Tout,
        typename Tcompute,
        int BLOCKSIZE,
        int WARPSIZE = 32&amp;gt;
    static __device__ __forceinline__ void
    block_reduce_add(
        int n,
        Tin *x,
        int incx,
        Tout *result)
    {
        static __shared__ Tcompute smem[BLOCKSIZE / WARPSIZE];
        {
            const Tcompute val = warp_allreduce_add&amp;lt;
                Tcompute,
                WARPSIZE&amp;gt;(
                thread_reduce_add&amp;lt;
                    Tin,
                    Tcompute&amp;gt;(
                    n,
                    x,
                    incx * BLOCKSIZE,
                    threadIdx.x));
            if (threadIdx.x % WARPSIZE == 0)
                smem[threadIdx.x / WARPSIZE] = val;
        }
        __syncthreads();
        if (threadIdx.x &amp;lt; WARPSIZE)
        {
            const Tcompute val = warp_allreduce_add&amp;lt;
                Tcompute,
                WARPSIZE&amp;gt;(
                thread_reduce_add&amp;lt;
                    Tcompute,
                    Tcompute&amp;gt;(
                    BLOCKSIZE / WARPSIZE,
                    smem,
                    WARPSIZE,
                    threadIdx.x));
            if (threadIdx.x % WARPSIZE == 0)
                result[threadIdx.x / WARPSIZE] = (Tout)val;
        }
    }

    template &amp;lt;
        typename Tin,
        typename Tout,
        typename Tcompute,
        int BLOCKSIZE,
        int WARPSIZE = 32&amp;gt;
    static __global__ __launch_bounds__(BLOCKSIZE) void global_reduce_add_strided_batched(
        int n, Tin *x, int incx,
        int stride_x,
        Tout *result)
    {
        block_reduce_add&amp;lt;
            Tin,
            Tout,
            Tcompute,
            BLOCKSIZE,
            WARPSIZE&amp;gt;(
            n,
            x + stride_x * blockIdx.x,
            incx,
            result + blockIdx.x);
    }

    template &amp;lt;
        typename Tin,
        typename Tout,
        typename Tcompute,
        int WARPSIZE = 32&amp;gt;
    void host_reduce_add_strided_batched(
        int n,
        Tin *x,
        int incx,
        int stride_x,
        Tout *result,
        int batch_count,
        cudaStream_t stream)
    {
        const int BLOCKSIZE = 1024;
        dim3 blockDim(BLOCKSIZE), gridDim(batch_count);
        global_reduce_add_strided_batched&amp;lt;
            Tin,
            Tout,
            Tcompute,
            BLOCKSIZE,
            WARPSIZE&amp;gt;&amp;lt;&amp;lt;&amp;lt;
            gridDim,
            blockDim,
            0,
            stream&amp;gt;&amp;gt;&amp;gt;(
            n,
            x,
            incx,
            stride_x,
            result);
    }

} // namespace wuk

void WuK_Timer(
    const char *tag,
    float flo,
    const std::function&amp;lt;void()&amp;gt; &amp;amp;kernel,
    int test_time = 9)
{
    float min_time = 9e99;
    while (test_time--)
    {
        cudaEvent_t beg, end;
        cudaEventCreate(&amp;amp;beg);
        cudaEventCreate(&amp;amp;end);
        cudaEventRecord(beg);
        kernel();
        cudaEventRecord(end);
        cudaEventSynchronize(beg);
        cudaEventSynchronize(end);
        float elapsed_time;
        cudaEventElapsedTime(&amp;amp;elapsed_time, beg, end);
        min_time = std::min(min_time, elapsed_time);
    }
    std::printf(&quot;%s: %f ms, %e FLOPS.\n&quot;, tag, min_time, flo * 1e3 / min_time);
}

typedef int Tcompute;
typedef Tcompute Tin;
typedef Tcompute Tout;

const int64_t
    n = 1 &amp;lt;&amp;lt; 19,
    incx = 1,
    stride_x = n * incx,
    batch_count = 1 &amp;lt;&amp;lt; 11;
thrust::device_vector&amp;lt;Tin&amp;gt; x_vector(stride_x *batch_count, 1);
thrust::device_vector&amp;lt;Tout&amp;gt; result_vector(batch_count, 0);
cudaStream_t stream = NULL;
int main()
{
    WuK_Timer(
        &quot;wuk::host_reduce_add_strided_batched&quot;,
        1.0 * n * batch_count,
        [&amp;amp;] {
            wuk::host_reduce_add_strided_batched&amp;lt;
                Tin,
                Tout,
                Tcompute&amp;gt;(
                n,
                thrust::raw_pointer_cast(x_vector.data()),
                incx,
                stride_x,
                thrust::raw_pointer_cast(result_vector.data()),
                batch_count,
                stream);
        });
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;reduce_add0ptx&quot;&gt;&lt;code&gt;reduce_add.0.ptx&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-llvm&quot;&gt;//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-28540450
// Cuda compilation tools, release 11.0, V11.0.194
// Based on LLVM 3.4svn
//

.version 7.0
.target sm_80
.address_size 64

        // .globl       _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1_
// _ZZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem has been demoted
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust6system6detail10sequential3seqE[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust6system3cpp3parE[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust8cuda_cub3parE[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_1E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_2E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_3E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_4E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_5E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_6E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_7E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_8E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_9E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders3_10E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust3seqE[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b16thrust6deviceE[1];

.visible .entry _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1_(
        .param .align 8 .b8 _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1__param_0[16],
        .param .u64 _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1__param_1
)
.maxntid 256, 1, 1
{
        .reg .pred      %p&amp;lt;8&amp;gt;;
        .reg .b32       %r&amp;lt;13&amp;gt;;
        .reg .b64       %rd&amp;lt;36&amp;gt;;


        ld.param.u64    %rd2, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1__param_0];
        ld.param.u32    %r1, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1__param_0+8];
        ld.param.u64    %rd9, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1__param_1];
        mov.u32         %r3, %ctaid.x;
        mul.wide.u32    %rd10, %r3, 512;
        sub.s64         %rd11, %rd9, %rd10;
        mov.u64         %rd12, 512;
        min.u64         %rd1, %rd11, %rd12;
        setp.eq.s64     %p1, %rd1, 512;
        mov.u32         %r2, %tid.x;
        cvt.u64.u32     %rd3, %r2;
        @%p1 bra        BB0_7;
        bra.uni         BB0_1;

BB0_7:
        add.s64         %rd28, %rd3, %rd10;
        shl.b64         %rd29, %rd28, 2;
        add.s64         %rd7, %rd2, %rd29;
        setp.eq.s64     %p6, %rd7, 0;
        @%p6 bra        BB0_9;

        cvta.to.global.u64      %rd30, %rd7;
        st.global.u32   [%rd30], %r1;

BB0_9:
        add.s32         %r11, %r2, 256;
        cvt.u64.u32     %rd31, %r11;
        add.s64         %rd33, %rd31, %rd10;
        shl.b64         %rd34, %rd33, 2;
        add.s64         %rd8, %rd2, %rd34;
        setp.eq.s64     %p7, %rd8, 0;
        @%p7 bra        BB0_11;

        cvta.to.global.u64      %rd35, %rd8;
        st.global.u32   [%rd35], %r1;
        bra.uni         BB0_11;

BB0_1:
        cvt.s64.s32     %rd4, %rd1;
        setp.ge.u64     %p2, %rd3, %rd4;
        @%p2 bra        BB0_4;

        add.s64         %rd14, %rd3, %rd10;
        shl.b64         %rd15, %rd14, 2;
        add.s64         %rd16, %rd2, %rd15;
        setp.eq.s64     %p3, %rd16, 0;
        @%p3 bra        BB0_4;

        cvta.to.global.u64      %rd22, %rd16;
        st.global.u32   [%rd22], %r1;

BB0_4:
        add.s32         %r8, %r2, 256;
        cvt.u64.u32     %rd5, %r8;
        setp.ge.u64     %p4, %rd5, %rd4;
        @%p4 bra        BB0_11;

        add.s64         %rd24, %rd5, %rd10;
        shl.b64         %rd25, %rd24, 2;
        add.s64         %rd6, %rd2, %rd25;
        setp.eq.s64     %p5, %rd6, 0;
        @%p5 bra        BB0_11;

        cvta.to.global.u64      %rd26, %rd6;
        st.global.u32   [%rd26], %r1;

BB0_11:
        ret;
}

        // .globl       _ZN3cub11EmptyKernelIvEEvv
.visible .entry _ZN3cub11EmptyKernelIvEEvv(

)
{



        ret;
}

.entry _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0_(
        .param .u32 _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_0,
        .param .u64 _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_1,
        .param .u32 _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_2,
        .param .u32 _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_3,
        .param .u64 _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_4
)
.maxntid 1024, 1, 1
{
        .reg .pred      %p&amp;lt;23&amp;gt;;
        .reg .b32       %r&amp;lt;120&amp;gt;;
        .reg .b64       %rd&amp;lt;15&amp;gt;;
        // demoted variable
        .shared .align 4 .b8 _ZZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem[128];

        ld.param.u32    %r35, [_ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_0];
        ld.param.u64    %rd4, [_ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_1];
        ld.param.u32    %r38, [_ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_2];
        ld.param.u32    %r39, [_ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_3];
        ld.param.u64    %rd3, [_ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_4];
        cvta.to.global.u64      %rd1, %rd4;
        mov.u32         %r1, %ctaid.x;
        mul.lo.s32      %r40, %r1, %r39;
        cvt.u64.u32     %rd2, %r40;
        shl.b32         %r2, %r38, 10;
        mov.u32         %r3, %tid.x;
        mov.u32         %r109, 0;
        setp.ge.s32     %p1, %r3, %r35;
        @%p1 bra        BB2_3;

        mov.u32         %r107, %r3;

BB2_2:
        cvt.s64.s32     %rd5, %r107;
        add.s64         %rd6, %rd5, %rd2;
        shl.b64         %rd7, %rd6, 2;
        add.s64         %rd8, %rd1, %rd7;
        ld.global.u32   %r41, [%rd8];
        add.s32         %r109, %r41, %r109;
        add.s32         %r107, %r107, %r2;
        setp.lt.s32     %p2, %r107, %r35;
        @%p2 bra        BB2_2;

BB2_3:
        mov.u32         %r42, 31;
        mov.u32         %r43, 16;
        mov.u32         %r44, -1;
        shfl.sync.bfly.b32      %r45|%p3, %r109, %r43, %r42, %r44;
        add.s32         %r46, %r45, %r109;
        mov.u32         %r47, 8;
        shfl.sync.bfly.b32      %r48|%p4, %r46, %r47, %r42, %r44;
        add.s32         %r49, %r48, %r46;
        mov.u32         %r50, 4;
        shfl.sync.bfly.b32      %r51|%p5, %r49, %r50, %r42, %r44;
        add.s32         %r52, %r51, %r49;
        mov.u32         %r53, 2;
        shfl.sync.bfly.b32      %r54|%p6, %r52, %r53, %r42, %r44;
        add.s32         %r55, %r54, %r52;
        mov.u32         %r56, 1;
        shfl.sync.bfly.b32      %r57|%p7, %r55, %r56, %r42, %r44;
        add.s32         %r9, %r57, %r55;
        and.b32         %r10, %r3, 31;
        setp.ne.s32     %p8, %r10, 0;
        @%p8 bra        BB2_5;

        shr.u32         %r58, %r3, 3;
        and.b32         %r59, %r58, 536870908;
        mov.u32         %r60, _ZZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem;
        add.s32         %r61, %r60, %r59;
        st.shared.u32   [%r61], %r9;

BB2_5:
        bar.sync        0;
        setp.gt.u32     %p9, %r3, 31;
        @%p9 bra        BB2_18;

        mov.u32         %r119, 0;
        setp.gt.s32     %p10, %r3, 31;
        @%p10 bra       BB2_16;

        setp.gt.s32     %p11, %r3, 0;
        mov.u32         %r119, 0;
        add.s32         %r64, %r3, 31;
        selp.b32        %r65, %r64, 31, %p11;
        sub.s32         %r66, %r65, %r3;
        shr.u32         %r67, %r66, 5;
        add.s32         %r11, %r67, 1;
        and.b32         %r12, %r11, 3;
        setp.eq.s32     %p12, %r12, 0;
        mov.u32         %r114, %r3;
        @%p12 bra       BB2_13;

        setp.eq.s32     %p13, %r12, 1;
        mov.u32         %r113, 0;
        mov.u32         %r112, %r3;
        @%p13 bra       BB2_12;

        setp.eq.s32     %p14, %r12, 2;
        mov.u32         %r111, 0;
        mov.u32         %r110, %r3;
        @%p14 bra       BB2_11;

        shl.b32         %r70, %r3, 2;
        mov.u32         %r71, _ZZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem;
        add.s32         %r72, %r71, %r70;
        ld.shared.u32   %r111, [%r72];
        add.s32         %r110, %r3, 32;

BB2_11:
        shl.b32         %r73, %r110, 2;
        mov.u32         %r74, _ZZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem;
        add.s32         %r75, %r74, %r73;
        ld.shared.u32   %r76, [%r75];
        add.s32         %r113, %r76, %r111;
        add.s32         %r112, %r110, 32;

BB2_12:
        shl.b32         %r77, %r112, 2;
        mov.u32         %r78, _ZZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem;
        add.s32         %r79, %r78, %r77;
        ld.shared.u32   %r80, [%r79];
        add.s32         %r119, %r80, %r113;
        add.s32         %r114, %r112, 32;

BB2_13:
        setp.lt.u32     %p15, %r11, 4;
        @%p15 bra       BB2_16;

        add.s32         %r117, %r114, -32;
        shl.b32         %r81, %r114, 2;
        mov.u32         %r82, _ZZN58_INTERNAL_36_tmpxft_000434ec_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem;
        add.s32         %r116, %r82, %r81;

BB2_15:
        ld.shared.u32   %r83, [%r116];
        add.s32         %r84, %r83, %r119;
        ld.shared.u32   %r85, [%r116+128];
        add.s32         %r86, %r85, %r84;
        ld.shared.u32   %r87, [%r116+256];
        add.s32         %r88, %r87, %r86;
        ld.shared.u32   %r89, [%r116+384];
        add.s32         %r119, %r89, %r88;
        add.s32         %r116, %r116, 512;
        add.s32         %r117, %r117, 128;
        setp.lt.s32     %p16, %r117, 0;
        @%p16 bra       BB2_15;

BB2_16:
        shfl.sync.bfly.b32      %r93|%p17, %r119, %r43, %r42, %r44;
        add.s32         %r94, %r93, %r119;
        shfl.sync.bfly.b32      %r96|%p18, %r94, %r47, %r42, %r44;
        add.s32         %r97, %r96, %r94;
        shfl.sync.bfly.b32      %r99|%p19, %r97, %r50, %r42, %r44;
        add.s32         %r100, %r99, %r97;
        shfl.sync.bfly.b32      %r102|%p20, %r100, %r53, %r42, %r44;
        add.s32         %r103, %r102, %r100;
        shfl.sync.bfly.b32      %r105|%p21, %r103, %r56, %r42, %r44;
        add.s32         %r34, %r105, %r103;
        @%p8 bra        BB2_18;

        shr.u32         %r106, %r3, 5;
        cvt.u64.u32     %rd9, %r106;
        cvt.u64.u32     %rd10, %r1;
        add.s64         %rd11, %rd9, %rd10;
        cvta.to.global.u64      %rd12, %rd3;
        shl.b64         %rd13, %rd11, 2;
        add.s64         %rd14, %rd12, %rd13;
        st.global.u32   [%rd14], %r34;

BB2_18:
        ret;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;reduce_add1ptx&quot;&gt;&lt;code&gt;reduce_add.1.ptx&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-llvm&quot;&gt;//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-28540450
// Cuda compilation tools, release 11.0, V11.0.194
// Based on LLVM 3.4svn
//

.version 7.0
.target sm_80
.address_size 64

        // .globl       _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1_
// _ZZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem has been demoted
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust6system6detail10sequential3seqE[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust6system3cpp3parE[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust8cuda_cub3parE[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_1E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_2E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_3E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_4E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_5E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_6E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_7E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_8E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders2_9E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust12placeholders3_10E[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust3seqE[1];
.global .align 1 .b8 _ZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b16thrust6deviceE[1];

.visible .entry _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1_(
        .param .align 8 .b8 _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1__param_0[16],
        .param .u64 _ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1__param_1
)
.maxntid 256, 1, 1
{
        .reg .pred      %p&amp;lt;8&amp;gt;;
        .reg .b32       %r&amp;lt;13&amp;gt;;
        .reg .b64       %rd&amp;lt;36&amp;gt;;


        ld.param.u64    %rd2, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1__param_0];
        ld.param.u32    %r1, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1__param_0+8];
        ld.param.u64    %rd9, [_ZN6thrust8cuda_cub4core13_kernel_agentINS0_14__parallel_for16ParallelForAgentINS0_20__uninitialized_fill7functorINS_10device_ptrIiEEiEEmEES9_mEEvT0_T1__param_1];
        mov.u32         %r3, %ctaid.x;
        mul.wide.u32    %rd10, %r3, 512;
        sub.s64         %rd11, %rd9, %rd10;
        mov.u64         %rd12, 512;
        min.u64         %rd1, %rd11, %rd12;
        setp.eq.s64     %p1, %rd1, 512;
        mov.u32         %r2, %tid.x;
        cvt.u64.u32     %rd3, %r2;
        @%p1 bra        BB0_7;
        bra.uni         BB0_1;

BB0_7:
        add.s64         %rd28, %rd3, %rd10;
        shl.b64         %rd29, %rd28, 2;
        add.s64         %rd7, %rd2, %rd29;
        setp.eq.s64     %p6, %rd7, 0;
        @%p6 bra        BB0_9;

        cvta.to.global.u64      %rd30, %rd7;
        st.global.u32   [%rd30], %r1;

BB0_9:
        add.s32         %r11, %r2, 256;
        cvt.u64.u32     %rd31, %r11;
        add.s64         %rd33, %rd31, %rd10;
        shl.b64         %rd34, %rd33, 2;
        add.s64         %rd8, %rd2, %rd34;
        setp.eq.s64     %p7, %rd8, 0;
        @%p7 bra        BB0_11;

        cvta.to.global.u64      %rd35, %rd8;
        st.global.u32   [%rd35], %r1;
        bra.uni         BB0_11;

BB0_1:
        cvt.s64.s32     %rd4, %rd1;
        setp.ge.u64     %p2, %rd3, %rd4;
        @%p2 bra        BB0_4;

        add.s64         %rd14, %rd3, %rd10;
        shl.b64         %rd15, %rd14, 2;
        add.s64         %rd16, %rd2, %rd15;
        setp.eq.s64     %p3, %rd16, 0;
        @%p3 bra        BB0_4;

        cvta.to.global.u64      %rd22, %rd16;
        st.global.u32   [%rd22], %r1;

BB0_4:
        add.s32         %r8, %r2, 256;
        cvt.u64.u32     %rd5, %r8;
        setp.ge.u64     %p4, %rd5, %rd4;
        @%p4 bra        BB0_11;

        add.s64         %rd24, %rd5, %rd10;
        shl.b64         %rd25, %rd24, 2;
        add.s64         %rd6, %rd2, %rd25;
        setp.eq.s64     %p5, %rd6, 0;
        @%p5 bra        BB0_11;

        cvta.to.global.u64      %rd26, %rd6;
        st.global.u32   [%rd26], %r1;

BB0_11:
        ret;
}

        // .globl       _ZN3cub11EmptyKernelIvEEvv
.visible .entry _ZN3cub11EmptyKernelIvEEvv(

)
{



        ret;
}

.entry _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0_(
        .param .u32 _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_0,
        .param .u64 _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_1,
        .param .u32 _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_2,
        .param .u32 _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_3,
        .param .u64 _ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_4
)
.maxntid 1024, 1, 1
{
        .reg .pred      %p&amp;lt;13&amp;gt;;
        .reg .b32       %r&amp;lt;90&amp;gt;;
        .reg .b64       %rd&amp;lt;15&amp;gt;;
        // demoted variable
        .shared .align 4 .b8 _ZZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem[128];

        ld.param.u32    %r35, [_ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_0];
        ld.param.u64    %rd4, [_ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_1];
        ld.param.u32    %r38, [_ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_2];
        ld.param.u32    %r39, [_ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_3];
        ld.param.u64    %rd3, [_ZN3wuk33global_reduce_add_strided_batchedIiiiLi1024ELi32EEEviPT_iiPT0__param_4];
        cvta.to.global.u64      %rd1, %rd4;
        mov.u32         %r1, %ctaid.x;
        mul.lo.s32      %r40, %r1, %r39;
        cvt.u64.u32     %rd2, %r40;
        shl.b32         %r2, %r38, 10;
        mov.u32         %r3, %tid.x;
        mov.u32         %r79, 0;
        setp.ge.s32     %p1, %r3, %r35;
        @%p1 bra        BB2_3;

        mov.u32         %r77, %r3;

BB2_2:
        cvt.s64.s32     %rd5, %r77;
        add.s64         %rd6, %rd5, %rd2;
        shl.b64         %rd7, %rd6, 2;
        add.s64         %rd8, %rd1, %rd7;
        ld.global.u32   %r41, [%rd8];
        add.s32         %r79, %r41, %r79;
        add.s32         %r77, %r77, %r2;
        setp.lt.s32     %p2, %r77, %r35;
        @%p2 bra        BB2_2;

BB2_3:
        mov.u32         %r42, -1;
        redux.sync.add.s32 %r9, %r79, %r42;
        and.b32         %r10, %r3, 31;
        setp.ne.s32     %p3, %r10, 0;
        @%p3 bra        BB2_5;

        shr.u32         %r43, %r3, 3;
        and.b32         %r44, %r43, 536870908;
        mov.u32         %r45, _ZZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem;
        add.s32         %r46, %r45, %r44;
        st.shared.u32   [%r46], %r9;

BB2_5:
        bar.sync        0;
        setp.gt.u32     %p4, %r3, 31;
        @%p4 bra        BB2_18;

        mov.u32         %r89, 0;
        setp.gt.s32     %p5, %r3, 31;
        @%p5 bra        BB2_16;

        setp.gt.s32     %p6, %r3, 0;
        mov.u32         %r89, 0;
        add.s32         %r49, %r3, 31;
        selp.b32        %r50, %r49, 31, %p6;
        sub.s32         %r51, %r50, %r3;
        shr.u32         %r52, %r51, 5;
        add.s32         %r11, %r52, 1;
        and.b32         %r12, %r11, 3;
        setp.eq.s32     %p7, %r12, 0;
        mov.u32         %r84, %r3;
        @%p7 bra        BB2_13;

        setp.eq.s32     %p8, %r12, 1;
        mov.u32         %r83, 0;
        mov.u32         %r82, %r3;
        @%p8 bra        BB2_12;

        setp.eq.s32     %p9, %r12, 2;
        mov.u32         %r81, 0;
        mov.u32         %r80, %r3;
        @%p9 bra        BB2_11;

        shl.b32         %r55, %r3, 2;
        mov.u32         %r56, _ZZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem;
        add.s32         %r57, %r56, %r55;
        ld.shared.u32   %r81, [%r57];
        add.s32         %r80, %r3, 32;

BB2_11:
        shl.b32         %r58, %r80, 2;
        mov.u32         %r59, _ZZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem;
        add.s32         %r60, %r59, %r58;
        ld.shared.u32   %r61, [%r60];
        add.s32         %r83, %r61, %r81;
        add.s32         %r82, %r80, 32;

BB2_12:
        shl.b32         %r62, %r82, 2;
        mov.u32         %r63, _ZZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem;
        add.s32         %r64, %r63, %r62;
        ld.shared.u32   %r65, [%r64];
        add.s32         %r89, %r65, %r83;
        add.s32         %r84, %r82, 32;

BB2_13:
        setp.lt.u32     %p10, %r11, 4;
        @%p10 bra       BB2_16;

        add.s32         %r87, %r84, -32;
        shl.b32         %r66, %r84, 2;
        mov.u32         %r67, _ZZN58_INTERNAL_36_tmpxft_00043bfa_00000000_7_a_cpp1_ii_1e97e9b13wuk16block_reduce_addIiiiLi1024ELi32EEEviPT_iPT0_E4smem;
        add.s32         %r86, %r67, %r66;

BB2_15:
        ld.shared.u32   %r68, [%r86];
        add.s32         %r69, %r68, %r89;
        ld.shared.u32   %r70, [%r86+128];
        add.s32         %r71, %r70, %r69;
        ld.shared.u32   %r72, [%r86+256];
        add.s32         %r73, %r72, %r71;
        ld.shared.u32   %r74, [%r86+384];
        add.s32         %r89, %r74, %r73;
        add.s32         %r86, %r86, 512;
        add.s32         %r87, %r87, 128;
        setp.lt.s32     %p11, %r87, 0;
        @%p11 bra       BB2_15;

BB2_16:
        redux.sync.add.s32 %r34, %r89, %r42;
        @%p3 bra        BB2_18;

        shr.u32         %r76, %r3, 5;
        cvt.u64.u32     %rd9, %r76;
        cvt.u64.u32     %rd10, %r1;
        add.s64         %rd11, %rd9, %rd10;
        cvta.to.global.u64      %rd12, %rd3;
        shl.b64         %rd13, %rd11, 2;
        add.s64         %rd14, %rd12, %rd13;
        st.global.u32   [%rd14], %r34;

BB2_18:
        ret;
}
&lt;/code&gt;&lt;/pre&gt;</content><author><name></name></author><category term="CUDA" /><summary type="html">结论</summary></entry><entry><title type="html">osu-micro-benchmarks</title><link href="http://localhost:4000/2021/03/07/osu-micro-benchmarks/" rel="alternate" type="text/html" title="osu-micro-benchmarks" /><published>2021-03-07T00:00:00+08:00</published><updated>2021-03-07T00:00:00+08:00</updated><id>http://localhost:4000/2021/03/07/osu-micro-benchmarks</id><content type="html" xml:base="http://localhost:4000/2021/03/07/osu-micro-benchmarks/">&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;

&lt;p&gt;无意中发现 &lt;a href=&quot;https://github.com/spack/spack/releases/tag/v0.16.1&quot;&gt;spack&lt;/a&gt; 可以直接安装 &lt;a href=&quot;http://mvapich.cse.ohio-state.edu/benchmarks/&quot;&gt;osu-micro-benchmarks&lt;/a&gt; 了，顺手写一个脚本横向比较一下一些常见 mpi 在单机上的性能。&lt;/p&gt;

&lt;h2 id=&quot;实验环境&quot;&gt;实验环境&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;个人开发机一台
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.intel.cn/content/www/cn/zh/products/processors/xeon/scalable/platinum-processors/platinum-8260.html&quot;&gt;Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;我的开发机实例只能使用 8 核 16 线程&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;linux-debian9-cascadelake&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;结果分析&quot;&gt;结果分析&lt;/h2&gt;

&lt;p&gt;log 很多，待分析补充。初步结果：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;openmpi@4.0.5 费拉不堪&lt;/li&gt;
  &lt;li&gt;mpich@3.3.2 很顶&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;运行脚本-run_osush&quot;&gt;运行脚本 &lt;code&gt;run_osu.sh&lt;/code&gt;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;#!/bin/bash
# time ./run_osu.sh &amp;gt; run_osu.log 2&amp;gt;&amp;amp;1

mpis=(
    openmpi@1.10.7%intel@19.0.5.281
    openmpi@2.1.6%intel@19.0.5.281
    openmpi@3.1.6%intel@19.0.5.281
    openmpi@4.0.5%intel@19.0.5.281
    openmpi@1.10.7%gcc@10.2.0
    openmpi@2.1.6%gcc@10.2.0
    openmpi@3.1.6%gcc@10.2.0
    openmpi@4.0.5%gcc@10.2.0
    mpich@3.3.2%gcc@10.2.0
)

osu_startup=(
    osu_hello
    osu_init
)

osu_pt2pt=(
    osu_bibw
    osu_latency
    osu_latency_mt
    osu_multi_lat
    osu_bw
    osu_latency_mp
    osu_mbw_mr
)

osu_collective=(
    osu_allgather
    osu_bcast
    osu_ialltoall
    osu_igatherv
    osu_scatter
    osu_allgatherv
    osu_gather
    osu_ialltoallv
    osu_ireduce
    osu_scatterv
    osu_allreduce
    osu_gatherv
    osu_ialltoallw
    osu_iscatter
    osu_alltoall
    osu_iallgather
    osu_ibarrier
    osu_iscatterv
    osu_alltoallv
    osu_iallgatherv
    osu_ibcast
    osu_reduce
    osu_barrier
    osu_iallreduce
    osu_igather
    osu_reduce_scatter
)

osu_oneSided=(
    osu_acc_latency
    osu_fop_latency
    osu_get_bw
    osu_put_bibw
    osu_put_latency
    osu_cas_latency
    osu_get_acc_latency
    osu_get_latency
    osu_put_bw
)

cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c

for mpi in &quot;${mpis[@]}&quot;; do
    spack unload -a
    spack install osu-micro-benchmarks ^ $mpi
done

for mpi in &quot;${mpis[@]}&quot;; do
    spack unload -a
    spack load osu-micro-benchmarks ^ $mpi
    spack find --loaded
    for benchmark in &quot;${osu_startup[@]}&quot;; do
        echo &quot;$mpi $benchmark&quot;
        mpiexec -n 2 $benchmark
    done
    for benchmark in &quot;${osu_pt2pt[@]}&quot;; do
        echo &quot;$mpi $benchmark&quot;
        mpiexec -n 2 $benchmark
    done
    for benchmark in &quot;${osu_collective[@]}&quot;; do
        echo &quot;$mpi $benchmark&quot;
        mpiexec -n 8 $benchmark
    done
    for benchmark in &quot;${osu_oneSided[@]}&quot;; do
        echo &quot;$mpi $benchmark&quot;
        mpiexec -n 2 $benchmark
    done
done
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;运行结果-run_osulog&quot;&gt;运行结果 &lt;code&gt;run_osu.log&lt;/code&gt;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;      8  Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libsigsegv-2.12-r3trcwozyk2v3lalhsjmncp26c2koqzn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/pkgconf-1.7.3-pzvmn24duawmwpx4rf2zcdq3dzvga54v
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/util-macros-1.19.1-z2kbfmkqxvwvdygxi7jvweix5sibk5ee
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libiconv-1.16-d2nsvkkfuq4pv2kzvosj53635vfbxema
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/xz-5.2.5-kwbssn3wsll7firxf7bmum3iezxu6yfd
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/zlib-1.2.11-v3uqb2f7txa5dgycabkbbpxscflzxjqy
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/berkeley-db-18.1.40-2snibmpw7eqeowr7qpuszgczav3fapcj
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/m4-1.4.18-nbbveg7v7fw7jrnchjgolyr4zikszko7
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/ncurses-6.2-ikon6r6rtuihduraz2ht65yixb55orhz
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libxml2-2.9.10-oloe4u365xzmzgoiw2x7pylu6iqs5bhz
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libtool-2.4.6-hiijdgc6r4mhtmnuciqgqh6xvjpw756n
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/readline-8.0-ct3r7lca37pwvipcnjmgn67r25ltz2ey
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libpciaccess-0.16-5cjnzd7l4tm4u7shpgdnv7epqpv2fyvm
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/gdbm-1.18.1-oifw6abhztgoqx6jhl6ekuud4g35d2tc
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/perl-5.32.0-rudsi7op6rwzgzsqoiya3a7tsrfumprn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/autoconf-2.69-4okdjloy7ew7k7vzajs4dizv433szapy
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/automake-1.16.2-5wdooseikitrp3g3sg65almbetg22mdb
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/numactl-2.0.14-djhfey4odgqq6kpnek4axqgxx6x6sepk
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/hwloc-1.11.11-grb2pwz4kcur725ue3rhbakair7kdy2x
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/openmpi-1.10.7-6ikpb5tdr4bfe327ei63rbofrvipw2ez
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/osu-micro-benchmarks-5.6.3-h5bo6bh4x3jz5f4advxg6ihrpy7e6kgi
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libsigsegv-2.12-r3trcwozyk2v3lalhsjmncp26c2koqzn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/pkgconf-1.7.3-pzvmn24duawmwpx4rf2zcdq3dzvga54v
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/util-macros-1.19.1-z2kbfmkqxvwvdygxi7jvweix5sibk5ee
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libiconv-1.16-d2nsvkkfuq4pv2kzvosj53635vfbxema
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/xz-5.2.5-kwbssn3wsll7firxf7bmum3iezxu6yfd
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/zlib-1.2.11-v3uqb2f7txa5dgycabkbbpxscflzxjqy
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/berkeley-db-18.1.40-2snibmpw7eqeowr7qpuszgczav3fapcj
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/m4-1.4.18-nbbveg7v7fw7jrnchjgolyr4zikszko7
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/ncurses-6.2-ikon6r6rtuihduraz2ht65yixb55orhz
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libxml2-2.9.10-oloe4u365xzmzgoiw2x7pylu6iqs5bhz
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libtool-2.4.6-hiijdgc6r4mhtmnuciqgqh6xvjpw756n
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/readline-8.0-ct3r7lca37pwvipcnjmgn67r25ltz2ey
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libpciaccess-0.16-5cjnzd7l4tm4u7shpgdnv7epqpv2fyvm
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/gdbm-1.18.1-oifw6abhztgoqx6jhl6ekuud4g35d2tc
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/perl-5.32.0-rudsi7op6rwzgzsqoiya3a7tsrfumprn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/autoconf-2.69-4okdjloy7ew7k7vzajs4dizv433szapy
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/automake-1.16.2-5wdooseikitrp3g3sg65almbetg22mdb
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/numactl-2.0.14-djhfey4odgqq6kpnek4axqgxx6x6sepk
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/hwloc-1.11.11-grb2pwz4kcur725ue3rhbakair7kdy2x
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/openmpi-2.1.6-q5d7vtlfrgekmwlkwkb22ogs523jczfj
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/osu-micro-benchmarks-5.6.3-wtzerrmxciy6stq4ydmegohq245uzpf3
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libsigsegv-2.12-r3trcwozyk2v3lalhsjmncp26c2koqzn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/pkgconf-1.7.3-pzvmn24duawmwpx4rf2zcdq3dzvga54v
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/util-macros-1.19.1-z2kbfmkqxvwvdygxi7jvweix5sibk5ee
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libiconv-1.16-d2nsvkkfuq4pv2kzvosj53635vfbxema
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/xz-5.2.5-kwbssn3wsll7firxf7bmum3iezxu6yfd
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/zlib-1.2.11-v3uqb2f7txa5dgycabkbbpxscflzxjqy
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/berkeley-db-18.1.40-2snibmpw7eqeowr7qpuszgczav3fapcj
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/m4-1.4.18-nbbveg7v7fw7jrnchjgolyr4zikszko7
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/ncurses-6.2-ikon6r6rtuihduraz2ht65yixb55orhz
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libxml2-2.9.10-oloe4u365xzmzgoiw2x7pylu6iqs5bhz
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libtool-2.4.6-hiijdgc6r4mhtmnuciqgqh6xvjpw756n
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/readline-8.0-ct3r7lca37pwvipcnjmgn67r25ltz2ey
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libpciaccess-0.16-5cjnzd7l4tm4u7shpgdnv7epqpv2fyvm
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/gdbm-1.18.1-oifw6abhztgoqx6jhl6ekuud4g35d2tc
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/perl-5.32.0-rudsi7op6rwzgzsqoiya3a7tsrfumprn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/autoconf-2.69-4okdjloy7ew7k7vzajs4dizv433szapy
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/automake-1.16.2-5wdooseikitrp3g3sg65almbetg22mdb
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/numactl-2.0.14-djhfey4odgqq6kpnek4axqgxx6x6sepk
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/hwloc-1.11.11-grb2pwz4kcur725ue3rhbakair7kdy2x
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/openmpi-3.1.6-qpo7jpiiw22n7o5vpwp7sniyqbld4wgc
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/osu-micro-benchmarks-5.6.3-5p46zg7fdfs7dvhjwr4ntnzu5ib34jh7
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libsigsegv-2.12-r3trcwozyk2v3lalhsjmncp26c2koqzn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/pkgconf-1.7.3-pzvmn24duawmwpx4rf2zcdq3dzvga54v
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/util-macros-1.19.1-z2kbfmkqxvwvdygxi7jvweix5sibk5ee
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libiconv-1.16-d2nsvkkfuq4pv2kzvosj53635vfbxema
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/xz-5.2.5-kwbssn3wsll7firxf7bmum3iezxu6yfd
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/zlib-1.2.11-v3uqb2f7txa5dgycabkbbpxscflzxjqy
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/berkeley-db-18.1.40-2snibmpw7eqeowr7qpuszgczav3fapcj
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/m4-1.4.18-nbbveg7v7fw7jrnchjgolyr4zikszko7
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/ncurses-6.2-ikon6r6rtuihduraz2ht65yixb55orhz
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libxml2-2.9.10-oloe4u365xzmzgoiw2x7pylu6iqs5bhz
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libtool-2.4.6-hiijdgc6r4mhtmnuciqgqh6xvjpw756n
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/readline-8.0-ct3r7lca37pwvipcnjmgn67r25ltz2ey
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/libpciaccess-0.16-5cjnzd7l4tm4u7shpgdnv7epqpv2fyvm
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/gdbm-1.18.1-oifw6abhztgoqx6jhl6ekuud4g35d2tc
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/hwloc-2.2.0-qby26sw44d3h4py2b3xtciaxjtkjb5gp
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/perl-5.32.0-rudsi7op6rwzgzsqoiya3a7tsrfumprn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/autoconf-2.69-4okdjloy7ew7k7vzajs4dizv433szapy
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/automake-1.16.2-5wdooseikitrp3g3sg65almbetg22mdb
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/numactl-2.0.14-djhfey4odgqq6kpnek4axqgxx6x6sepk
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/openmpi-4.0.5-txaw3t3jdubmua2lhalkcoz363k77jbv
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/intel-19.0.5.281/osu-micro-benchmarks-5.6.3-xckp2qje7tq7dazshqbk7hgkthji5zpf
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libsigsegv-2.12-revfghrsqkagjs2xadvsrf2iga5fcj6k
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/pkgconf-1.7.3-w463hvpx56ry72lwjblgfkbixzn375tn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/util-macros-1.19.1-myitxdfhruqmh2h2rvzxic6vk7c7evic
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libiconv-1.16-wfcdbcqmdtjuwo7l3j7bcmmhfx5blylj
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/xz-5.2.5-speqt5b2xkvwuvkm3cpuwg2l3fwbcfok
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/zlib-1.2.11-vfj3uem6ngffcbvs55jfuq3gjvwhaq4q
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/berkeley-db-18.1.40-uvbl62lsobhamxciclelykwmswoyuf5h
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/m4-1.4.18-j6vmwnc2anbdreog273n7jgcehicq37r
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/ncurses-6.2-szheta7z7xrjst2ogjvzodrsoaip3l2j
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libxml2-2.9.10-6kl5wqn7g76swqsyr5gd6mpfcywpha2i
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libtool-2.4.6-22wvtlj36fltcv2x4goef2rl63f22bqq
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/readline-8.0-qpmephgn35osuuadod2hv3uksunmnqzh
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libpciaccess-0.16-26l4fb7klvyexldxnxtbhtcqmsp5zh2w
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/gdbm-1.18.1-4e2l4ueetcwjqekofilvlq7qlyxanfvd
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/perl-5.32.0-t66jap6sniuyncgvv55grfs3p7pofqyc
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/autoconf-2.69-4aiqamfnwmgvzo7qwolgmngqbjc5bqdv
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/automake-1.16.2-uobg65rv7v2eeskllx7gv6f6x3yovv3p
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/numactl-2.0.14-t4bpx4kxudgjftlpckmo6pnstk6jivme
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/hwloc-1.11.11-hjh3jhuzfhxfo5ubtbl3y44wgb72ktjv
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/openmpi-1.10.7-u3vbsyqj2zdrqhcqojj7l3tq4gvoog46
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/osu-micro-benchmarks-5.6.3-xbphx6dnyradx7ksyooz37ynwyznvkic
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libsigsegv-2.12-revfghrsqkagjs2xadvsrf2iga5fcj6k
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/pkgconf-1.7.3-w463hvpx56ry72lwjblgfkbixzn375tn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/util-macros-1.19.1-myitxdfhruqmh2h2rvzxic6vk7c7evic
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libiconv-1.16-wfcdbcqmdtjuwo7l3j7bcmmhfx5blylj
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/xz-5.2.5-speqt5b2xkvwuvkm3cpuwg2l3fwbcfok
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/zlib-1.2.11-vfj3uem6ngffcbvs55jfuq3gjvwhaq4q
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/berkeley-db-18.1.40-uvbl62lsobhamxciclelykwmswoyuf5h
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/m4-1.4.18-j6vmwnc2anbdreog273n7jgcehicq37r
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/ncurses-6.2-szheta7z7xrjst2ogjvzodrsoaip3l2j
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libxml2-2.9.10-6kl5wqn7g76swqsyr5gd6mpfcywpha2i
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libtool-2.4.6-22wvtlj36fltcv2x4goef2rl63f22bqq
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/readline-8.0-qpmephgn35osuuadod2hv3uksunmnqzh
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libpciaccess-0.16-26l4fb7klvyexldxnxtbhtcqmsp5zh2w
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/gdbm-1.18.1-4e2l4ueetcwjqekofilvlq7qlyxanfvd
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/perl-5.32.0-t66jap6sniuyncgvv55grfs3p7pofqyc
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/autoconf-2.69-4aiqamfnwmgvzo7qwolgmngqbjc5bqdv
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/automake-1.16.2-uobg65rv7v2eeskllx7gv6f6x3yovv3p
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/numactl-2.0.14-t4bpx4kxudgjftlpckmo6pnstk6jivme
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/hwloc-1.11.11-hjh3jhuzfhxfo5ubtbl3y44wgb72ktjv
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/openmpi-2.1.6-p5uipts4jdye5xyjxsww24mg4srwunyx
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/osu-micro-benchmarks-5.6.3-p7xodeq2xiuc5xk53u55ltnu7ot26umf
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libsigsegv-2.12-revfghrsqkagjs2xadvsrf2iga5fcj6k
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/pkgconf-1.7.3-w463hvpx56ry72lwjblgfkbixzn375tn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/util-macros-1.19.1-myitxdfhruqmh2h2rvzxic6vk7c7evic
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libiconv-1.16-wfcdbcqmdtjuwo7l3j7bcmmhfx5blylj
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/xz-5.2.5-speqt5b2xkvwuvkm3cpuwg2l3fwbcfok
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/zlib-1.2.11-vfj3uem6ngffcbvs55jfuq3gjvwhaq4q
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/berkeley-db-18.1.40-uvbl62lsobhamxciclelykwmswoyuf5h
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/m4-1.4.18-j6vmwnc2anbdreog273n7jgcehicq37r
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/ncurses-6.2-szheta7z7xrjst2ogjvzodrsoaip3l2j
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libxml2-2.9.10-6kl5wqn7g76swqsyr5gd6mpfcywpha2i
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libtool-2.4.6-22wvtlj36fltcv2x4goef2rl63f22bqq
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/readline-8.0-qpmephgn35osuuadod2hv3uksunmnqzh
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libpciaccess-0.16-26l4fb7klvyexldxnxtbhtcqmsp5zh2w
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/gdbm-1.18.1-4e2l4ueetcwjqekofilvlq7qlyxanfvd
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/perl-5.32.0-t66jap6sniuyncgvv55grfs3p7pofqyc
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/autoconf-2.69-4aiqamfnwmgvzo7qwolgmngqbjc5bqdv
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/automake-1.16.2-uobg65rv7v2eeskllx7gv6f6x3yovv3p
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/numactl-2.0.14-t4bpx4kxudgjftlpckmo6pnstk6jivme
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/hwloc-1.11.11-hjh3jhuzfhxfo5ubtbl3y44wgb72ktjv
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/openmpi-3.1.6-hzkz7tjt4rjde65n5vo24rjbq5qpppcs
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/osu-micro-benchmarks-5.6.3-vecfeq3rd7tvjtnwa6esg4q7yqxdhvey
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libsigsegv-2.12-revfghrsqkagjs2xadvsrf2iga5fcj6k
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/pkgconf-1.7.3-w463hvpx56ry72lwjblgfkbixzn375tn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/util-macros-1.19.1-myitxdfhruqmh2h2rvzxic6vk7c7evic
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libiconv-1.16-wfcdbcqmdtjuwo7l3j7bcmmhfx5blylj
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/xz-5.2.5-speqt5b2xkvwuvkm3cpuwg2l3fwbcfok
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/zlib-1.2.11-vfj3uem6ngffcbvs55jfuq3gjvwhaq4q
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/berkeley-db-18.1.40-uvbl62lsobhamxciclelykwmswoyuf5h
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/m4-1.4.18-j6vmwnc2anbdreog273n7jgcehicq37r
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/ncurses-6.2-szheta7z7xrjst2ogjvzodrsoaip3l2j
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libxml2-2.9.10-6kl5wqn7g76swqsyr5gd6mpfcywpha2i
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libtool-2.4.6-22wvtlj36fltcv2x4goef2rl63f22bqq
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/readline-8.0-qpmephgn35osuuadod2hv3uksunmnqzh
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libpciaccess-0.16-26l4fb7klvyexldxnxtbhtcqmsp5zh2w
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/gdbm-1.18.1-4e2l4ueetcwjqekofilvlq7qlyxanfvd
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/hwloc-2.2.0-mm3sjtd27gfjnuzrx6btdwkdnqmx5tal
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/perl-5.32.0-t66jap6sniuyncgvv55grfs3p7pofqyc
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/autoconf-2.69-4aiqamfnwmgvzo7qwolgmngqbjc5bqdv
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/automake-1.16.2-uobg65rv7v2eeskllx7gv6f6x3yovv3p
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/numactl-2.0.14-t4bpx4kxudgjftlpckmo6pnstk6jivme
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/openmpi-4.0.5-xwzfomywgk3vthdg77ik5kjr64qpbvau
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/osu-micro-benchmarks-5.6.3-222sfjueqic4u4bravosklinqugztnhk
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libsigsegv-2.12-revfghrsqkagjs2xadvsrf2iga5fcj6k
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/berkeley-db-18.1.40-uvbl62lsobhamxciclelykwmswoyuf5h
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/pkgconf-1.7.3-w463hvpx56ry72lwjblgfkbixzn375tn
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/util-macros-1.19.1-myitxdfhruqmh2h2rvzxic6vk7c7evic
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libiconv-1.16-wfcdbcqmdtjuwo7l3j7bcmmhfx5blylj
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/xz-5.2.5-speqt5b2xkvwuvkm3cpuwg2l3fwbcfok
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/zlib-1.2.11-vfj3uem6ngffcbvs55jfuq3gjvwhaq4q
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/m4-1.4.18-j6vmwnc2anbdreog273n7jgcehicq37r
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/ncurses-6.2-szheta7z7xrjst2ogjvzodrsoaip3l2j
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libxml2-2.9.10-6kl5wqn7g76swqsyr5gd6mpfcywpha2i
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libtool-2.4.6-22wvtlj36fltcv2x4goef2rl63f22bqq
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/readline-8.0-qpmephgn35osuuadod2hv3uksunmnqzh
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/libpciaccess-0.16-26l4fb7klvyexldxnxtbhtcqmsp5zh2w
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/gdbm-1.18.1-4e2l4ueetcwjqekofilvlq7qlyxanfvd
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/hwloc-2.2.0-mm3sjtd27gfjnuzrx6btdwkdnqmx5tal
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/perl-5.32.0-t66jap6sniuyncgvv55grfs3p7pofqyc
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/texinfo-6.5-3fryou4wi7d2zoa7uepcc5xojkeo3bej
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/autoconf-2.69-4aiqamfnwmgvzo7qwolgmngqbjc5bqdv
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/automake-1.16.2-uobg65rv7v2eeskllx7gv6f6x3yovv3p
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/findutils-4.6.0-i6lbkjedtdenw7nb3ozulkco5ngnouv4
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/mpich-3.3.2-mcjsx53qbyta5h4dif4tukljuxkb2wsb
[+] /data00/home/wukan.i/spack-0.16.1/opt/spack/linux-debian9-cascadelake/gcc-10.2.0/osu-micro-benchmarks-5.6.3-tn7hdjei4w7hxthtooucgabxu4asjbvw
-- linux-debian9-cascadelake / intel@19.0.5.281 -----------------
hwloc@1.11.11
libiconv@1.16
libpciaccess@0.16
libxml2@2.9.10
numactl@2.0.14
openmpi@1.10.7
osu-micro-benchmarks@5.6.3
xz@5.2.5
zlib@1.2.11
openmpi@1.10.7%intel@19.0.5.281 osu_hello
# OSU MPI Hello World Test v5.6.3
This is a test with 2 processes
openmpi@1.10.7%intel@19.0.5.281 osu_init
# OSU MPI Init Test v5.6.3
nprocs: 2, min: 13 ms, max: 14 ms, avg: 13 ms
openmpi@1.10.7%intel@19.0.5.281 osu_bibw
# OSU MPI Bi-Directional Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       9.42
2                      20.08
4                      33.78
8                      80.40
16                    150.75
32                    317.69
64                    386.16
128                   532.48
256                   920.94
512                  1698.04
1024                 3041.29
2048                 5283.63
4096                 5660.52
8192                 7977.62
16384                9625.81
32768                9567.82
65536                9906.10
131072              10027.46
262144              10244.80
524288               9908.48
1048576              9974.68
2097152              9676.33
4194304              9145.41
openmpi@1.10.7%intel@19.0.5.281 osu_latency
# OSU MPI Latency Test v5.6.3
# Size          Latency (us)
0                       0.20
1                       0.24
2                       0.23
4                       0.23
8                       0.24
16                      0.24
32                      0.24
64                      0.26
128                     0.31
256                     0.38
512                     0.48
1024                    0.64
2048                    0.88
4096                    1.48
8192                    2.31
16384                   4.54
32768                   7.13
65536                  10.27
131072                 16.56
262144                 29.68
524288                 56.79
1048576               122.20
2097152               250.83
4194304               525.00
openmpi@1.10.7%intel@19.0.5.281 osu_latency_mt
MPI_Init_thread must return MPI_THREAD_MULTIPLE!
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[153,1],1]
  Exit code:    1
--------------------------------------------------------------------------
openmpi@1.10.7%intel@19.0.5.281 osu_multi_lat
# OSU MPI Multi Latency Test v5.6.3
# Size          Latency (us)
0                       0.21
1                       0.23
2                       0.23
4                       0.23
8                       0.23
16                      0.23
32                      0.23
64                      0.25
128                     0.31
256                     0.38
512                     0.50
1024                    0.65
2048                    0.87
4096                    1.52
8192                    2.32
16384                   4.57
32768                   7.15
65536                  10.36
131072                 16.81
262144                 32.49
524288                 63.74
1048576               121.11
2097152               252.56
4194304               525.35
openmpi@1.10.7%intel@19.0.5.281 osu_bw
# OSU MPI Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       8.92
2                      16.97
4                      36.88
8                      61.22
16                    144.04
32                    286.43
64                    386.88
128                   520.04
256                   821.23
512                  1409.15
1024                 2758.85
2048                 5420.53
4096                 6085.26
8192                 7845.11
16384                8840.87
32768                9351.03
65536                9649.71
131072               9587.43
262144               9632.31
524288               9584.70
1048576              9614.89
2097152              9322.57
4194304              8921.37
openmpi@1.10.7%intel@19.0.5.281 osu_latency_mp
# OSU MPI Multi-process Latency Test v5.6.3
# Number of forked processes in sender: 2
# Number of forked processes in receiver: 2
# Size          Latency (us)
0                       0.22
1                       0.26
2                       0.25
4                       0.28
8                       0.25
16                      0.25
32                      0.22
64                      0.23
128                     0.29
256                     0.36
512                     0.49
1024                    0.64
2048                    0.87
4096                    1.50
8192                    2.28
16384                   4.50
32768                   7.10
65536                  10.14
131072                 16.44
262144                 29.38
524288                 56.57
1048576               119.65
2097152               248.95
4194304               540.59
openmpi@1.10.7%intel@19.0.5.281 osu_mbw_mr
# OSU MPI Multiple Bandwidth / Message Rate Test v5.6.3
# [ pairs: 1 ] [ window size: 64 ]
# Size                  MB/s        Messages/s
1                       8.35        8346864.13
2                      17.61        8805275.16
4                      29.66        7414667.51
8                      73.66        9207798.75
16                    140.84        8802393.42
32                    274.37        8574203.61
64                    376.18        5877755.78
128                   517.71        4044635.19
256                   749.77        2928774.99
512                  1377.88        2691173.16
1024                 2747.96        2683558.53
2048                 5127.55        2503688.04
4096                 5960.93        1455304.52
8192                 7847.60         957958.64
16384                8476.67         517374.61
32768                9388.72         286521.12
65536                9338.38         142492.41
131072               9676.70          73827.34
262144               9296.05          35461.63
524288               9492.21          18104.96
1048576              9155.79           8731.64
2097152              9513.54           4536.41
4194304              9326.06           2223.51
openmpi@1.10.7%intel@19.0.5.281 osu_allgather

# OSU MPI Allgather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       1.60
2                       1.62
4                       1.43
8                       1.55
16                      1.51
32                      1.65
64                      4.07
128                     1.98
256                     2.44
512                     3.10
1024                    4.60
2048                    7.10
4096                   11.92
8192                   22.03
16384                  32.21
32768                  57.72
65536                 110.56
131072                205.14
262144                442.67
524288               1134.68
1048576              5576.45
openmpi@1.10.7%intel@19.0.5.281 osu_bcast

# OSU MPI Broadcast Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.79
2                       0.79
4                       0.79
8                       0.84
16                      0.83
32                      0.86
64                      0.96
128                     0.94
256                     1.06
512                     1.44
1024                    1.84
2048                    2.60
4096                    4.02
8192                    7.83
16384                  14.24
32768                  27.59
65536                  48.60
131072                 86.81
262144                162.66
524288                258.93
1048576               548.23
openmpi@1.10.7%intel@19.0.5.281 osu_ialltoall

# OSU MPI Non-blocking All-to-All Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      11.09              6.10              5.18              3.46
2                      10.87              6.04              5.10              5.46
4                       9.98              5.87              4.93             16.62
8                       9.10              4.76              3.97              0.00
16                      9.42              5.01              4.16              0.00
32                      9.71              5.05              4.19              0.00
64                      9.50              5.10              4.23              0.00
128                     9.49              5.07              4.28              0.00
256                    10.33              5.45              4.55              0.00
512                    12.08              6.56              5.53              0.10
1024                   18.75              9.11              7.89              0.00
2048                   21.42             11.35              9.82              0.00
4096                   46.55             21.84             18.72              0.00
8192                   67.48             36.68             31.80              3.13
16384                 106.60             47.54             40.09              0.00
32768                 164.22             79.48             69.15              0.00
65536                 541.73            208.43            157.42              0.00
131072               1267.14            617.06            530.06              0.00
262144               2190.84           1104.64            986.49              0.00
524288               4192.84           2233.96           1987.84              1.46
1048576              8597.34           4389.34           3894.26              0.00
openmpi@1.10.7%intel@19.0.5.281 osu_igatherv

# OSU MPI Non-blocking Gatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.92              1.58              1.08              0.00
2                       2.91              1.62              1.12              0.00
4                       2.80              1.53              1.04              0.00
8                       2.41              1.36              0.91              0.00
16                      2.32              1.29              0.83              0.00
32                      2.31              1.23              0.80              0.00
64                      2.35              1.25              0.84              0.00
128                     2.49              1.31              0.86              0.00
256                     2.44              1.31              0.91              0.00
512                     2.64              1.36              1.03              0.00
1024                    3.34              1.82              1.41              0.00
2048                    3.95              2.14              1.57              0.00
4096                   18.25              7.39              6.32              0.00
8192                   29.20             13.17             11.38              0.00
16384                  45.61             21.16             18.53              0.00
32768                  62.54             27.95             24.62              0.00
65536                 103.82             44.67             39.54              0.00
131072                212.45             95.59             84.91              0.00
262144                428.28            177.30            158.19              0.00
524288                890.53            435.86            388.23              0.00
1048576              2002.43            965.08            857.64              0.00
openmpi@1.10.7%intel@19.0.5.281 osu_scatter

# OSU MPI Scatter Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.86
2                       0.81
4                       0.84
8                       0.88
16                      0.85
32                      0.84
64                      0.79
128                     0.89
256                     1.07
512                     1.49
1024                    1.70
2048                    3.15
4096                    7.81
8192                   10.94
16384                  22.71
32768                  33.33
65536                  48.98
131072                 90.33
262144                192.49
524288                355.85
1048576               874.89
openmpi@1.10.7%intel@19.0.5.281 osu_allgatherv

# OSU MPI Allgatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       4.14
2                       4.09
4                       7.65
8                      10.06
16                      4.02
32                      7.18
64                      5.82
128                    17.03
256                    11.77
512                     6.62
1024                    8.48
2048                   23.91
4096                   30.42
8192                   38.63
16384                  32.12
32768                  53.87
65536                 216.94
131072                368.91
262144                712.55
524288               2185.79
1048576              5003.20
openmpi@1.10.7%intel@19.0.5.281 osu_gather

# OSU MPI Gather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.40
2                       0.36
4                       0.37
8                       0.35
16                      0.35
32                      0.35
64                      0.35
128                     0.49
256                     0.44
512                     0.59
1024                    0.74
2048                    1.08
4096                    8.83
8192                   12.21
16384                  18.01
32768                  23.68
65536                  35.12
131072                 82.33
262144                135.84
524288                245.08
1048576               542.83
openmpi@1.10.7%intel@19.0.5.281 osu_ialltoallv

# OSU MPI Non-blocking All-to-Allv Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      10.54              5.85              4.89              4.31
2                      10.47              5.80              4.95              5.65
4                       9.63              4.88              4.05              0.00
8                       9.39              5.37              4.46              9.80
16                      8.97              4.86              3.94              0.00
32                      9.01              4.83              4.01              0.00
64                      9.26              4.97              4.16              0.00
128                    10.43              5.91              4.94              8.47
256                    10.56              5.29              4.42              0.00
512                    12.34              6.60              5.59              0.00
1024                   17.41              9.14              7.85              0.00
2048                   22.69             12.42             10.25              0.00
4096                   41.66             23.11             20.29              8.59
8192                   60.95             32.72             28.83              2.08
16384                  79.61             43.10             38.14              4.29
32768                 128.40             66.88             59.10              0.00
65536                 576.70            175.08            128.52              0.00
131072               1371.20            739.07            626.76              0.00
262144               1976.58           1212.74           1079.89             29.27
524288               3956.76           2099.62           1871.22              0.75
1048576              8450.06           4305.06           3833.01              0.00
openmpi@1.10.7%intel@19.0.5.281 osu_ireduce

# OSU MPI Non-blocking Reduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       3.93              2.28              1.73              4.88
8                       3.52              2.10              1.64             13.56
16                      3.41              1.88              1.43              0.00
32                      3.34              1.90              1.44              0.00
64                      3.52              1.99              1.50              0.00
128                     3.77              2.08              1.57              0.00
256                     3.81              2.16              1.68              1.86
512                     4.83              2.69              2.14              0.00
1024                    5.65              3.26              2.57              7.36
2048                    7.52              4.06              3.35              0.00
4096                   17.31              6.78              5.79              0.00
8192                   25.36              9.96              8.52              0.00
16384                  47.09             18.10             15.78              0.00
32768                  76.83             29.18             25.74              0.00
65536                 148.48             46.05             40.78              0.00
131072                257.05             99.04             88.11              0.00
262144                414.28            147.08            131.27              0.00
524288                863.28            311.29            277.12              0.00
1048576              1690.76            618.06            550.07              0.00
openmpi@1.10.7%intel@19.0.5.281 osu_scatterv

# OSU MPI Scatterv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.67
2                       0.67
4                       0.70
8                       0.66
16                      0.75
32                      0.73
64                      0.74
128                     0.83
256                     1.04
512                     1.47
1024                    1.73
2048                    2.43
4096                    6.04
8192                   10.92
16384                  21.20
32768                  32.68
65536                  49.43
131072                 99.74
262144                182.58
524288                352.40
1048576               728.52
openmpi@1.10.7%intel@19.0.5.281 osu_allreduce

# OSU MPI Allreduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       1.48
8                       1.40
16                      1.46
32                      1.57
64                      1.56
128                     1.80
256                     2.00
512                     2.79
1024                    4.12
2048                    6.01
4096                   11.00
8192                   17.69
16384                  23.41
32768                  44.13
65536                  72.27
131072                115.26
262144                199.46
524288                356.94
1048576               927.10
openmpi@1.10.7%intel@19.0.5.281 osu_gatherv

# OSU MPI Gatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.30
2                       0.26
4                       0.25
8                       0.27
16                      0.27
32                      0.25
64                      0.27
128                     0.34
256                     0.31
512                     0.47
1024                    0.71
2048                    1.17
4096                    8.73
8192                   14.93
16384                  22.23
32768                  34.56
65536                  49.44
131072                 85.06
262144                160.21
524288                332.43
1048576               671.25
openmpi@1.10.7%intel@19.0.5.281 osu_ialltoallw

# OSU MPI Non-blocking All-to-Allw Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      11.19              5.98              5.10              0.00
2                      10.45              5.58              4.76              0.00
4                      10.83              5.75              4.81              0.00
8                       9.98              5.69              4.80             10.61
16                      9.71              5.46              4.50              5.57
32                      8.92              4.75              3.97              0.00
64                      9.41              5.10              4.34              0.66
128                     9.50              5.07              4.25              0.00
256                    10.26              5.40              4.52              0.00
512                    12.38              6.60              5.66              0.00
1024                   17.38              9.26              7.96              0.00
2048                   21.54             11.47              9.94              0.00
4096                   37.92             20.30             17.83              1.15
8192                   57.64             29.87             26.27              0.00
16384                  80.81             42.96             37.94              0.24
32768                 152.10             72.71             64.03              0.00
65536                 260.58            135.93            120.27              0.00
131072                585.59            292.20            260.44              0.00
262144               1756.00            905.42            804.74              0.00
524288               3991.15           2126.31           1892.11              1.44
1048576              8679.36           4677.81           4159.59              3.80
openmpi@1.10.7%intel@19.0.5.281 osu_iscatter

# OSU MPI Non-blocking Scatter Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.24              2.17              1.69             36.85
2                       3.21              2.13              1.68             35.64
4                       3.21              2.15              1.68             36.78
8                       3.37              2.28              1.72             36.81
16                      3.39              2.31              1.73             37.55
32                      3.43              2.33              1.81             38.87
64                      3.50              2.38              1.85             39.70
128                     3.70              2.58              2.03             44.89
256                     3.86              2.72              2.17             47.03
512                     4.64              3.35              2.67             51.77
1024                    5.09              3.52              2.87             45.54
2048                    6.13              4.39              3.71             52.92
4096                   15.58              8.09              6.97              0.00
8192                   22.74             11.51              9.86              0.00
16384                  35.23             16.38             14.18              0.00
32768                  58.32             26.25             23.14              0.00
65536                 104.35             45.76             40.43              0.00
131072                276.12            137.33            122.27              0.00
262144                577.57            288.41            257.00              0.00
524288               1201.28            623.50            548.46              0.00
1048576              2569.87           1280.91           1143.14              0.00
openmpi@1.10.7%intel@19.0.5.281 osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       2.88
2                       2.90
4                       2.96
8                       2.74
16                      2.32
32                      2.28
64                      2.44
128                     2.53
256                     2.81
512                     3.40
1024                    5.53
2048                    9.58
4096                   18.80
8192                   31.44
16384                  45.51
32768                  71.15
65536                 128.45
131072                355.01
262144                730.43
524288               1976.52
1048576              6609.03
openmpi@1.10.7%intel@19.0.5.281 osu_iallgather

# OSU MPI Non-blocking Allgather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      21.44             14.81              9.27             28.47
2                      14.01              9.72              8.28             48.18
4                      17.44              4.89              3.95              0.00
8                      20.47             11.12              7.79              0.00
16                     12.96              6.10              4.13              0.00
32                     19.05             14.78             12.17             64.93
64                      9.40              5.03              4.20              0.00
128                    14.00              9.02              5.44              8.42
256                    28.65             17.04             13.03             10.97
512                    41.97             27.06             17.46             14.64
1024                  156.27             26.04             19.92              0.00
2048                   23.96             13.32             11.34              6.14
4096                  104.90             45.83             36.64              0.00
8192                   95.96             46.13             38.00              0.00
16384                 102.89             45.36             37.85              0.00
32768                 154.73             69.14             61.25              0.00
65536                 389.83            203.02            173.87              0.00
131072                547.34            288.64            255.50              0.00
262144               1088.86            551.14            489.25              0.00
524288               2942.49           1423.03           1271.26              0.00
1048576              6720.63           3545.93           3166.01              0.00
openmpi@1.10.7%intel@19.0.5.281 osu_ibarrier

# OSU MPI Non-blocking Barrier Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

       Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
              8.85              5.04              4.02              5.04
openmpi@1.10.7%intel@19.0.5.281 osu_iscatterv

# OSU MPI Non-blocking Scatterv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.40              2.27              1.72             34.40
2                       3.40              2.27              1.69             33.18
4                       3.58              2.20              1.70             18.76
8                       3.46              2.30              1.71             32.09
16                      3.42              2.27              1.79             35.93
32                      3.69              2.51              1.95             39.83
64                      3.94              2.70              2.16             42.70
128                     3.93              2.70              2.15             42.56
256                     4.34              3.05              2.46             47.48
512                     5.28              3.80              3.10             52.35
1024                    5.52              3.75              3.12             43.14
2048                    6.79              4.71              3.90             46.67
4096                   19.31              9.73              8.37              0.00
8192                   26.74             13.01             11.22              0.00
16384                  41.20             18.62             16.25              0.00
32768                  65.79             28.54             25.16              0.00
65536                 116.63             48.48             42.89              0.00
131072                619.08            192.35            135.33              0.00
262144                566.90            288.15            258.20              0.00
524288               1182.64            605.67            536.85              0.00
1048576              2494.80           1256.44           1121.45              0.00
openmpi@1.10.7%intel@19.0.5.281 osu_alltoallv

# OSU MPI All-to-Allv Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.43
2                       3.45
4                       3.30
8                       3.31
16                      3.47
32                      3.43
64                      3.52
128                     3.99
256                     4.52
512                     6.18
1024                    8.81
2048                   10.02
4096                   18.91
8192                   32.35
16384                  54.02
32768                  78.67
65536                 138.31
131072                276.91
262144                627.53
524288               1823.79
1048576              3907.04
openmpi@1.10.7%intel@19.0.5.281 osu_iallgatherv

# OSU MPI Non-blocking Allgatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.36              4.94              4.06              0.00
2                      10.59              5.70              4.77              0.00
4                       9.05              4.81              4.01              0.00
8                       9.34              5.06              4.11              0.00
16                      9.19              4.97              4.07              0.00
32                      9.34              5.00              4.13              0.00
64                      9.29              5.01              4.30              0.41
128                     9.50              5.09              4.29              0.00
256                    10.67              5.46              4.60              0.00
512                    11.97              6.55              5.50              1.39
1024                   16.87              9.04              7.77              0.00
2048                   20.76             11.03              9.55              0.00
4096                   37.79             18.66             16.27              0.00
8192                   60.23             31.22             27.55              0.00
16384                  78.70             41.29             36.31              0.00
32768                 126.04             64.48             57.34              0.00
65536                 263.91            124.48            111.12              0.00
131072                521.20            280.37            248.99              3.28
262144               1100.51            550.29            489.63              0.00
524288               2779.00           1401.40           1250.13              0.00
1048576              6813.17           3744.18           3341.08              8.14
openmpi@1.10.7%intel@19.0.5.281 osu_ibcast

# OSU MPI Non-Blocking Broadcast Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       4.06              2.42              1.96             16.31
2                       4.16              2.45              1.94             11.97
4                       3.76              2.30              1.76             17.05
8                       3.71              2.26              1.66             12.81
16                      3.51              2.14              1.61             15.17
32                      3.41              2.10              1.62             18.76
64                      3.57              2.21              1.69             20.07
128                     3.67              2.25              1.71             17.31
256                     3.94              2.37              1.84             15.09
512                     5.13              3.44              2.78             39.30
1024                    5.42              3.34              2.64             21.18
2048                    6.32              3.80              3.13             19.26
4096                   10.44              6.67              5.69             33.59
8192                   16.16              9.58              8.24             20.27
16384                  24.13             13.34             11.56              6.62
32768                  38.16             20.31             17.74              0.00
65536                  93.44             51.00             45.05              5.79
131072                175.74             99.85             88.83             14.56
262144                405.91            179.83            160.20              0.00
524288                395.31            221.67            197.01             11.86
1048576               808.97            434.17            385.52              2.78
openmpi@1.10.7%intel@19.0.5.281 osu_reduce

# OSU MPI Reduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.84
8                       0.83
16                      0.93
32                      0.88
64                      0.97
128                     1.02
256                     1.18
512                     1.45
1024                    1.87
2048                    2.83
4096                    4.49
8192                    8.34
16384                  18.01
32768                  31.46
65536                  79.87
131072                140.44
262144                287.34
524288                494.00
1048576               997.63
openmpi@1.10.7%intel@19.0.5.281 osu_barrier

# OSU MPI Barrier Latency Test v5.6.3
# Avg Latency(us)
             1.57
openmpi@1.10.7%intel@19.0.5.281 osu_iallreduce

# OSU MPI Non-blocking Allreduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       8.77              5.32              4.43             22.08
8                       8.63              4.71              3.90              0.00
16                      8.52              4.90              4.01              9.69
32                      8.14              4.57              3.80              6.11
64                      8.74              4.98              4.12              8.70
128                     9.28              5.19              4.37              6.64
256                    10.58              6.00              5.02              8.59
512                    13.34              7.28              6.12              1.03
1024                   16.55              9.43              8.07             11.78
2048                   20.39             11.57             10.06             12.31
4096                   41.36             21.33             18.71              0.00
8192                   64.53             33.60             29.61              0.00
16384                 116.61             56.62             49.85              0.00
32768                 197.95             97.47             86.49              0.00
65536                 256.98            136.92            121.69              1.34
131072                453.33            236.34            210.62              0.00
262144                828.41            424.46            379.74              0.00
524288               5417.53            833.37            645.58              0.00
1048576              3025.81           1497.11           1332.12              0.00
openmpi@1.10.7%intel@19.0.5.281 osu_igather

# OSU MPI Non-blocking Gather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.47              1.30              0.89              0.00
2                       4.40              1.31              0.88              0.00
4                       6.18              1.91              1.39              0.00
8                       6.92              5.73              3.25             63.37
16                      2.45              1.32              0.88              0.00
32                      3.79              2.31              0.88              0.00
64                      4.26              3.08              0.94              0.00
128                     8.65              1.40              0.96              0.00
256                     2.70              1.45              1.02              0.00
512                     4.51              1.60              1.18              0.00
1024                    9.93              4.23              3.36              0.00
2048                    5.20              2.25              1.65              0.00
4096                   56.25              7.82              6.17              0.00
8192                   54.36             15.58             13.40              0.00
16384                  54.14             23.17             20.11              0.00
32768                  69.23             29.50             25.92              0.00
65536                 335.13            116.53             72.16              0.00
131072                511.88            138.28            122.80              0.00
262144               1133.18            300.25            251.28              0.00
524288               1618.79            595.37            491.50              0.00
1048576              2240.23           1172.50           1039.49              0.00
openmpi@1.10.7%intel@19.0.5.281 osu_reduce_scatter

# OSU MPI Reduce_scatter Latency Test v5.6.3
# Size       Avg Latency(us)
4                       1.06
8                       1.18
16                      1.34
32                      2.19
64                      2.22
128                     5.12
256                     2.46
512                     2.57
1024                    3.34
2048                    5.68
4096                    6.02
8192                   11.06
16384                  15.57
32768                  21.34
65536                  34.51
131072                159.98
262144                371.74
524288                255.72
1048576               520.80
openmpi@1.10.7%intel@19.0.5.281 osu_acc_latency
# OSU MPI_Accumulate latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.04
2                       0.04
4                       0.04
8                       0.05
16                      0.05
32                      0.06
64                      0.11
128                     0.15
256                     0.24
512                     0.42
1024                    0.79
2048                    1.51
4096                    2.97
8192                    5.90
16384                  11.85
32768                  23.86
65536                  48.14
131072                 95.59
262144                188.83
524288                380.11
1048576               759.38
2097152              1508.83
4194304              3008.60
openmpi@1.10.7%intel@19.0.5.281 osu_fop_latency
# OSU MPI_Fetch_and_op latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.04
openmpi@1.10.7%intel@19.0.5.281 osu_get_bw
# OSU MPI_Get Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      32.07
2                      63.40
4                     133.23
8                     271.43
16                    460.17
32                   1092.98
64                   2053.61
128                  4054.78
256                  7256.43
512                 13215.30
1024                22289.26
2048                29023.27
4096                34177.12
8192                21260.35
16384               15027.12
32768               11713.43
65536               10694.92
131072              11272.81
262144               5499.26
524288               4573.08
1048576              4518.88
2097152              4558.19
4194304              4720.81
openmpi@1.10.7%intel@19.0.5.281 osu_put_bibw
# OSU MPI_Put Bi-directional Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_post/start/complete/wait
# Size      Bandwidth (MB/s)
1                      43.58
2                      89.91
4                     187.85
8                     378.62
16                    729.87
32                   1442.62
64                   3009.62
128                  6039.32
256                 11645.79
512                 18415.66
1024                28604.10
2048                37409.70
4096                52539.34
8192                42739.29
16384               30080.73
32768               24625.49
65536               22238.81
131072              13441.36
262144               9756.95
524288               8483.85
1048576              8956.58
2097152              8457.78
4194304              8805.68
openmpi@1.10.7%intel@19.0.5.281 osu_put_latency
# OSU MPI_Put Latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.06
2                       0.06
4                       0.07
8                       0.07
16                      0.07
32                      0.07
64                      0.07
128                     0.07
256                     0.07
512                     0.09
1024                    0.09
2048                    0.11
4096                    0.18
8192                    0.31
16384                   0.76
32768                   1.63
65536                   3.00
131072                  5.56
262144                 12.86
524288                 21.05
1048576                69.96
2097152               175.69
4194304               347.49
openmpi@1.10.7%intel@19.0.5.281 osu_cas_latency
# OSU MPI_Compare_and_swap latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.04
openmpi@1.10.7%intel@19.0.5.281 osu_get_acc_latency
# OSU MPI_Get_accumulate latency Test v5.6.3
# Window creation: MPI_Win_create
# Synchronization: MPI_Win_lock/unlock
# Size          Latency (us)
1                       1.83
2                       1.82
4                       1.84
8                       1.84
16                      1.86
32                      1.83
64                      1.97
128                     2.05
256                     2.32
512                     2.64
1024                    3.36
2048                    4.63
4096                    8.18
8192                   13.16
16384                  24.43
32768                  43.41
65536                  76.83
131072                143.85
262144                274.35
524288                571.58
1048576              1134.42
2097152              2144.37
4194304              4497.17
openmpi@1.10.7%intel@19.0.5.281 osu_get_latency
# OSU MPI_Get latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.04
2                       0.04
4                       0.05
8                       0.04
16                      0.06
32                      0.06
64                      0.04
128                     0.06
256                     0.06
512                     0.05
1024                    0.06
2048                    0.07
4096                    0.10
8192                    0.13
16384                   0.32
32768                   1.07
65536                   2.80
131072                  5.45
262144                 11.74
524288                 23.52
1048576                62.98
2097152               171.98
4194304               352.69
openmpi@1.10.7%intel@19.0.5.281 osu_put_bw
# OSU MPI_Put Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      24.48
2                      52.26
4                     101.90
8                     215.10
16                    429.19
32                    861.04
64                   1664.52
128                  3207.95
256                  5922.20
512                  9309.38
1024                14177.15
2048                19083.45
4096                24157.44
8192                16494.11
16384               10811.86
32768               10470.39
65536               11708.31
131072               9809.97
262144               5310.63
524288               4500.46
1048576              4568.12
2097152              4426.52
4194304              4395.14
-- linux-debian9-cascadelake / intel@19.0.5.281 -----------------
hwloc@1.11.11
libiconv@1.16
libpciaccess@0.16
libxml2@2.9.10
numactl@2.0.14
openmpi@2.1.6
osu-micro-benchmarks@5.6.3
xz@5.2.5
zlib@1.2.11
openmpi@2.1.6%intel@19.0.5.281 osu_hello
# OSU MPI Hello World Test v5.6.3
This is a test with 2 processes
openmpi@2.1.6%intel@19.0.5.281 osu_init
# OSU MPI Init Test v5.6.3
nprocs: 2, min: 12 ms, max: 12 ms, avg: 12 ms
openmpi@2.1.6%intel@19.0.5.281 osu_bibw
# OSU MPI Bi-Directional Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       8.92
2                      16.95
4                      33.52
8                      67.68
16                     88.84
32                    252.95
64                    375.00
128                   535.12
256                   926.33
512                  1802.61
1024                 3361.25
2048                 5382.69
4096                 5140.69
8192                 7955.22
16384               10000.16
32768                9870.20
65536                9896.33
131072               9752.05
262144               9978.75
524288               9615.85
1048576              9410.59
2097152              9158.23
4194304              8470.56
openmpi@2.1.6%intel@19.0.5.281 osu_latency
# OSU MPI Latency Test v5.6.3
# Size          Latency (us)
0                       0.23
1                       0.28
2                       0.27
4                       0.28
8                       0.27
16                      0.29
32                      0.29
64                      0.32
128                     0.37
256                     0.42
512                     0.67
1024                    0.72
2048                    0.89
4096                    1.60
8192                    2.35
16384                   4.49
32768                   6.99
65536                  10.08
131072                 16.29
262144                 29.14
524288                 55.02
1048576               116.52
2097152               244.99
4194304               517.89
openmpi@2.1.6%intel@19.0.5.281 osu_latency_mt
MPI_Init_thread must return MPI_THREAD_MULTIPLE!
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[1546,1],1]
  Exit code:    1
--------------------------------------------------------------------------
openmpi@2.1.6%intel@19.0.5.281 osu_multi_lat
# OSU MPI Multi Latency Test v5.6.3
# Size          Latency (us)
0                       0.22
1                       0.27
2                       0.27
4                       0.27
8                       0.27
16                      0.30
32                      0.29
64                      0.32
128                     0.36
256                     0.42
512                     0.65
1024                    0.71
2048                    0.88
4096                    1.59
8192                    2.35
16384                   4.40
32768                   8.21
65536                  10.18
131072                 16.71
262144                 29.91
524288                 56.73
1048576               119.68
2097152               247.43
4194304               510.59
openmpi@2.1.6%intel@19.0.5.281 osu_bw
# OSU MPI Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       8.46
2                      17.72
4                      35.44
8                      70.48
16                    135.27
32                    256.80
64                    361.15
128                   459.85
256                   772.37
512                  1847.95
1024                 3120.92
2048                 5022.12
4096                 5417.52
8192                 7856.87
16384                9188.71
32768                9640.80
65536                9869.41
131072               9556.05
262144               9821.22
524288               9790.17
1048576              9627.09
2097152              8812.62
4194304              7867.53
openmpi@2.1.6%intel@19.0.5.281 osu_latency_mp
# OSU MPI Multi-process Latency Test v5.6.3
# Number of forked processes in sender: 2
# Number of forked processes in receiver: 2
# Size          Latency (us)
0                       0.22
1                       0.28
2                       0.29
4                       0.28
8                       0.28
16                      0.30
32                      0.31
64                      0.34
128                     0.39
256                     0.44
512                     0.67
1024                    0.73
2048                    0.89
4096                    1.64
8192                    2.38
16384                   4.64
32768                   7.20
65536                  10.32
131072                 16.77
262144                 30.12
524288                 56.88
1048576               124.58
2097152               254.43
4194304               529.80
openmpi@2.1.6%intel@19.0.5.281 osu_mbw_mr
# OSU MPI Multiple Bandwidth / Message Rate Test v5.6.3
# [ pairs: 1 ] [ window size: 64 ]
# Size                  MB/s        Messages/s
1                       6.79        6794617.70
2                      11.70        5848829.48
4                      26.91        6726497.67
8                      49.72        6215583.60
16                    105.07        6567018.83
32                    202.71        6334570.24
64                    265.08        4141815.89
128                   385.79        3014005.69
256                   633.87        2476070.15
512                  1404.77        2743685.35
1024                 2504.81        2446101.69
2048                 4215.61        2058403.35
4096                 4438.00        1083495.71
8192                 7293.66         890338.90
16384                9161.62         559180.96
32768                9860.82         300928.19
65536                9961.30         151997.41
131072               9737.62          74292.16
262144               9749.65          37191.98
524288               9696.88          18495.32
1048576              9509.58           9069.04
2097152              8799.16           4195.77
4194304              7825.80           1865.82
openmpi@2.1.6%intel@19.0.5.281 osu_allgather

# OSU MPI Allgather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       1.47
2                       1.39
4                       3.59
8                       1.54
16                      1.46
32                      1.55
64                      1.66
128                     2.21
256                     2.95
512                     3.52
1024                    5.08
2048                    7.66
4096                   13.43
8192                   23.66
16384                  39.78
32768                  54.31
65536                 102.60
131072                205.14
262144                455.10
524288               1084.47
1048576              3053.25
openmpi@2.1.6%intel@19.0.5.281 osu_bcast

# OSU MPI Broadcast Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.86
2                       0.82
4                       0.86
8                       0.83
16                      0.89
32                      0.94
64                      3.08
128                     1.75
256                     1.18
512                     1.71
1024                    2.03
2048                    4.02
4096                    6.28
8192                   11.38
16384                  20.80
32768                  39.09
65536                  74.45
131072                143.04
262144                277.11
524288                266.62
1048576               567.44
openmpi@2.1.6%intel@19.0.5.281 osu_ialltoall

# OSU MPI Non-blocking All-to-All Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       8.74              4.66              3.88              0.00
2                       8.73              4.64              3.88              0.00
4                       9.28              4.95              4.12              0.00
8                       8.98              4.96              4.18              4.05
16                      8.82              4.51              3.67              0.00
32                      8.68              4.74              3.97              0.70
64                      8.49              4.56              3.81              0.00
128                     9.17              4.87              4.05              0.00
256                     9.51              5.13              4.24              0.00
512                    15.95              8.17              7.03              0.00
1024                   21.09             10.65              9.23              0.00
2048                   24.63             13.12             11.30              0.00
4096                   41.47             22.79             20.01              6.60
8192                   55.24             28.79             25.33              0.00
16384                  77.59             40.48             35.65              0.00
32768                 125.96             64.72             57.07              0.00
65536                 242.66            126.28            112.33              0.00
131072                581.84            297.07            264.38              0.00
262144               1781.06            881.95            781.04              0.00
524288               3884.13           2077.81           1854.89              2.62
1048576              8740.47           4770.57           4265.86              6.94
openmpi@2.1.6%intel@19.0.5.281 osu_igatherv

# OSU MPI Non-blocking Gatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.42              1.28              0.91              0.00
2                       2.36              1.29              0.87              0.00
4                       2.42              1.29              0.83              0.00
8                       2.31              1.24              0.80              0.00
16                      2.37              1.30              0.84              0.00
32                      2.32              1.24              0.81              0.00
64                      2.27              1.24              0.83              0.00
128                     2.42              1.31              0.90              0.00
256                     2.65              1.53              1.10              0.00
512                     6.75              3.39              2.80              0.00
1024                    8.72              4.28              3.56              0.00
2048                   11.36              5.29              4.36              0.00
4096                   18.86              8.12              6.85              0.00
8192                   29.28             12.67             10.99              0.00
16384                  45.33             21.04             18.51              0.00
32768                  63.58             28.53             25.17              0.00
65536                 103.83             45.31             40.09              0.00
131072                204.28             91.46             80.96              0.00
262144                402.79            187.30            166.97              0.00
524288                848.42            414.02            369.83              0.00
1048576              1917.12            966.54            862.67              0.00
openmpi@2.1.6%intel@19.0.5.281 osu_scatter

# OSU MPI Scatter Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.76
2                       0.75
4                       0.83
8                       0.87
16                      0.91
32                      0.82
64                      1.12
128                     1.25
256                     1.32
512                     4.11
1024                    4.40
2048                    5.45
4096                    9.59
8192                   13.85
16384                  20.72
32768                  32.81
65536                  47.93
131072                 91.00
262144                183.65
524288                362.25
1048576               806.97
openmpi@2.1.6%intel@19.0.5.281 osu_allgatherv

# OSU MPI Allgatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.92
2                       3.90
4                       3.83
8                      14.66
16                      9.05
32                     14.24
64                      7.62
128                     7.52
256                     5.90
512                     6.58
1024                    8.23
2048                   12.94
4096                   25.92
8192                   37.56
16384                  31.67
32768                  54.46
65536                 101.10
131072                306.41
262144                852.55
524288               1914.82
1048576              5890.42
openmpi@2.1.6%intel@19.0.5.281 osu_gather

# OSU MPI Gather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.31
2                       0.27
4                       0.29
8                       0.28
16                      0.28
32                      0.30
64                      0.29
128                     0.31
256                     0.35
512                     1.43
1024                    1.96
2048                    2.79
4096                   10.85
8192                   14.11
16384                  17.97
32768                  24.56
65536                  37.59
131072                 82.10
262144                137.80
524288                396.25
1048576               575.67
openmpi@2.1.6%intel@19.0.5.281 osu_ialltoallv

# OSU MPI Non-blocking All-to-Allv Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.56              5.12              4.28              0.00
2                       9.53              4.90              4.09              0.00
4                       9.03              4.91              4.13              0.44
8                       8.87              4.83              4.00              0.00
16                      8.71              4.64              3.86              0.00
32                      9.05              4.96              4.09              0.00
64                      9.13              4.84              4.04              0.00
128                     9.80              5.53              4.70              9.19
256                     9.99              5.30              4.39              0.00
512                    15.54              8.42              7.25              1.75
1024                   19.22              9.63              8.28              0.00
2048                   24.37             11.92             10.35              0.00
4096                   45.20             23.28             20.49              0.00
8192                   56.88             29.92             26.28              0.00
16384                  78.77             40.61             35.84              0.00
32768                 127.18             65.81             58.12              0.00
65536                 255.62            136.90            121.48              2.28
131072                593.51            295.17            259.69              0.00
262144               1795.37            934.79            835.66              0.00
524288               4012.79           2177.13           1943.54              5.55
1048576              8819.01           4764.11           4244.19              4.46
openmpi@2.1.6%intel@19.0.5.281 osu_ireduce

# OSU MPI Non-blocking Reduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       3.10              1.69              1.20              0.00
8                       3.09              1.75              1.20              0.00
16                      3.56              1.85              1.38              0.00
32                      3.31              1.86              1.42              0.00
64                      3.32              1.91              1.47              4.45
128                     3.40              1.86              1.42              0.00
256                     3.63              2.02              1.53              0.00
512                     6.83              3.09              2.53              0.00
1024                    8.19              3.59              2.94              0.00
2048                   10.00              4.49              3.78              0.00
4096                   14.14              5.75              4.81              0.00
8192                   20.93              8.01              6.79              0.00
16384                  40.26             15.20             13.26              0.00
32768                  61.48             21.75             19.09              0.00
65536                  95.35             34.09             30.06              0.00
131072                176.18             62.31             55.22              0.00
262144                335.47            116.15            103.29              0.00
524288                661.40            223.22            198.86              0.00
1048576              1396.63            472.27            420.99              0.00
openmpi@2.1.6%intel@19.0.5.281 osu_scatterv

# OSU MPI Scatterv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.68
2                       0.69
4                       0.69
8                       0.71
16                      0.79
32                      0.79
64                      0.83
128                     0.93
256                     1.09
512                     3.64
1024                    4.00
2048                    4.86
4096                    7.77
8192                   10.91
16384                  20.47
32768                  32.90
65536                  47.81
131072                106.04
262144                196.18
524288                383.50
1048576               727.20
openmpi@2.1.6%intel@19.0.5.281 osu_allreduce

# OSU MPI Allreduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       1.88
8                       1.88
16                      1.88
32                      1.99
64                      2.41
128                     2.27
256                     2.56
512                     4.11
1024                    4.98
2048                    7.53
4096                   14.35
8192                   19.17
16384                  22.82
32768                  52.25
65536                  78.30
131072                155.96
262144                188.91
524288                385.03
1048576               886.70
openmpi@2.1.6%intel@19.0.5.281 osu_gatherv

# OSU MPI Gatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.27
2                       0.26
4                       0.35
8                       0.27
16                      0.29
32                      0.28
64                      0.28
128                     0.31
256                     0.34
512                     1.52
1024                    2.05
2048                    2.81
4096                    9.42
8192                   13.62
16384                  22.82
32768                  34.51
65536                  48.59
131072                 84.68
262144                151.67
524288                342.01
1048576               633.76
openmpi@2.1.6%intel@19.0.5.281 osu_ialltoallw

# OSU MPI Non-blocking All-to-Allw Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      12.63              6.76              5.77              0.00
2                      11.42              6.14              5.24              0.00
4                      11.93              6.12              5.20              0.00
8                       9.56              5.34              4.53              6.76
16                      8.86              4.62              3.91              0.00
32                      9.98              4.76              3.99              0.00
64                      8.90              4.75              3.94              0.00
128                     9.32              5.02              4.18              0.00
256                     9.88              5.25              4.37              0.00
512                    15.32              8.11              6.92              0.00
1024                   17.90              9.51              8.18              0.00
2048                   23.37             12.15             10.48              0.00
4096                   40.24             21.40             18.75              0.00
8192                   62.06             34.23             30.21              7.86
16384                  80.80             41.29             36.41              0.00
32768                 124.43             65.55             58.24              0.00
65536                 265.00            138.04            122.76              0.00
131072                619.35            312.52            279.68              0.00
262144               1907.91            995.58            889.49              0.00
524288               3980.96           2063.89           1841.34              0.00
1048576              8659.64           4626.93           4124.62              2.23
openmpi@2.1.6%intel@19.0.5.281 osu_iscatter

# OSU MPI Non-blocking Scatter Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.65              2.09              1.61              3.35
2                       4.88              3.12              2.46             28.43
4                       3.39              2.17              1.70             28.02
8                       5.22              3.37              1.64              0.00
16                      3.43              2.35              1.85             41.73
32                      4.21              2.18              1.71              0.00
64                      3.31              2.22              1.73             36.73
128                     3.67              2.52              1.93             40.65
256                     4.04              2.82              2.22             45.27
512                     4.53              2.96              2.45             35.92
1024                    5.19              3.50              2.84             40.60
2048                    6.20              4.39              3.61             50.14
4096                   16.36              8.23              7.02              0.00
8192                   33.59             13.70             10.15              0.00
16384                  45.36             19.41             17.02              0.00
32768                  60.72             26.68             23.41              0.00
65536                 134.00             53.34             42.77              0.00
131072                596.35            200.03            164.00              0.00
262144               1022.79            412.41            347.93              0.00
524288               2455.76           1370.18           1148.11              5.45
1048576              4744.13           1892.29           1581.38              0.00
openmpi@2.1.6%intel@19.0.5.281 osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       6.68
2                       3.79
4                       3.64
8                       3.59
16                      3.86
32                      3.63
64                      3.90
128                     4.00
256                     4.61
512                     5.23
1024                    6.24
2048                    8.10
4096                   25.53
8192                   33.66
16384                  48.53
32768                  73.15
65536                 135.57
131072                267.67
262144                627.03
524288               1736.35
1048576              3559.36
openmpi@2.1.6%intel@19.0.5.281 osu_iallgather

# OSU MPI Non-blocking Allgather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      11.21              6.11              5.11              0.08
2                      11.20              6.26              5.35              7.72
4                      10.59              5.93              5.05              7.65
8                       9.63              5.56              4.64             12.09
16                      8.81              4.72              3.89              0.00
32                      8.93              4.94              4.13              3.36
64                      9.08              4.77              3.94              0.00
128                    10.17              5.59              4.66              1.53
256                    10.08              5.40              4.50              0.00
512                    32.40             13.21             10.66              0.00
1024                   20.77             11.01              9.49              0.00
2048                   23.40             12.28             10.62              0.00
4096                   42.71             21.74             19.00              0.00
8192                   59.48             30.51             26.83              0.00
16384                  81.68             42.18             36.91              0.00
32768                 133.58             69.36             61.70              0.00
65536                 238.83            130.36            115.86              6.38
131072                496.55            261.47            233.09              0.00
262144               1247.36            598.32            534.27              0.00
524288               3502.02           1587.49           1391.83              0.00
1048576              6599.01           3398.23           3034.49              0.00
openmpi@2.1.6%intel@19.0.5.281 osu_ibarrier

# OSU MPI Non-blocking Barrier Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

       Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
              8.28              4.99              4.15             20.66
openmpi@2.1.6%intel@19.0.5.281 osu_iscatterv

# OSU MPI Non-blocking Scatterv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.51              2.43              1.84             41.13
2                       3.70              2.25              1.76             17.64
4                       3.45              2.23              1.76             30.30
8                       3.33              2.27              1.77             39.60
16                      3.34              2.26              1.77             38.88
32                      3.40              2.30              1.76             37.79
64                      3.62              2.39              1.84             33.11
128                     3.80              2.63              2.03             42.34
256                     4.53              3.35              2.68             55.95
512                     4.89              3.34              2.69             42.32
1024                    5.32              3.60              2.94             41.65
2048                    7.32              5.02              3.83             39.86
4096                   18.00              8.45              7.27              0.00
8192                   23.36             11.57              9.74              0.00
16384                  35.29             16.51             14.42              0.00
32768                  58.27             26.46             23.36              0.00
65536                 105.58             46.67             41.24              0.00
131072                274.06            138.97            123.75              0.00
262144                577.90            295.17            263.17              0.00
524288               1228.17            634.48            565.51              0.00
1048576              2586.50           1291.26           1151.47              0.00
openmpi@2.1.6%intel@19.0.5.281 osu_alltoallv

# OSU MPI All-to-Allv Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.53
2                       3.40
4                       3.75
8                       3.71
16                      5.26
32                      3.48
64                      4.05
128                     4.33
256                     7.26
512                     8.12
1024                    9.00
2048                   10.44
4096                   22.48
8192                   31.85
16384                  50.66
32768                  72.46
65536                 138.12
131072                271.58
262144                637.95
524288               1776.75
1048576              3654.55
openmpi@2.1.6%intel@19.0.5.281 osu_iallgatherv

# OSU MPI Non-blocking Allgatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.70              5.09              4.21              0.00
2                      10.10              5.59              4.67              3.29
4                      11.91              6.03              5.06              0.00
8                      10.31              5.91              4.97             11.61
16                      9.87              5.36              4.50              0.00
32                     10.54              6.01              4.90              7.54
64                      9.91              5.37              4.56              0.45
128                    10.66              5.91              5.01              5.21
256                    10.79              5.91              4.98              2.08
512                    15.19              8.18              6.99              0.00
1024                   17.72              9.26              7.94              0.00
2048                   22.75             11.66              9.99              0.00
4096                   45.67             20.77             18.19              0.00
8192                   55.78             28.83             25.29              0.00
16384                  90.28             51.42             45.16             13.95
32768                 121.50             65.27             57.91              2.90
65536                 222.15            117.05            104.01              0.00
131072                500.93            245.45            218.55              0.00
262144               1044.94            530.44            472.30              0.00
524288               3009.92           1523.92           1364.11              0.00
1048576             10723.12           5501.16           4904.41              0.00
openmpi@2.1.6%intel@19.0.5.281 osu_ibcast

# OSU MPI Non-Blocking Broadcast Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       4.04              2.56              1.95             24.07
2                       3.79              2.23              1.72              9.33
4                       3.58              2.15              1.61             11.58
8                       3.49              2.13              1.59             14.23
16                      3.51              2.19              1.68             21.64
32                      3.36              2.09              1.58             19.68
64                      3.42              2.13              1.69             23.32
128                     4.01              2.40              1.85             13.58
256                     4.09              2.47              1.87             13.17
512                     5.47              3.10              2.52              6.07
1024                    6.05              3.38              2.75              2.86
2048                    7.69              4.52              3.78             16.21
4096                   12.90              7.73              6.36             18.70
8192                   16.88             10.34              8.92             26.62
16384                  33.58             18.94             16.59             11.76
32768                  37.37             20.83             18.28              9.53
65536                  61.88             31.08             27.21              0.00
131072                110.64             69.49             61.41             32.98
262144                210.35            131.03            116.64             32.00
524288                395.93            264.79            235.79             44.39
1048576               919.19            554.86            482.53             24.50
openmpi@2.1.6%intel@19.0.5.281 osu_reduce

# OSU MPI Reduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.69
8                       0.70
16                      0.73
32                      0.73
64                      0.82
128                     0.88
256                     1.00
512                     1.80
1024                    2.08
2048                    6.07
4096                    8.60
8192                   18.45
16384                  15.86
32768                  41.45
65536                  84.64
131072                116.00
262144                189.83
524288                343.90
1048576               617.75
openmpi@2.1.6%intel@19.0.5.281 osu_barrier

# OSU MPI Barrier Latency Test v5.6.3
# Avg Latency(us)
             1.26
openmpi@2.1.6%intel@19.0.5.281 osu_iallreduce

# OSU MPI Non-blocking Allreduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       7.61              4.18              3.44              0.12
8                      65.90             28.10             22.91              0.00
16                      8.62              4.43              3.57              0.00
32                     18.80              5.62              3.52              0.00
64                     20.46              6.08              3.81              0.00
128                     8.52              4.44              3.70              0.00
256                    12.10              6.33              4.08              0.00
512                    57.62             19.93             14.60              0.00
1024                   15.06              7.72              6.62              0.00
2048                   82.35             18.80             11.93              0.00
4096                   57.99             18.98             15.40              0.00
8192                  139.30             58.29             46.88              0.00
16384                 157.37             56.60             46.70              0.00
32768                 141.31             74.73             66.29              0.00
65536                 166.18             85.87             76.07              0.00
131072                783.04            631.43            559.53             72.91
262144                403.15            210.88            187.63              0.00
524288                711.62            366.80            326.81              0.00
1048576              1586.16            767.29            679.23              0.00
openmpi@2.1.6%intel@19.0.5.281 osu_igather

# OSU MPI Non-blocking Gather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.96              1.43              0.95              0.00
2                       2.72              1.53              1.01              0.00
4                       2.64              1.44              0.94              0.00
8                       2.62              1.42              0.96              0.00
16                      2.65              1.46              0.93              0.00
32                      2.63              1.42              0.93              0.00
64                      2.64              1.43              0.95              0.00
128                     2.65              1.41              0.94              0.00
256                     2.78              1.50              1.07              0.00
512                     7.38              3.57              2.90              0.00
1024                    9.31              4.37              3.60              0.00
2048                   11.24              5.59              4.62              0.00
4096                   19.44              8.18              7.02              0.00
8192                   29.12             12.48             10.79              0.00
16384                  44.94             20.58             18.11              0.00
32768                  64.35             28.62             25.16              0.00
65536                 108.03             44.09             39.04              0.00
131072                203.79             90.12             78.93              0.00
262144                425.41            196.00            173.97              0.00
524288                856.05            414.31            367.19              0.00
1048576              2071.75            957.75            851.23              0.00
openmpi@2.1.6%intel@19.0.5.281 osu_reduce_scatter

# OSU MPI Reduce_scatter Latency Test v5.6.3
# Size       Avg Latency(us)
4                       1.00
8                       1.10
16                      1.31
32                      1.88
64                      1.89
128                     1.94
256                     2.06
512                     3.22
1024                    4.64
2048                    4.52
4096                    5.82
8192                    9.98
16384                  15.10
32768                  20.52
65536                  31.28
131072                167.39
262144                364.61
524288                307.90
1048576               566.18
openmpi@2.1.6%intel@19.0.5.281 osu_acc_latency
# OSU MPI_Accumulate latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.04
2                       0.04
4                       0.06
8                       0.04
16                      0.05
32                      0.06
64                      0.09
128                     0.13
256                     0.24
512                     0.42
1024                    0.80
2048                    1.54
4096                    2.97
8192                    5.86
16384                  11.78
32768                  23.35
65536                  46.67
131072                 96.43
262144                189.12
524288                382.19
1048576               760.70
2097152              1518.62
4194304              3042.18
openmpi@2.1.6%intel@19.0.5.281 osu_fop_latency
# OSU MPI_Fetch_and_op latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.05
openmpi@2.1.6%intel@19.0.5.281 osu_get_bw
# OSU MPI_Get Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      29.46
2                      64.64
4                     136.82
8                     225.84
16                    443.51
32                    943.59
64                   2061.50
128                  3510.71
256                  8464.47
512                 12072.76
1024                22191.97
2048                28954.60
4096                31731.87
8192                22829.70
16384               13665.80
32768               11874.46
65536               10402.65
131072               9908.09
262144               4976.74
524288               4552.14
1048576              4526.48
2097152              4411.96
4194304              4550.30
openmpi@2.1.6%intel@19.0.5.281 osu_put_bibw
# OSU MPI_Put Bi-directional Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_post/start/complete/wait
# Size      Bandwidth (MB/s)
1                      38.40
2                      87.21
4                     174.88
8                     360.10
16                    596.21
32                    910.65
64                   2791.47
128                  5553.26
256                 10832.43
512                 18718.10
1024                28528.83
2048                36663.55
4096                40292.44
8192                40415.26
16384               31497.54
32768               24328.16
65536               20821.22
131072              12802.37
262144               8810.24
524288               8966.17
1048576              9152.86
2097152              8450.97
4194304              9032.01
openmpi@2.1.6%intel@19.0.5.281 osu_put_latency
# OSU MPI_Put Latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.05
2                       0.05
4                       0.05
8                       0.06
16                      0.06
32                      0.06
64                      0.06
128                     0.06
256                     0.06
512                     0.07
1024                    0.08
2048                    0.10
4096                    0.19
8192                    0.31
16384                   0.77
32768                   1.67
65536                   3.00
131072                  6.03
262144                 12.44
524288                 25.16
1048576                61.98
2097152               168.64
4194304               345.60
openmpi@2.1.6%intel@19.0.5.281 osu_cas_latency
# OSU MPI_Compare_and_swap latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.04
openmpi@2.1.6%intel@19.0.5.281 osu_get_acc_latency
# OSU MPI_Get_accumulate latency Test v5.6.3
# Window creation: MPI_Win_create
# Synchronization: MPI_Win_lock/unlock
# Size          Latency (us)
1                       3.39
2                       3.78
4                       3.47
8                       3.38
16                      3.38
32                      3.38
64                      3.43
128                     3.61
256                     3.81
512                     4.07
1024                    4.92
2048                    6.22
4096                    9.58
8192                   14.01
16384                  24.87
32768                  43.28
65536                  77.25
131072                146.51
262144                276.28
524288                557.65
1048576              1135.87
2097152              2190.09
4194304              4432.57
openmpi@2.1.6%intel@19.0.5.281 osu_get_latency
# OSU MPI_Get latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.03
2                       0.04
4                       0.03
8                       0.04
16                      0.03
32                      0.04
64                      0.04
128                     0.04
256                     0.04
512                     0.04
1024                    0.04
2048                    0.05
4096                    0.08
8192                    0.13
16384                   0.41
32768                   1.48
65536                   2.79
131072                  6.53
262144                  7.13
524288                 21.65
1048576                64.58
2097152               169.95
4194304               350.33
openmpi@2.1.6%intel@19.0.5.281 osu_put_bw
# OSU MPI_Put Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      34.63
2                      69.30
4                     131.81
8                     276.97
16                    428.39
32                   1107.91
64                   2200.66
128                  4265.56
256                  8474.06
512                 12759.13
1024                16738.52
2048                18843.34
4096                24683.39
8192                22171.23
16384               15268.09
32768               12247.81
65536               11328.61
131072              10885.45
262144               5275.34
524288               4605.30
1048576              4423.61
2097152              4610.65
4194304              4620.09
-- linux-debian9-cascadelake / intel@19.0.5.281 -----------------
hwloc@1.11.11
libiconv@1.16
libpciaccess@0.16
libxml2@2.9.10
numactl@2.0.14
openmpi@3.1.6
osu-micro-benchmarks@5.6.3
xz@5.2.5
zlib@1.2.11
openmpi@3.1.6%intel@19.0.5.281 osu_hello
# OSU MPI Hello World Test v5.6.3
This is a test with 2 processes
openmpi@3.1.6%intel@19.0.5.281 osu_init
# OSU MPI Init Test v5.6.3
nprocs: 2, min: 10 ms, max: 13 ms, avg: 11 ms
openmpi@3.1.6%intel@19.0.5.281 osu_bibw
# OSU MPI Bi-Directional Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       9.62
2                      19.35
4                      38.70
8                      71.39
16                    115.76
32                    305.12
64                    374.66
128                   556.87
256                   918.72
512                  1942.17
1024                 3264.54
2048                 5506.34
4096                 5189.28
8192                 8016.92
16384                9748.94
32768                9698.25
65536                9890.45
131072               9741.49
262144               9901.64
524288               9734.14
1048576              9683.57
2097152              8922.22
4194304              8294.01
openmpi@3.1.6%intel@19.0.5.281 osu_latency
# OSU MPI Latency Test v5.6.3
# Size          Latency (us)
0                       0.22
1                       0.28
2                       0.28
4                       0.30
8                       0.27
16                      0.29
32                      0.30
64                      0.33
128                     0.37
256                     0.43
512                     0.64
1024                    0.71
2048                    0.92
4096                    1.63
8192                    2.30
16384                   4.56
32768                   7.09
65536                  10.34
131072                 16.82
262144                 29.93
524288                 57.27
1048576               123.99
2097152               253.78
4194304               518.71
openmpi@3.1.6%intel@19.0.5.281 osu_latency_mt
# Number of Sender threads: 1
# Number of Receiver threads: 2
# OSU MPI Multi-threaded Latency Test v5.6.3
# Size          Latency (us)
0                       0.38
1                       0.44
2                       0.44
4                       0.44
8                       0.45
16                      0.45
32                      0.45
64                      0.47
128                     0.53
256                     0.59
512                     0.99
1024                    1.04
2048                    1.20
4096                    4.48
8192                    3.71
16384                   4.98
32768                   7.48
65536                  10.84
131072                 17.72
262144                 32.22
524288                 93.59
1048576               209.51
2097152               585.60
4194304              1066.35
openmpi@3.1.6%intel@19.0.5.281 osu_multi_lat
# OSU MPI Multi Latency Test v5.6.3
# Size          Latency (us)
0                       0.23
1                       0.30
2                       0.28
4                       0.28
8                       0.29
16                      0.30
32                      0.31
64                      0.35
128                     0.39
256                     0.44
512                     0.66
1024                    0.72
2048                    1.15
4096                    1.69
8192                    2.36
16384                   4.60
32768                   7.18
65536                  10.45
131072                 16.96
262144                 30.08
524288                 57.91
1048576               122.69
2097152               250.35
4194304               523.15
openmpi@3.1.6%intel@19.0.5.281 osu_bw
# OSU MPI Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       7.40
2                      17.02
4                      33.54
8                      65.04
16                    121.26
32                    218.49
64                    342.97
128                   470.59
256                   810.48
512                  1844.30
1024                 3156.29
2048                 5086.51
4096                 5620.43
8192                 7709.40
16384                8771.23
32768                9473.18
65536                9660.13
131072               9461.03
262144               9506.56
524288               9384.24
1048576              9356.19
2097152              8821.14
4194304              7553.89
openmpi@3.1.6%intel@19.0.5.281 osu_latency_mp
# OSU MPI Multi-process Latency Test v5.6.3
# Number of forked processes in sender: 2
# Number of forked processes in receiver: 2
# Size          Latency (us)
0                       0.24
1                       0.29
2                       0.29
4                       0.30
8                       0.29
16                      0.30
32                      0.30
64                      0.32
128                     0.36
256                     0.42
512                     0.63
1024                    0.69
2048                    0.89
4096                    1.63
8192                    2.53
16384                   4.66
32768                   7.13
65536                  10.34
131072                 16.82
262144                 30.10
524288                 57.04
1048576               121.50
2097152               251.02
4194304               535.91
openmpi@3.1.6%intel@19.0.5.281 osu_mbw_mr
# OSU MPI Multiple Bandwidth / Message Rate Test v5.6.3
# [ pairs: 1 ] [ window size: 64 ]
# Size                  MB/s        Messages/s
1                       4.46        4457690.59
2                       8.92        4462287.39
4                      17.89        4471578.22
8                      33.96        4244811.53
16                     70.97        4435837.72
32                    142.23        4444697.52
64                    265.20        4143722.40
128                   472.96        3695015.68
256                   839.25        3278312.32
512                  1933.42        3776210.00
1024                 3620.90        3536030.80
2048                 5315.60        2595508.64
4096                 5239.15        1279089.04
8192                 7361.52         898623.23
16384                9168.47         559598.75
32768                9687.09         295626.40
65536               10071.60         153680.48
131072               9892.98          75477.46
262144               9784.49          37324.88
524288               9790.86          18674.58
1048576              9587.63           9143.48
2097152              8990.90           4287.19
4194304              8060.21           1921.70
openmpi@3.1.6%intel@19.0.5.281 osu_allgather

# OSU MPI Allgather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       1.33
2                       1.37
4                       1.37
8                       1.43
16                      1.49
32                      1.52
64                      1.62
128                     2.33
256                     2.78
512                     3.36
1024                    5.02
2048                    8.04
4096                   12.88
8192                   23.82
16384                  31.12
32768                  48.61
65536                 104.12
131072                206.36
262144                456.07
524288               1310.93
1048576              3051.16
openmpi@3.1.6%intel@19.0.5.281 osu_bcast

# OSU MPI Broadcast Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.80
2                       0.79
4                       0.78
8                       0.85
16                      0.81
32                      0.84
64                      0.91
128                     1.07
256                     1.26
512                     1.76
1024                    2.06
2048                    4.30
4096                    6.54
8192                   11.98
16384                  21.72
32768                  38.91
65536                  76.24
131072                145.01
262144                265.36
524288                252.47
1048576               506.53
openmpi@3.1.6%intel@19.0.5.281 osu_ialltoall

# OSU MPI Non-blocking All-to-All Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      11.28              5.81              4.90              0.00
2                      10.50              5.79              4.86              2.92
4                       8.97              4.90              4.11              0.88
8                       8.62              4.62              3.89              0.00
16                      8.72              4.68              3.94              0.00
32                      9.40              4.89              4.11              0.00
64                     10.08              6.01              5.11             20.31
128                     8.88              4.68              3.92              0.00
256                     9.87              5.47              4.63              4.76
512                    13.75              7.09              6.05              0.00
1024                   15.65              8.36              7.14              0.00
2048                   20.19             10.76              9.29              0.00
4096                   42.06             21.57             18.59              0.00
8192                   57.40             28.53             25.04              0.00
16384                 107.20             56.11             49.70              0.00
32768                 124.50             65.00             57.47              0.00
65536                 270.50            128.78            111.78              0.00
131072                623.84            324.43            289.86              0.00
262144               1503.03            794.06            708.89              0.00
524288               3753.61           1914.84           1698.68              0.00
1048576              8005.21           4338.19           3878.19              5.45
openmpi@3.1.6%intel@19.0.5.281 osu_igatherv

# OSU MPI Non-blocking Gatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.36              1.30              0.85              0.00
2                       3.04              1.28              0.82              0.00
4                       2.33              1.24              0.79              0.00
8                       2.23              1.24              0.78              0.00
16                      2.24              1.24              0.78              0.00
32                      2.24              1.23              0.79              0.00
64                      2.26              1.24              0.81              0.00
128                     2.75              1.25              0.81              0.00
256                     2.49              1.34              0.87              0.00
512                     7.14              3.42              2.75              0.00
1024                    7.96              3.91              3.19              0.00
2048                   10.87              5.01              4.10              0.00
4096                   19.65              7.78              6.63              0.00
8192                   27.95             11.85             10.19              0.00
16384                  43.84             19.98             17.51              0.00
32768                  60.30             27.21             23.72              0.00
65536                  96.45             41.68             36.90              0.00
131072                187.71             82.75             73.28              0.00
262144                371.79            184.84            164.71              0.00
524288                812.47            384.45            336.95              0.00
1048576              1685.25            822.11            734.92              0.00
openmpi@3.1.6%intel@19.0.5.281 osu_scatter

# OSU MPI Scatter Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.91
2                       0.89
4                       0.95
8                       0.80
16                      0.85
32                      0.86
64                      0.91
128                     1.04
256                     1.16
512                     3.56
1024                    3.85
2048                    4.90
4096                    8.51
8192                   10.90
16384                  21.04
32768                  32.88
65536                  47.79
131072                 92.68
262144                185.44
524288                425.00
1048576               742.19
openmpi@3.1.6%intel@19.0.5.281 osu_allgatherv

# OSU MPI Allgatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       4.23
2                       4.14
4                       5.03
8                       4.39
16                      4.53
32                      4.73
64                      4.60
128                     5.06
256                     5.82
512                     6.48
1024                    8.51
2048                   11.74
4096                   24.14
8192                   24.18
16384                  31.85
32768                  50.17
65536                 106.38
131072                209.38
262144                464.25
524288               1127.34
1048576              3441.50
openmpi@3.1.6%intel@19.0.5.281 osu_gather

# OSU MPI Gather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.31
2                       0.27
4                       0.28
8                       0.29
16                      0.38
32                      0.39
64                      0.59
128                     0.86
256                     0.85
512                     1.49
1024                    1.95
2048                    2.83
4096                    9.59
8192                   12.58
16384                  17.91
32768                  24.69
65536                  37.74
131072                 96.99
262144                144.61
524288                249.87
1048576               538.62
openmpi@3.1.6%intel@19.0.5.281 osu_ialltoallv

# OSU MPI Non-blocking All-to-Allv Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      11.14              5.68              4.72              0.00
2                       9.78              5.30              4.39              0.00
4                       9.75              5.22              4.35              0.00
8                       9.82              5.51              4.60              6.28
16                      8.91              4.74              3.99              0.00
32                      9.45              5.31              4.43              6.68
64                      8.92              4.76              3.93              0.00
128                     9.06              4.85              4.07              0.00
256                     9.68              5.16              4.31              0.00
512                    14.54              7.65              6.52              0.00
1024                   17.99              9.62              8.23              0.00
2048                   21.13             10.95              9.43              0.00
4096                   38.05             20.09             17.58              0.00
8192                   52.78             27.97             24.58              0.00
16384                  74.11             38.93             34.35              0.00
32768                 119.47             63.64             56.30              0.83
65536                 244.06            127.78            113.44              0.00
131072                562.85            277.58            243.90              0.00
262144               1497.48            743.03            660.16              0.00
524288               4000.84           1915.47           1704.20              0.00
1048576             11661.80           7221.68           6392.68             30.54
openmpi@3.1.6%intel@19.0.5.281 osu_ireduce

# OSU MPI Non-blocking Reduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       2.86              1.61              1.19              0.00
8                       2.80              1.58              1.14              0.00
16                      2.86              1.59              1.13              0.00
32                      4.89              1.59              1.15              0.00
64                      3.45              1.82              1.29              0.00
128                     7.13              1.68              1.27              0.00
256                     3.92              2.09              1.59              0.00
512                    17.25              7.44              6.33              0.00
1024                    7.50              3.39              2.74              0.00
2048                   15.14              5.74              3.15              0.00
4096                   46.53             13.61             11.65              0.00
8192                   29.77              9.01              7.43              0.00
16384                  76.51             25.56             21.35              0.00
32768                  61.73             22.76             20.00              0.00
65536                  96.31             34.38             30.37              0.00
131072                175.02             61.77             54.80              0.00
262144                332.21            113.22            100.53              0.00
524288                657.87            224.74            200.02              0.00
1048576              1382.50            463.93            414.92              0.00
openmpi@3.1.6%intel@19.0.5.281 osu_scatterv

# OSU MPI Scatterv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.82
2                       0.81
4                       0.86
8                       0.98
16                      0.88
32                      0.88
64                      0.93
128                     1.05
256                     1.23
512                     3.59
1024                    4.23
2048                    5.21
4096                    8.30
8192                   11.59
16384                  20.87
32768                  33.24
65536                  48.74
131072                 93.20
262144                190.11
524288                375.62
1048576               747.72
openmpi@3.1.6%intel@19.0.5.281 osu_allreduce

# OSU MPI Allreduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       1.69
8                       1.71
16                      1.56
32                      1.66
64                      1.68
128                     1.87
256                     2.12
512                     3.44
1024                    4.43
2048                    5.55
4096                   11.25
8192                   15.93
16384                  22.82
32768                  52.26
65536                  68.64
131072                113.07
262144                182.51
524288                346.68
1048576               990.85
openmpi@3.1.6%intel@19.0.5.281 osu_gatherv

# OSU MPI Gatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.30
2                       0.29
4                       0.30
8                       0.30
16                      0.30
32                      0.31
64                      0.31
128                     0.33
256                     0.39
512                     1.53
1024                    2.00
2048                    2.85
4096                    9.25
8192                   13.17
16384                  21.82
32768                  32.08
65536                  47.25
131072                 81.51
262144                145.42
524288                282.21
1048576               606.11
openmpi@3.1.6%intel@19.0.5.281 osu_ialltoallw

# OSU MPI Non-blocking All-to-Allw Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.35              5.06              4.25              0.00
2                       9.18              4.82              4.05              0.00
4                       9.09              4.91              4.10              0.00
8                       9.11              4.83              4.04              0.00
16                      9.30              4.91              4.05              0.00
32                      9.16              4.81              4.07              0.00
64                      9.51              5.16              4.28              0.00
128                     9.68              5.09              4.25              0.00
256                    10.01              5.42              4.54              0.00
512                    14.80              7.78              6.67              0.00
1024                   16.38              8.83              7.58              0.36
2048                   20.92             11.16              9.59              0.00
4096                   40.56             20.96             18.33              0.00
8192                   57.28             30.00             26.47              0.00
16384                  81.89             40.69             36.06              0.00
32768                 131.60             68.11             60.93              0.00
65536                 248.49            128.66            113.38              0.00
131072                536.88            272.80            242.80              0.00
262144               1655.67            833.99            745.47              0.00
524288               3866.34           1954.75           1737.45              0.00
1048576              8202.53           4260.08           3778.87              0.00
openmpi@3.1.6%intel@19.0.5.281 osu_iscatter

# OSU MPI Non-blocking Scatter Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       4.53              3.18              2.47             44.90
2                       4.25              2.91              2.33             42.49
4                       3.93              2.77              2.18             47.06
8                       2.97              1.98              1.55             35.92
16                      3.03              2.03              1.55             35.89
32                      3.38              2.10              1.57             18.83
64                      3.20              2.18              1.63             37.75
128                     3.47              2.25              1.77             31.06
256                     3.99              2.89              2.28             51.91
512                     4.27              2.83              2.26             36.36
1024                    4.62              3.10              2.49             38.76
2048                    5.92              4.16              3.39             48.06
4096                   14.96              7.41              6.39              0.00
8192                   20.78             10.49              9.14              0.00
16384                  32.98             15.63             13.71              0.00
32768                  53.34             24.04             21.20              0.00
65536                  94.38             40.42             35.77              0.00
131072                242.44            121.73            108.31              0.00
262144                549.11            268.32            239.37              0.00
524288               1143.78            570.35            510.03              0.00
1048576              2328.73           1163.73           1038.31              0.00
openmpi@3.1.6%intel@19.0.5.281 osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       5.38
2                       5.38
4                       4.97
8                       4.80
16                      4.09
32                      3.98
64                      3.84
128                     3.99
256                     4.07
512                     4.69
1024                    5.60
2048                    7.71
4096                   21.48
8192                   30.06
16384                  50.37
32768                  73.62
65536                 133.62
131072                270.58
262144                732.57
524288               1982.74
1048576              3924.34
openmpi@3.1.6%intel@19.0.5.281 osu_iallgather

# OSU MPI Non-blocking Allgather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.49              4.98              4.13              0.00
2                       9.07              4.86              4.02              0.00
4                       9.55              5.21              4.36              0.44
8                       8.74              4.64              3.85              0.00
16                      8.77              4.71              3.95              0.00
32                      8.74              4.66              3.86              0.00
64                      8.82              4.69              3.87              0.00
128                     9.94              5.27              4.46              0.00
256                    10.18              5.61              4.68              2.40
512                    14.59              7.91              6.74              0.90
1024                   16.32              8.67              7.36              0.00
2048                   20.69             11.44              9.92              6.78
4096                   38.17             20.26             17.79              0.00
8192                   54.19             28.04             24.68              0.00
16384                  76.08             40.78             36.02              2.00
32768                 118.03             61.85             54.95              0.00
65536                 306.21            117.90            103.20              0.00
131072                582.12            292.12            255.02              0.00
262144               1014.52            512.00            452.49              0.00
524288               2754.42           1456.00           1300.70              0.18
1048576              6091.31           3052.28           2710.81              0.00
openmpi@3.1.6%intel@19.0.5.281 osu_ibarrier

# OSU MPI Non-blocking Barrier Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

       Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
             13.51              5.52              2.91              0.00
openmpi@3.1.6%intel@19.0.5.281 osu_iscatterv

# OSU MPI Non-blocking Scatterv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.49              2.09              1.55              9.93
2                       3.56              2.18              1.67             17.26
4                       4.99              2.18              1.60              0.00
8                       3.21              2.09              1.55             27.46
16                      4.37              2.20              1.68              0.00
32                      4.71              3.69              1.69             39.53
64                      3.24              2.22              1.72             40.41
128                     3.55              2.45              1.88             41.59
256                     3.91              2.79              2.14             47.43
512                     4.31              2.85              2.21             33.80
1024                    5.28              3.16              2.54             16.50
2048                   29.82             16.88             13.63              5.07
4096                   31.59             10.31              8.07              0.00
8192                  109.95             32.52             24.06              0.00
16384                  34.36             15.00             12.95              0.00
32768                  55.28             23.88             20.87              0.00
65536                 135.99             47.10             36.45              0.00
131072                582.51            192.33            163.62              0.00
262144                851.39            348.07            284.08              0.00
524288               1759.03            750.69            628.20              0.00
1048576              2554.07           1236.51           1096.29              0.00
openmpi@3.1.6%intel@19.0.5.281 osu_alltoallv

# OSU MPI All-to-Allv Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.31
2                       3.60
4                       3.38
8                       3.87
16                      3.71
32                      3.60
64                      3.72
128                     4.15
256                     4.62
512                     7.36
1024                    8.12
2048                   11.86
4096                   22.46
8192                   32.33
16384                  51.68
32768                  78.15
65536                 206.76
131072                334.02
262144                692.22
524288               1789.29
1048576              3630.26
openmpi@3.1.6%intel@19.0.5.281 osu_iallgatherv

# OSU MPI Non-blocking Allgatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      10.99              5.77              4.92              0.00
2                      12.92              6.95              4.87              0.00
4                      13.89              6.34              4.77              0.00
8                       8.89              4.73              3.91              0.00
16                      9.57              4.84              4.05              0.00
32                      9.00              4.82              4.03              0.00
64                      9.28              4.94              4.15              0.00
128                     9.40              5.11              4.23              0.00
256                     9.53              5.14              4.29              0.00
512                    14.24              7.48              6.41              0.00
1024                   16.84              8.88              7.66              0.00
2048                   19.78             10.47              9.08              0.00
4096                   38.28             20.45             17.92              0.51
8192                   55.94             28.89             25.47              0.00
16384                  74.27             39.67             35.06              1.31
32768                 127.11             63.65             56.54              0.00
65536                 245.19            121.05            106.68              0.00
131072                486.19            254.07            226.34              0.00
262144               1742.88            550.73            450.41              0.00
524288               2670.89           1340.30           1191.07              0.00
1048576              6252.47           3211.06           2863.43              0.00
openmpi@3.1.6%intel@19.0.5.281 osu_ibcast

# OSU MPI Non-Blocking Broadcast Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.66              2.34              1.71             22.65
2                       3.67              2.35              1.72             22.66
4                       3.57              2.12              1.60              9.00
8                       3.42              2.08              1.59             15.46
16                      3.73              2.09              1.58              0.00
32                      3.38              2.10              1.59             19.88
64                      3.57              2.09              1.62              8.77
128                     3.97              2.23              1.75              0.31
256                     3.94              2.44              1.96             23.25
512                     5.36              2.97              2.44              2.29
1024                    5.64              3.34              2.72             15.27
2048                    7.13              4.12              3.34             10.01
4096                   11.57              6.99              6.00             23.69
8192                   15.97              9.50              8.13             20.37
16384                  24.03             13.03             11.37              3.27
32768                  37.89             20.72             17.99              4.56
65536                  57.30             30.01             26.31              0.00
131072                117.63             76.86             68.20             40.23
262144                232.73            133.22            115.43             13.80
524288                410.74            271.45            241.68             42.37
1048576               833.48            543.71            484.08             40.14
openmpi@3.1.6%intel@19.0.5.281 osu_reduce

# OSU MPI Reduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.70
8                       0.64
16                      0.65
32                      0.66
64                      0.73
128                     0.84
256                     1.00
512                     1.75
1024                    2.03
2048                    3.56
4096                    6.98
8192                   14.19
16384                  13.98
32768                  23.37
65536                  49.76
131072                102.01
262144                181.81
524288                320.11
1048576               622.66
openmpi@3.1.6%intel@19.0.5.281 osu_barrier

# OSU MPI Barrier Latency Test v5.6.3
# Avg Latency(us)
             1.15
openmpi@3.1.6%intel@19.0.5.281 osu_iallreduce

# OSU MPI Non-blocking Allreduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       7.39              4.08              3.27              0.00
8                       6.87              3.71              2.95              0.00
16                      6.73              3.82              3.07              5.43
32                      7.05              3.55              2.92              0.00
64                      8.66              5.05              4.14             12.82
128                     8.26              4.52              3.68              0.00
256                     8.27              4.49              3.68              0.00
512                    11.25              6.00              5.10              0.00
1024                   13.80              7.42              6.34              0.00
2048                   18.58              9.70              8.40              0.00
4096                   33.36             16.49             14.41              0.00
8192                   51.46             25.92             22.73              0.00
16384                  83.92             42.72             37.68              0.00
32768                 139.54             67.87             59.90              0.00
65536                 191.58             91.73             81.37              0.00
131072                322.17            132.32            115.48              0.00
262144                402.25            205.43            182.39              0.00
524288                748.19            401.68            358.14              3.25
1048576              1480.65            753.75            673.35              0.00
openmpi@3.1.6%intel@19.0.5.281 osu_igather

# OSU MPI Non-blocking Gather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.47              1.35              0.90              0.00
2                       2.44              1.34              0.88              0.00
4                       2.50              1.33              0.88              0.00
8                       2.44              1.34              0.87              0.00
16                      2.48              1.39              0.87              0.00
32                      2.28              1.26              0.77              0.00
64                      2.42              1.36              0.89              0.00
128                     2.36              1.29              0.83              0.00
256                     2.29              1.25              0.83              0.00
512                     7.04              3.45              2.76              0.00
1024                    7.87              3.78              3.06              0.00
2048                   10.15              4.57              3.86              0.00
4096                   18.82              7.49              6.36              0.00
8192                   27.78             11.45              9.90              0.00
16384                  43.73             19.98             17.49              0.00
32768                  61.06             26.97             23.29              0.00
65536                  93.74             40.28             35.61              0.00
131072                182.75             79.07             70.20              0.00
262144                366.98            178.24            158.51              0.00
524288                768.97            370.55            327.38              0.00
1048576              1731.39            825.52            730.04              0.00
openmpi@3.1.6%intel@19.0.5.281 osu_reduce_scatter

# OSU MPI Reduce_scatter Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.93
8                       1.23
16                      1.20
32                      1.68
64                      1.71
128                     1.93
256                     1.94
512                     4.39
1024                    2.98
2048                    4.01
4096                    4.91
8192                    7.48
16384                  12.41
32768                  19.77
65536                  32.60
131072                154.42
262144                369.74
524288                255.89
1048576               557.28
openmpi@3.1.6%intel@19.0.5.281 osu_acc_latency
# OSU MPI_Accumulate latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.04
2                       0.04
4                       0.04
8                       0.05
16                      0.06
32                      0.08
64                      0.10
128                     0.14
256                     0.25
512                     0.43
1024                    0.80
2048                    1.51
4096                    3.01
8192                    5.84
16384                  11.77
32768                  23.64
65536                  46.90
131072                 93.49
262144                191.22
524288                373.15
1048576               807.06
2097152              1504.05
4194304              3025.12
openmpi@3.1.6%intel@19.0.5.281 osu_fop_latency
# OSU MPI_Fetch_and_op latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.04
openmpi@3.1.6%intel@19.0.5.281 osu_get_bw
# OSU MPI_Get Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      29.26
2                      53.35
4                     129.08
8                     232.75
16                    459.38
32                    977.86
64                   1675.45
128                  4053.98
256                  7742.11
512                 13307.77
1024                21749.20
2048                29635.46
4096                34553.78
8192                22092.82
16384               14376.98
32768               12055.41
65536               11633.95
131072              10665.34
262144               5564.94
524288               4749.93
1048576              4351.38
2097152              4595.23
4194304              4677.29
openmpi@3.1.6%intel@19.0.5.281 osu_put_bibw
# OSU MPI_Put Bi-directional Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_post/start/complete/wait
# Size      Bandwidth (MB/s)
1                      34.13
2                      89.89
4                     164.95
8                     360.66
16                    615.23
32                   1447.56
64                   2816.90
128                  4998.38
256                 10430.06
512                 19078.78
1024                25738.85
2048                35054.85
4096                44654.57
8192                35593.49
16384               30004.75
32768               23389.71
65536               20292.21
131072              13366.40
262144               9108.29
524288               8453.53
1048576              8537.94
2097152              8460.00
4194304              8488.45
openmpi@3.1.6%intel@19.0.5.281 osu_put_latency
# OSU MPI_Put Latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.04
2                       0.04
4                       0.04
8                       0.04
16                      0.04
32                      0.04
64                      0.04
128                     0.04
256                     0.04
512                     0.04
1024                    0.05
2048                    0.06
4096                    0.16
8192                    0.28
16384                   0.50
32768                   1.58
65536                   2.12
131072                  6.25
262144                 12.12
524288                 24.31
1048576                60.60
2097152               170.74
4194304               355.81
openmpi@3.1.6%intel@19.0.5.281 osu_cas_latency
# OSU MPI_Compare_and_swap latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.04
openmpi@3.1.6%intel@19.0.5.281 osu_get_acc_latency
# OSU MPI_Get_accumulate latency Test v5.6.3
# Window creation: MPI_Win_create
# Synchronization: MPI_Win_lock/unlock
# Size          Latency (us)
1                       3.15
2                       3.12
4                       3.15
8                       3.11
16                      3.17
32                      3.36
64                      3.22
128                     3.41
256                     3.67
512                     3.99
1024                    4.81
2048                    6.90
4096                    9.56
8192                   14.33
16384                  25.05
32768                  43.77
65536                  78.03
131072                146.31
262144                279.49
524288                562.77
1048576              1132.34
2097152              2157.79
4194304              4506.81
openmpi@3.1.6%intel@19.0.5.281 osu_get_latency
# OSU MPI_Get latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.04
2                       0.04
4                       0.04
8                       0.04
16                      0.04
32                      0.04
64                      0.04
128                     0.04
256                     0.04
512                     0.04
1024                    0.05
2048                    0.06
4096                    0.08
8192                    0.13
16384                   0.31
32768                   1.20
65536                   2.53
131072                  5.53
262144                 11.03
524288                 25.15
1048576                64.53
2097152               169.11
4194304               356.51
openmpi@3.1.6%intel@19.0.5.281 osu_put_bw
# OSU MPI_Put Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      19.97
2                      34.55
4                      78.24
8                     147.89
16                    278.91
32                    613.85
64                   1070.59
128                  2347.28
256                  4350.14
512                  7256.91
1024                10824.26
2048                14134.66
4096                18253.63
8192                14228.70
16384               11410.04
32768                9538.47
65536                8391.24
131072               9378.04
262144               4781.64
524288               4477.02
1048576              4580.57
2097152              4587.38
4194304              4577.24
-- linux-debian9-cascadelake / intel@19.0.5.281 -----------------
hwloc@2.2.0
libiconv@1.16
libpciaccess@0.16
libxml2@2.9.10
numactl@2.0.14
openmpi@4.0.5
osu-micro-benchmarks@5.6.3
xz@5.2.5
zlib@1.2.11
openmpi@4.0.5%intel@19.0.5.281 osu_hello
# OSU MPI Hello World Test v5.6.3
This is a test with 2 processes
openmpi@4.0.5%intel@19.0.5.281 osu_init
# OSU MPI Init Test v5.6.3
nprocs: 2, min: 18 ms, max: 19 ms, avg: 18 ms
openmpi@4.0.5%intel@19.0.5.281 osu_bibw
# OSU MPI Bi-Directional Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       9.38
2                      20.41
4                      33.91
8                      78.17
16                    104.23
32                    263.65
64                    399.20
128                   586.26
256                   994.84
512                  2078.18
1024                 3473.33
2048                 5567.63
4096                 5089.26
8192                 7552.06
16384                8777.03
32768               10053.49
65536               10101.57
131072              10620.82
262144              10922.64
524288              11028.38
1048576             10735.00
2097152             10979.95
4194304             10954.07
openmpi@4.0.5%intel@19.0.5.281 osu_latency
# OSU MPI Latency Test v5.6.3
# Size          Latency (us)
0                       0.26
1                       0.29
2                       0.29
4                       0.29
8                       0.29
16                      0.30
32                      0.31
64                      0.33
128                     0.38
256                     0.45
512                     0.64
1024                    0.76
2048                    0.96
4096                    2.06
8192                    3.22
16384                   5.34
32768                   8.05
65536                  14.58
131072                 28.02
262144                 55.44
524288                110.14
1048576               229.04
2097152               481.64
4194304              1149.12
openmpi@4.0.5%intel@19.0.5.281 osu_latency_mt
# Number of Sender threads: 1
# Number of Receiver threads: 2
# OSU MPI Multi-threaded Latency Test v5.6.3
# Size          Latency (us)
0                       0.39
1                       0.44
2                       0.44
4                       0.44
8                       0.44
16                      0.45
32                      0.45
64                      0.48
128                     0.53
256                     0.58
512                     1.04
1024                    1.11
2048                    1.23
4096                    4.15
8192                    6.59
16384                   5.64
32768                   8.24
65536                  15.01
131072                 28.81
262144                109.26
524288                180.67
1048576               410.83
2097152               858.45
4194304              2196.37
openmpi@4.0.5%intel@19.0.5.281 osu_multi_lat
# OSU MPI Multi Latency Test v5.6.3
# Size          Latency (us)
0                       0.24
1                       0.28
2                       0.27
4                       0.28
8                       0.27
16                      0.29
32                      0.29
64                      0.33
128                     0.36
256                     0.44
512                     0.62
1024                    0.74
2048                    0.92
4096                    2.08
8192                    2.94
16384                   5.17
32768                   7.96
65536                  14.53
131072                 28.09
262144                 55.46
524288                112.81
1048576               228.60
2097152               484.50
4194304              1081.60
openmpi@4.0.5%intel@19.0.5.281 osu_bw
# OSU MPI Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       8.24
2                      17.31
4                      32.83
8                      69.19
16                    135.11
32                    267.09
64                    417.26
128                   541.95
256                   816.69
512                  1889.68
1024                 3147.37
2048                 5132.74
4096                 4768.33
8192                 6917.90
16384                8563.48
32768                9144.86
65536                9964.01
131072               8710.03
262144              10486.81
524288              10641.49
1048576             10621.07
2097152             10617.73
4194304             10341.12
openmpi@4.0.5%intel@19.0.5.281 osu_latency_mp
# OSU MPI Multi-process Latency Test v5.6.3
# Number of forked processes in sender: 2
# Number of forked processes in receiver: 2
# Size          Latency (us)
0                       0.25
1                       0.28
2                       0.28
4                       0.29
8                       0.28
16                      0.31
32                      0.30
64                      0.33
128                     0.37
256                     0.44
512                     0.63
1024                    0.77
2048                    0.95
4096                    2.09
8192                    2.95
16384                   5.23
32768                   7.95
65536                  14.69
131072                 28.32
262144                 55.83
524288                111.65
1048576               238.71
2097152               493.62
4194304              1056.68
openmpi@4.0.5%intel@19.0.5.281 osu_mbw_mr
# OSU MPI Multiple Bandwidth / Message Rate Test v5.6.3
# [ pairs: 1 ] [ window size: 64 ]
# Size                  MB/s        Messages/s
1                       7.21        7209943.41
2                      13.91        6956189.05
4                      26.90        6725748.16
8                      59.72        7464958.44
16                    108.13        6758190.87
32                    187.79        5868560.74
64                    370.63        5791095.29
128                   554.77        4334146.44
256                   867.60        3389051.46
512                  1961.03        3830128.99
1024                 3226.58        3150958.48
2048                 5232.29        2554830.05
4096                 4895.51        1195192.04
8192                 7298.68         890952.68
16384                9362.71         571454.49
32768                9411.56         287217.91
65536               10058.56         153481.44
131072              10268.70          78343.99
262144              10235.35          39044.76
524288              10485.63          19999.75
1048576             10591.09          10100.45
2097152             10467.21           4991.15
4194304             10367.73           2471.86
openmpi@4.0.5%intel@19.0.5.281 osu_allgather

# OSU MPI Allgather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       1.67
2                       1.64
4                       1.84
8                       1.71
16                      1.73
32                      1.85
64                      1.96
128                     2.76
256                     3.44
512                     4.84
1024                    6.96
2048                   11.12
4096                   14.96
8192                   28.01
16384                  39.36
32768                  63.00
65536                 110.68
131072                218.81
262144                481.53
524288               1127.45
1048576              3363.58
openmpi@4.0.5%intel@19.0.5.281 osu_bcast

# OSU MPI Broadcast Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.95
2                       0.92
4                       0.88
8                       0.93
16                      0.93
32                      0.95
64                      1.02
128                     1.14
256                     1.34
512                     1.72
1024                    2.21
2048                    4.12
4096                    6.32
8192                   12.15
16384                  21.48
32768                  40.42
65536                  80.37
131072                155.52
262144                295.00
524288                316.82
1048576               645.12
openmpi@4.0.5%intel@19.0.5.281 osu_ialltoall

# OSU MPI Non-blocking All-to-All Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      10.76              5.98              5.03              5.06
2                      10.48              5.74              4.78              0.82
4                      10.97              6.41              5.36             14.87
8                       9.01              4.70              3.97              0.00
16                      8.92              4.80              4.04              0.00
32                      9.11              4.67              3.86              0.00
64                      9.21              4.91              4.10              0.00
128                    13.13              8.77              7.55             42.31
256                    10.62              5.80              4.87              1.17
512                    14.76              7.86              6.69              0.00
1024                   17.81              9.28              8.00              0.00
2048                   20.97             11.30              9.77              1.01
4096                   49.16             25.89             22.83              0.00
8192                   62.72             32.99             29.16              0.00
16384                  76.93             40.24             35.49              0.00
32768                 143.73             70.89             63.08              0.00
65536                 283.72            147.52            131.62              0.00
131072                543.93            281.02            251.39              0.00
262144               1287.64            669.29            595.57              0.00
524288               3721.96           1942.38           1735.51              0.00
1048576              7419.11           3764.99           3355.22              0.00
openmpi@4.0.5%intel@19.0.5.281 osu_igatherv

# OSU MPI Non-blocking Gatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.50              1.33              0.87              0.00
2                       2.44              1.29              0.84              0.00
4                       2.18              1.19              0.76              0.00
8                       2.36              1.22              0.76              0.00
16                      2.32              1.26              0.83              0.00
32                      2.55              1.41              0.90              0.00
64                      2.36              1.27              0.84              0.00
128                     2.45              1.29              0.86              0.00
256                     2.33              1.25              0.85              0.00
512                     6.91              3.25              2.59              0.00
1024                   12.04              3.93              3.25              0.00
2048                  106.28             13.43             10.38              0.00
4096                   20.02              8.96              7.68              0.00
8192                   27.89             12.11             10.44              0.00
16384                  40.47             17.49             15.23              0.00
32768                  72.30             36.93             32.68              0.00
65536                 127.75             66.16             58.73              0.00
131072                235.60            125.52            112.09              1.79
262144                448.25            233.69            207.75              0.00
524288                917.40            483.76            432.98              0.00
1048576              2000.56           1026.19            913.72              0.00
openmpi@4.0.5%intel@19.0.5.281 osu_scatter

# OSU MPI Scatter Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.83
2                       0.83
4                       0.83
8                       1.12
16                      0.90
32                      0.96
64                      1.08
128                     1.20
256                     1.43
512                     3.35
1024                    4.06
2048                    4.91
4096                    9.92
8192                   13.64
16384                  23.30
32768                  37.30
65536                  70.12
131072                146.57
262144                299.84
524288                601.19
1048576              1266.25
openmpi@4.0.5%intel@19.0.5.281 osu_allgatherv

# OSU MPI Allgatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       5.44
2                       5.96
4                       5.44
8                       5.42
16                      4.76
32                      4.92
64                      5.10
128                     5.79
256                     6.61
512                     8.30
1024                   17.15
2048                   19.21
4096                   33.77
8192                   99.26
16384                  83.92
32768                 125.75
65536                 344.13
131072                715.34
262144               1477.75
524288               3545.90
1048576              4810.87
openmpi@4.0.5%intel@19.0.5.281 osu_gather

# OSU MPI Gather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.41
2                       0.41
4                       0.43
8                       0.47
16                      0.46
32                      0.45
64                      0.46
128                     0.46
256                     0.49
512                     1.62
1024                    2.33
2048                    3.06
4096                   10.75
8192                   15.81
16384                  21.08
32768                  30.19
65536                  54.44
131072                114.07
262144                211.31
524288                695.33
1048576              1432.31
openmpi@4.0.5%intel@19.0.5.281 osu_ialltoallv

# OSU MPI Non-blocking All-to-Allv Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.76              5.24              4.37              0.00
2                       9.79              5.07              4.24              0.00
4                       9.13              4.93              4.07              0.00
8                       9.60              4.90              4.06              0.00
16                     10.36              5.67              4.70              0.30
32                     10.47              5.49              4.51              0.00
64                     10.26              5.55              4.65              0.00
128                     9.90              5.31              4.44              0.00
256                    10.76              5.76              4.82              0.00
512                    29.14              9.84              8.43              0.00
1024                   18.74             10.24              8.83              3.76
2048                   21.51             11.62             10.07              1.71
4096                   49.95             26.24             23.09              0.00
8192                   65.40             34.59             30.49              0.00
16384                  78.65             40.52             35.68              0.00
32768                 132.67             71.14             63.18              2.60
65536                 249.10            128.65            114.63              0.00
131072                554.83            287.49            256.69              0.00
262144               1317.65            630.92            563.19              0.00
524288               3728.83           2044.27           1827.47              7.82
1048576              7398.15           3862.25           3447.22              0.00
openmpi@4.0.5%intel@19.0.5.281 osu_ireduce

# OSU MPI Non-blocking Reduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       2.94              1.59              1.12              0.00
8                       2.86              1.59              1.12              0.00
16                      2.96              1.74              1.27              3.71
32                      2.89              1.59              1.15              0.00
64                      2.94              1.65              1.22              0.00
128                     3.07              1.71              1.29              0.00
256                     3.27              1.86              1.41              0.00
512                     6.52              3.37              2.72              0.00
1024                    8.64              3.36              2.62              0.00
2048                    8.40              3.77              3.07              0.00
4096                   18.51              7.67              6.50              0.00
8192                   25.33              9.66              7.98              0.00
16384                  39.66             15.07             13.11              0.00
32768                  66.03             24.62             21.64              0.00
65536                 122.01             44.61             39.49              0.00
131072                230.49             83.95             74.45              0.00
262144                453.79            162.72            145.08              0.00
524288                999.80            364.86            324.95              0.00
1048576              1991.50            691.73            614.15              0.00
openmpi@4.0.5%intel@19.0.5.281 osu_scatterv

# OSU MPI Scatterv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.90
2                       0.83
4                       0.80
8                       0.79
16                      0.89
32                      0.88
64                      2.23
128                     1.11
256                     1.34
512                     3.53
1024                    7.41
2048                    5.06
4096                   10.44
8192                   13.61
16384                  28.18
32768                  37.06
65536                  67.91
131072                140.49
262144                293.48
524288                597.90
1048576              1358.82
openmpi@4.0.5%intel@19.0.5.281 osu_allreduce

# OSU MPI Allreduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       1.55
8                       1.67
16                      1.65
32                      1.61
64                      1.70
128                     2.02
256                     5.18
512                    15.29
1024                    9.09
2048                    8.06
4096                   12.66
8192                   16.73
16384                  22.74
32768                  58.45
65536                  86.91
131072                117.32
262144                209.64
524288                433.21
1048576               937.64
openmpi@4.0.5%intel@19.0.5.281 osu_gatherv

# OSU MPI Gatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.32
2                       0.29
4                       0.31
8                       0.29
16                      0.29
32                      0.32
64                      0.32
128                     0.35
256                     0.38
512                     1.74
1024                    2.35
2048                    3.38
4096                    9.56
8192                   13.21
16384                  21.38
32768                  36.32
65536                  67.20
131072                133.45
262144                269.48
524288                556.05
1048576              1294.92
openmpi@4.0.5%intel@19.0.5.281 osu_ialltoallw

# OSU MPI Non-blocking All-to-Allw Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.76              5.19              4.30              0.00
2                       9.15              4.89              4.04              0.00
4                       9.99              5.32              4.41              0.00
8                       9.97              5.28              4.46              0.00
16                     11.52              5.56              4.52              0.00
32                     15.39              9.44              7.86             24.29
64                      8.94              4.73              3.94              0.00
128                     9.35              4.92              4.06              0.00
256                    10.92              5.50              4.51              0.00
512                    19.62             12.28             10.66             31.14
1024                   20.35             10.28              8.85              0.00
2048                   27.03             12.72             10.69              0.00
4096                   51.53             27.11             23.88              0.00
8192                   68.76             33.36             29.38              0.00
16384                  76.66             40.17             35.57              0.00
32768                 145.82             75.15             66.66              0.00
65536                 238.08            124.96            111.21              0.00
131072                512.92            268.38            239.17              0.00
262144               1263.56            635.43            567.06              0.00
524288               3584.61           1705.94           1528.20              0.00
1048576              7452.71           3904.22           3480.22              0.00
openmpi@4.0.5%intel@19.0.5.281 osu_iscatter

# OSU MPI Non-blocking Scatter Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.86              2.70              2.15             46.33
2                       3.31              2.26              1.70             38.35
4                       3.24              2.17              1.62             33.85
8                       3.27              2.20              1.68             36.03
16                      3.18              2.16              1.69             39.11
32                      3.36              2.28              1.74             37.82
64                      3.44              2.32              1.81             38.51
128                     3.78              2.70              2.10             48.62
256                     4.15              2.97              2.32             48.99
512                     4.26              2.81              2.21             34.24
1024                    5.08              3.44              2.77             40.70
2048                    6.02              4.18              3.34             44.89
4096                   16.44              7.39              6.34              0.00
8192                   49.12             12.96             11.24              0.00
16384                  32.45             13.75             12.03              0.00
32768                  59.29             29.83             26.40              0.00
65536                 110.95             55.73             49.45              0.00
131072                235.28            122.07            108.57              0.00
262144                639.22            294.75            258.58              0.00
524288               1047.05            542.69            478.25              0.00
1048576              3414.15           1335.48           1125.13              0.00
openmpi@4.0.5%intel@19.0.5.281 osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       4.54
2                       4.30
4                       4.29
8                      22.05
16                      5.65
32                     18.62
64                      7.22
128                    16.40
256                    10.68
512                     7.31
1024                    6.76
2048                    8.04
4096                   26.37
8192                  110.54
16384                 137.23
32768                  94.48
65536                 149.15
131072                535.96
262144               1272.43
524288               2730.19
1048576              4269.18
openmpi@4.0.5%intel@19.0.5.281 osu_iallgather

# OSU MPI Non-blocking Allgather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.33              4.92              4.04              0.00
2                       9.49              5.20              4.28              0.00
4                       9.57              5.23              4.38              1.04
8                       9.26              4.98              4.14              0.00
16                      9.51              5.46              4.59             11.70
32                      9.38              5.16              3.80              0.00
64                      9.34              5.18              3.82              0.00
128                     9.25              5.12              4.24              2.68
256                    12.31              7.73              6.53             29.76
512                    16.60              8.13              6.89              0.00
1024                   22.89             14.27             12.41             30.54
2048                   22.24             11.63             10.09              0.00
4096                   49.84             26.18             23.05              0.00
8192                   65.26             35.05             30.81              1.96
16384                  76.14             40.41             35.65              0.00
32768                 132.60             71.75             63.44              4.08
65536                 227.54            118.45            105.12              0.00
131072                446.01            227.61            202.39              0.00
262144                955.07            496.95            442.25              0.00
524288               2426.46           1196.52           1054.79              0.00
1048576              6021.35           3256.94           2894.51              4.49
openmpi@4.0.5%intel@19.0.5.281 osu_ibarrier

# OSU MPI Non-blocking Barrier Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

       Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
              9.98              6.54              5.51             37.57
openmpi@4.0.5%intel@19.0.5.281 osu_iscatterv

# OSU MPI Non-blocking Scatterv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.47              2.28              1.75             32.35
2                       3.60              2.38              1.88             35.36
4                       3.36              2.30              1.78             40.11
8                       3.28              2.20              1.75             38.24
16                      3.39              2.33              1.80             41.09
32                      3.39              2.32              1.82             41.64
64                      3.61              2.50              1.90             41.80
128                     3.81              2.65              2.08             44.01
256                     4.00              2.83              2.30             49.03
512                     4.31              2.84              2.24             34.26
1024                    5.10              3.42              2.76             39.39
2048                    5.69              4.00              3.31             48.95
4096                   16.50              7.36              6.30              0.00
8192                   60.96             23.04              8.12              0.00
16384                 112.69             54.23             12.35              0.00
32768                 437.39            125.89            101.96              0.00
65536                 118.25             56.16             49.66              0.00
131072                249.00            123.70            110.22              0.00
262144                516.42            268.84            239.90              0.00
524288               1046.27            537.11            481.52              0.00
1048576              2129.80           1090.62            967.11              0.00
openmpi@4.0.5%intel@19.0.5.281 osu_alltoallv

# OSU MPI All-to-Allv Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.49
2                       3.50
4                       3.44
8                       3.60
16                      3.55
32                      3.94
64                      3.88
128                     4.20
256                     5.14
512                     7.45
1024                    8.32
2048                   10.14
4096                   26.73
8192                   39.80
16384                  54.14
32768                  84.62
65536                 143.30
131072                308.10
262144                692.16
524288               1903.50
1048576              4011.16
openmpi@4.0.5%intel@19.0.5.281 osu_iallgatherv

# OSU MPI Non-blocking Allgatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.30              5.18              4.35              5.26
2                       8.87              4.84              3.99              0.00
4                       8.69              4.71              3.93              0.00
8                       8.56              4.55              3.78              0.00
16                     11.57              4.62              3.84              0.00
32                      8.81              4.62              3.86              0.00
64                     11.36              7.06              6.01             28.54
128                     9.41              5.07              4.25              0.00
256                    10.58              5.85              4.85              2.50
512                    14.87              7.92              6.74              0.00
1024                   18.41              9.68              8.31              0.00
2048                   21.40             11.49              9.93              0.25
4096                   50.21             26.98             23.78              2.33
8192                   65.07             33.71             29.76              0.00
16384                  75.06             39.55             35.04              0.00
32768                 135.83             69.02             61.27              0.00
65536                 244.71            123.06            109.86              0.00
131072                445.73            234.53            209.02              0.00
262144                904.67            474.99            423.98              0.00
524288               2657.71           1283.59           1138.07              0.00
1048576              6432.49           3153.59           2803.28              0.00
openmpi@4.0.5%intel@19.0.5.281 osu_ibcast

# OSU MPI Non-Blocking Broadcast Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.40              2.12              1.63             21.66
2                       3.31              2.10              1.56             21.98
4                       3.33              2.05              1.48             12.88
8                       3.43              2.09              1.48              9.23
16                      3.33              2.02              1.49             12.22
32                      3.32              2.03              1.49             13.13
64                      3.36              2.05              1.52             13.93
128                     3.54              2.13              1.65             14.89
256                     3.76              2.35              1.81             22.14
512                     5.17              2.90              2.30              1.11
1024                    6.05              3.44              2.81              7.33
2048                    6.79              4.09              3.40             20.62
4096                   12.91              7.33              6.24             10.48
8192                   17.62              9.78              8.39              6.58
16384                  25.24             14.08             12.29              9.19
32768                  43.13             24.62             21.57             14.19
65536                  71.58             41.58             36.82             18.54
131072                134.66             78.12             69.07             18.13
262144                278.65            169.02            150.69             27.25
524288                526.74            302.55            270.43             17.10
1048576              1067.23            607.11            541.12             14.97
openmpi@4.0.5%intel@19.0.5.281 osu_reduce

# OSU MPI Reduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.60
8                       1.53
16                      0.63
32                      0.63
64                      0.66
128                     0.79
256                     0.85
512                     1.59
1024                    2.03
2048                    3.48
4096                    6.93
8192                   14.89
16384                  14.33
32768                  24.82
65536                  52.52
131072                133.54
262144                209.47
524288                349.35
1048576               666.12
openmpi@4.0.5%intel@19.0.5.281 osu_barrier

# OSU MPI Barrier Latency Test v5.6.3
# Avg Latency(us)
             1.21
openmpi@4.0.5%intel@19.0.5.281 osu_iallreduce

# OSU MPI Non-blocking Allreduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       8.17              4.68              3.95             11.69
8                       7.89              4.29              3.58              0.00
16                      8.26              4.55              3.72              0.40
32                      8.01              4.38              3.62              0.00
64                      8.78              4.87              4.00              2.32
128                     9.42              5.16              4.28              0.64
256                     9.61              5.65              4.69             15.56
512                    10.95              5.78              4.87              0.00
1024                   15.18              8.30              7.07              2.68
2048                   19.42              9.74              8.38              0.00
4096                   44.01             18.65             15.93              0.00
8192                   54.83             27.18             23.99              0.00
16384                  85.28             43.19             38.20              0.00
32768                 157.24             80.43             71.41              0.00
65536                 211.69            110.24             98.27              0.00
131072                294.08            168.12            149.64             15.82
262144                547.25            235.92            202.74              0.00
524288               1733.02            542.44            415.69              0.00
1048576              1763.75            964.91            863.55              7.49
openmpi@4.0.5%intel@19.0.5.281 osu_igather

# OSU MPI Non-blocking Gather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.43              1.32              0.86              0.00
2                       2.41              1.28              0.86              0.00
4                       2.30              1.24              0.82              0.00
8                       2.37              1.31              0.85              0.00
16                      2.46              1.36              0.90              0.00
32                      2.36              1.25              0.82              0.00
64                      2.54              1.34              0.87              0.00
128                     2.34              1.30              0.84              0.00
256                     2.27              1.25              0.83              0.00
512                     6.97              3.32              2.64              0.00
1024                    8.68              4.11              3.34              0.00
2048                   10.04              4.82              3.98              0.00
4096                   19.53              8.75              7.53              0.00
8192                   28.37             11.93             10.37              0.00
16384                  40.06             17.28             15.19              0.00
32768                  70.99             36.66             32.45              0.00
65536                 124.39             64.95             57.60              0.00
131072                236.79            124.54            109.97              0.00
262144                461.31            240.06            214.08              0.00
524288                960.79            489.14            432.54              0.00
1048576              4033.34           1676.04           1375.72              0.00
openmpi@4.0.5%intel@19.0.5.281 osu_reduce_scatter

# OSU MPI Reduce_scatter Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.98
8                       1.07
16                      1.29
32                      1.96
64                      1.95
128                     6.90
256                     1.90
512                     2.06
1024                    3.25
2048                   15.46
4096                   11.59
8192                   10.60
16384                  14.04
32768                  39.59
65536                 140.36
131072                332.60
262144                479.89
524288                501.35
1048576              1018.66
openmpi@4.0.5%intel@19.0.5.281 osu_acc_latency
# OSU MPI_Accumulate latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.05
2                       0.05
4                       0.05
8                       0.06
16                      0.06
32                      0.07
64                      0.09
128                     0.14
256                     0.24
512                     0.43
1024                    0.79
2048                    1.55
4096                    2.99
8192                    5.97
16384                  12.07
32768                  23.59
65536                  46.64
131072                 92.99
262144                187.90
524288                376.19
1048576               752.79
2097152              1511.89
4194304              3023.59
openmpi@4.0.5%intel@19.0.5.281 osu_fop_latency
# OSU MPI_Fetch_and_op latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.06
openmpi@4.0.5%intel@19.0.5.281 osu_get_bw
# OSU MPI_Get Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      34.55
2                      65.80
4                     131.88
8                     276.72
16                    533.22
32                   1044.96
64                   1909.07
128                  3638.51
256                  7908.48
512                 14187.86
1024                21721.81
2048                29253.62
4096                34854.85
8192                14837.14
16384               11061.31
32768               10165.55
65536               10920.47
131072              10557.66
262144               6261.39
524288               4682.30
1048576              4507.16
2097152              4668.46
4194304              4527.16
openmpi@4.0.5%intel@19.0.5.281 osu_put_bibw
# OSU MPI_Put Bi-directional Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_post/start/complete/wait
# Size      Bandwidth (MB/s)
1                      36.40
2                      80.20
4                     173.29
8                     322.95
16                    713.75
32                   1445.86
64                   2755.30
128                  5810.44
256                 10091.47
512                 13350.45
1024                25592.55
2048                37728.77
4096                44371.40
8192                39208.48
16384               28517.78
32768               23581.03
65536               21740.01
131072              11668.51
262144               8862.07
524288               8648.30
1048576              8276.65
2097152              8447.26
4194304              8912.40
openmpi@4.0.5%intel@19.0.5.281 osu_put_latency
# OSU MPI_Put Latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.04
2                       0.04
4                       0.04
8                       0.04
16                      0.04
32                      0.04
64                      0.04
128                     0.04
256                     0.04
512                     0.05
1024                    0.05
2048                    0.06
4096                    0.16
8192                    0.29
16384                   0.43
32768                   1.58
65536                   3.25
131072                  6.13
262144                 12.29
524288                 24.53
1048576                62.71
2097152               167.27
4194304               348.68
openmpi@4.0.5%intel@19.0.5.281 osu_cas_latency
# OSU MPI_Compare_and_swap latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.05
openmpi@4.0.5%intel@19.0.5.281 osu_get_acc_latency
# OSU MPI_Get_accumulate latency Test v5.6.3
# Window creation: MPI_Win_create
# Synchronization: MPI_Win_lock/unlock
# Size          Latency (us)
1                       3.12
2                       3.10
4                       3.15
8                       3.24
16                      3.18
32                      3.16
64                      3.26
128                     3.38
256                     3.57
512                     3.97
1024                    4.77
2048                    6.28
4096                   10.31
8192                   15.18
16384                  26.34
32768                  49.13
65536                  79.79
131072                147.82
262144                284.53
524288                571.24
1048576              1183.40
2097152              2128.43
4194304              4506.51
openmpi@4.0.5%intel@19.0.5.281 osu_get_latency
# OSU MPI_Get latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.05
2                       0.05
4                       0.05
8                       0.05
16                      0.05
32                      0.05
64                      0.05
128                     0.05
256                     0.05
512                     0.06
1024                    0.06
2048                    0.08
4096                    0.10
8192                    0.16
16384                   0.58
32768                   1.64
65536                   3.28
131072                  7.53
262144                 14.10
524288                 24.67
1048576                69.45
2097152               167.40
4194304               355.67
openmpi@4.0.5%intel@19.0.5.281 osu_put_bw
# OSU MPI_Put Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      30.73
2                      60.28
4                     135.00
8                     250.51
16                    507.54
32                   1042.27
64                   1784.66
128                  3372.25
256                  7859.01
512                 11540.95
1024                16721.27
2048                19905.84
4096                24056.88
8192                20009.44
16384               14936.97
32768               12207.42
65536               11517.28
131072               9791.22
262144               5285.74
524288               4566.58
1048576              4476.25
2097152              4568.04
4194304              4423.09
-- linux-debian9-cascadelake / gcc@10.2.0 -----------------------
hwloc@1.11.11
libiconv@1.16
libpciaccess@0.16
libxml2@2.9.10
numactl@2.0.14
openmpi@1.10.7
osu-micro-benchmarks@5.6.3
xz@5.2.5
zlib@1.2.11
openmpi@1.10.7%gcc@10.2.0 osu_hello
# OSU MPI Hello World Test v5.6.3
This is a test with 2 processes
openmpi@1.10.7%gcc@10.2.0 osu_init
# OSU MPI Init Test v5.6.3
nprocs: 2, min: 12 ms, max: 13 ms, avg: 12 ms
openmpi@1.10.7%gcc@10.2.0 osu_bibw
# OSU MPI Bi-Directional Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       8.62
2                      17.35
4                      33.68
8                      63.47
16                    123.70
32                    215.95
64                    364.00
128                   538.03
256                   927.84
512                  1615.87
1024                 2380.09
2048                 4040.41
4096                 4883.75
8192                 7706.10
16384                9865.96
32768               10566.57
65536               11109.04
131072              11187.95
262144              11222.83
524288              11407.97
1048576             11466.07
2097152             10712.57
4194304             10515.91
openmpi@1.10.7%gcc@10.2.0 osu_latency
# OSU MPI Latency Test v5.6.3
# Size          Latency (us)
0                       0.21
1                       0.23
2                       0.23
4                       0.24
8                       0.23
16                      0.27
32                      0.24
64                      0.26
128                     0.32
256                     0.39
512                     0.46
1024                    0.62
2048                    0.83
4096                    1.34
8192                    2.32
16384                   4.08
32768                   6.46
65536                   9.53
131072                 15.42
262144                 27.80
524288                 53.24
1048576               120.16
2097152               238.10
4194304               491.10
openmpi@1.10.7%gcc@10.2.0 osu_latency_mt
MPI_Init_thread must return MPI_THREAD_MULTIPLE!
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[11934,1],1]
  Exit code:    1
--------------------------------------------------------------------------
openmpi@1.10.7%gcc@10.2.0 osu_multi_lat
# OSU MPI Multi Latency Test v5.6.3
# Size          Latency (us)
0                       0.20
1                       0.23
2                       0.22
4                       0.22
8                       0.22
16                      0.23
32                      0.23
64                      0.25
128                     0.30
256                     0.37
512                     0.47
1024                    0.64
2048                    0.82
4096                    1.34
8192                    2.20
16384                   3.94
32768                   6.44
65536                   9.37
131072                 15.39
262144                 27.80
524288                 55.71
1048576               114.55
2097152               236.89
4194304               491.18
openmpi@1.10.7%gcc@10.2.0 osu_bw
# OSU MPI Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       7.30
2                      18.22
4                      33.77
8                      69.50
16                    139.21
32                    282.38
64                    317.22
128                   478.82
256                   827.97
512                  1332.68
1024                 1755.09
2048                 4759.90
4096                 5148.91
8192                 6421.39
16384                8140.34
32768                9292.18
65536                9800.00
131072               9487.17
262144               9891.07
524288               9646.52
1048576             10392.17
2097152             10150.83
4194304              9752.91
openmpi@1.10.7%gcc@10.2.0 osu_latency_mp
# OSU MPI Multi-process Latency Test v5.6.3
# Number of forked processes in sender: 2
# Number of forked processes in receiver: 2
# Size          Latency (us)
0                       0.21
1                       0.23
2                       0.23
4                       0.23
8                       0.24
16                      0.24
32                      0.24
64                      0.25
128                     0.29
256                     0.36
512                     0.47
1024                    0.65
2048                    0.83
4096                    1.39
8192                    2.33
16384                   4.10
32768                   6.51
65536                   9.53
131072                 15.70
262144                 28.04
524288                 53.89
1048576               115.92
2097152               243.96
4194304               511.74
openmpi@1.10.7%gcc@10.2.0 osu_mbw_mr
# OSU MPI Multiple Bandwidth / Message Rate Test v5.6.3
# [ pairs: 1 ] [ window size: 64 ]
# Size                  MB/s        Messages/s
1                      10.53       10525000.30
2                      21.92       10961343.28
4                      43.06       10764661.03
8                      86.24       10780218.02
16                    144.96        9060010.40
32                    326.21       10194214.92
64                    320.05        5000812.66
128                   543.21        4243834.89
256                   885.75        3459971.99
512                  1499.63        2928971.99
1024                 2538.11        2478620.92
2048                 4481.14        2188055.89
4096                 5563.61        1358303.77
8192                 6484.47         791561.17
16384               10206.59         622960.66
32768               10582.03         322937.86
65536               10821.38         165121.23
131072              10732.55          81882.85
262144              10645.39          40608.94
524288              10598.81          20215.63
1048576             10684.89          10189.91
2097152             10242.14           4883.83
4194304              9579.48           2283.93
openmpi@1.10.7%gcc@10.2.0 osu_allgather

# OSU MPI Allgather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       1.20
2                       1.19
4                       1.28
8                       1.25
16                      1.22
32                      1.29
64                      1.62
128                     1.78
256                     2.15
512                     2.74
1024                    4.04
2048                    6.73
4096                   12.17
8192                   21.42
16384                  27.94
32768                  55.32
65536                 125.06
131072                199.51
262144                436.18
524288               1127.69
1048576              3128.85
openmpi@1.10.7%gcc@10.2.0 osu_bcast

# OSU MPI Broadcast Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.76
2                       0.72
4                       0.74
8                       0.77
16                      0.79
32                      0.86
64                      0.97
128                     1.12
256                     1.18
512                     1.46
1024                    1.80
2048                    2.75
4096                    4.11
8192                    8.54
16384                  15.68
32768                  26.49
65536                  50.32
131072                 93.30
262144                173.16
524288                261.49
1048576               492.83
openmpi@1.10.7%gcc@10.2.0 osu_ialltoall

# OSU MPI Non-blocking All-to-All Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       8.06              4.46              3.34              0.00
2                       8.01              4.46              3.27              0.00
4                       7.86              4.43              3.51              2.25
8                       7.85              4.43              3.39              0.00
16                      8.00              4.44              3.36              0.00
32                      7.99              4.45              3.40              0.00
64                      8.09              4.43              3.41              0.00
128                     8.76              4.64              3.85              0.00
256                     9.04              4.77              4.06              0.00
512                    10.83              5.82              4.91              0.00
1024                   15.37              8.20              7.25              1.03
2048                   18.92             10.00              8.89              0.00
4096                   32.42             16.29             15.04              0.00
8192                   48.70             24.90             23.46              0.00
16384                  71.89             34.60             32.41              0.00
32768                 112.46             56.32             53.51              0.00
65536                 224.68            112.81            107.92              0.00
131072                509.72            259.09            248.60              0.00
262144               1488.16            764.70            735.83              1.68
524288               3635.77           1846.54           1768.10              0.00
1048576             11118.03           4438.32           3987.81              0.00
openmpi@1.10.7%gcc@10.2.0 osu_igatherv

# OSU MPI Non-blocking Gatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.17              1.29              0.69              0.00
2                       2.22              1.32              0.67              0.00
4                       2.18              1.30              0.68              0.00
8                       2.29              1.43              0.69              0.00
16                      2.16              1.29              0.70              0.00
32                      2.33              1.44              0.69              0.00
64                      2.34              1.43              0.70              0.00
128                     2.41              1.47              0.73              0.00
256                     2.40              1.42              0.75              0.00
512                     2.49              1.44              0.85              0.00
1024                    3.09              1.72              1.18              0.00
2048                    3.79              2.15              1.36              0.00
4096                   14.09              5.85              5.08              0.00
8192                   25.67             12.40             11.21              0.00
16384                  37.60             16.89             15.49              0.00
32768                  52.75             22.52             21.10              0.00
65536                  85.75             36.44             34.27              0.00
131072                193.59             82.58             78.93              0.00
262144                397.89            179.31            163.55              0.00
524288                946.63            407.04            387.00              0.00
1048576              2168.27            946.72            907.43              0.00
openmpi@1.10.7%gcc@10.2.0 osu_scatter

# OSU MPI Scatter Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.60
2                       0.61
4                       0.59
8                       0.63
16                      0.66
32                      0.67
64                      0.70
128                     0.87
256                     0.95
512                     1.32
1024                    1.49
2048                    2.11
4096                    5.29
8192                   11.35
16384                  18.48
32768                  30.01
65536                  44.40
131072                 86.26
262144                172.74
524288                570.62
1048576               676.85
openmpi@1.10.7%gcc@10.2.0 osu_allgatherv

# OSU MPI Allgatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.54
2                       3.52
4                       3.36
8                       3.60
16                      3.41
32                      3.55
64                      4.16
128                     3.88
256                     4.45
512                     5.09
1024                    7.94
2048                   12.25
4096                   13.27
8192                   20.55
16384                  28.14
32768                  47.96
65536                  92.12
131072                197.80
262144                424.03
524288               1021.36
1048576              3002.77
openmpi@1.10.7%gcc@10.2.0 osu_gather

# OSU MPI Gather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.27
2                       0.27
4                       0.26
8                       0.25
16                      0.25
32                      0.25
64                      0.26
128                     0.30
256                     0.35
512                     0.46
1024                    0.71
2048                    0.99
4096                    7.56
8192                   10.23
16384                  15.62
32768                  21.99
65536                  32.33
131072                 74.31
262144                132.72
524288                268.00
1048576               567.87
openmpi@1.10.7%gcc@10.2.0 osu_ialltoallv

# OSU MPI Non-blocking All-to-Allv Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       8.15              4.56              3.56              0.00
2                       7.73              4.32              3.31              0.00
4                       7.90              4.31              3.27              0.00
8                       7.97              4.61              3.70              9.27
16                      7.56              4.13              3.19              0.00
32                      7.92              4.43              3.33              0.00
64                      7.86              4.31              3.37              0.00
128                     8.42              4.74              4.06              9.20
256                     8.85              4.64              3.91              0.00
512                    10.81              5.89              4.98              1.31
1024                   17.83              7.85              7.09              0.00
2048                   18.62              9.85              8.71              0.00
4096                   31.85             16.31             14.97              0.00
8192                   49.66             25.07             23.27              0.00
16384                  75.89             37.60             35.64              0.00
32768                 125.18             60.42             57.63              0.00
65536                 236.85            119.65            114.60              0.00
131072                548.40            279.79            268.39              0.00
262144               1450.93            708.79            680.82              0.00
524288               3792.39           1934.13           1847.51              0.00
1048576             13274.68           6204.27           5706.93              0.00
openmpi@1.10.7%gcc@10.2.0 osu_ireduce

# OSU MPI Non-blocking Reduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       8.40              1.84              1.07              0.00
8                       4.13              1.84              1.13              0.00
16                      3.43              1.70              1.03              0.00
32                      3.10              1.70              1.07              0.00
64                      3.66              1.85              1.17              0.00
128                     4.91              2.46              1.39              0.00
256                     6.29              2.88              2.21              0.00
512                    12.36              3.65              2.94              0.00
1024                    7.02              3.60              2.56              0.00
2048                    5.60              3.07              2.40              0.00
4096                   12.83              5.15              4.21              0.00
8192                   21.45              8.55              7.44              0.00
16384                  31.67             11.87             10.96              0.00
32768                  51.03             18.74             17.43              0.00
65536                  80.54             28.73             27.02              0.00
131072                154.26             54.19             51.56              0.00
262144                289.42            100.00             95.63              0.00
524288                561.64            191.53            183.74              0.00
1048576              1170.28            386.27            371.89              0.00
openmpi@1.10.7%gcc@10.2.0 osu_scatterv

# OSU MPI Scatterv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.65
2                       0.61
4                       0.63
8                       0.64
16                      0.67
32                      0.70
64                      0.73
128                     0.98
256                     1.32
512                     1.64
1024                    1.98
2048                    2.90
4096                    5.49
8192                   10.79
16384                  18.80
32768                  30.73
65536                  45.42
131072                 81.69
262144                169.57
524288                338.67
1048576               663.43
openmpi@1.10.7%gcc@10.2.0 osu_allreduce

# OSU MPI Allreduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       1.46
8                       2.65
16                      1.49
32                      1.56
64                      1.54
128                     1.80
256                     1.99
512                     2.67
1024                    3.65
2048                    5.40
4096                   10.17
8192                   16.58
16384                  19.08
32768                  38.90
65536                  62.35
131072                100.15
262144                169.04
524288                335.75
1048576               823.87
openmpi@1.10.7%gcc@10.2.0 osu_gatherv

# OSU MPI Gatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.24
2                       0.25
4                       0.25
8                       0.26
16                      0.24
32                      0.26
64                      0.28
128                     0.29
256                     0.32
512                     0.47
1024                    0.66
2048                    0.91
4096                    7.28
8192                   12.50
16384                  19.48
32768                  29.70
65536                  44.03
131072                 76.70
262144                167.63
524288                291.87
1048576               633.48
openmpi@1.10.7%gcc@10.2.0 osu_ialltoallw

# OSU MPI Non-blocking All-to-Allw Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.81              6.00              4.90             22.30
2                       7.90              4.43              3.69              5.88
4                       7.76              4.08              3.18              0.00
8                       7.78              4.36              3.29              0.00
16                      7.50              4.01              3.21              0.00
32                      7.87              4.37              3.21              0.00
64                     10.53              4.41              3.39              0.00
128                     8.12              4.40              3.50              0.00
256                     8.67              4.41              4.02              0.00
512                    10.60              5.50              4.87              0.00
1024                   15.48              8.08              7.24              0.00
2048                   18.87              9.87              8.89              0.00
4096                   31.84             16.33             15.07              0.00
8192                   48.47             24.79             23.11              0.00
16384                  77.34             37.07             34.21              0.00
32768                 112.32             57.92             55.11              1.29
65536                 228.88            114.66            109.21              0.00
131072                516.86            252.88            242.01              0.00
262144               4942.67           2922.49           2808.73             28.07
524288               3841.66           1907.52           1825.46              0.00
1048576              8198.11           4251.87           4075.02              3.16
openmpi@1.10.7%gcc@10.2.0 osu_iscatter

# OSU MPI Non-blocking Scatter Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.02              2.14              1.47             40.12
2                       3.00              2.13              1.45             39.96
4                       3.00              2.13              1.43             39.12
8                       3.06              2.14              1.44             36.07
16                      3.06              2.15              1.53             40.34
32                      3.15              2.18              1.59             39.40
64                      3.36              2.38              1.70             41.77
128                     3.93              2.89              2.00             48.32
256                     4.25              2.96              2.18             41.22
512                     4.46              3.25              2.56             52.54
1024                    5.08              3.59              2.83             47.35
2048                    6.10              4.36              3.44             49.58
4096                   13.98              6.90              5.93              0.00
8192                   23.09             10.56              8.92              0.00
16384                  31.89             14.23             12.93              0.00
32768                  53.65             23.55             22.01              0.00
65536                  94.89             40.69             38.58              0.00
131072                246.79            119.61            114.12              0.00
262144                524.19            252.45            241.83              0.00
524288               1101.63            547.78            527.22              0.00
1048576              2421.56           1149.28           1105.54              0.00
openmpi@1.10.7%gcc@10.2.0 osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       1.89
2                       1.98
4                       2.11
8                       1.93
16                      1.93
32                      2.16
64                      2.28
128                     2.44
256                     2.79
512                     3.51
1024                    5.46
2048                    9.51
4096                   21.53
8192                   36.24
16384                  48.33
32768                  88.50
65536                 132.10
131072                271.17
262144                655.83
524288               1754.00
1048576              3467.02
openmpi@1.10.7%gcc@10.2.0 osu_iallgather

# OSU MPI Non-blocking Allgather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       8.02              4.43              3.51              0.00
2                       7.92              4.42              3.30              0.00
4                       7.76              4.31              3.24              0.00
8                       7.86              4.33              3.23              0.00
16                      7.98              4.44              3.33              0.00
32                      7.95              4.43              3.32              0.00
64                     10.38              5.66              4.51              0.00
128                     8.54              4.46              3.81              0.00
256                     8.83              4.59              4.03              0.00
512                    10.94              6.14              5.11              6.00
1024                   16.21              9.10              8.19             13.19
2048                   19.11             10.24              9.12              2.69
4096                   32.81             17.55             16.49              7.49
8192                   49.33             24.84             23.03              0.00
16384                  67.64             34.23             32.36              0.00
32768                 108.89             55.28             52.61              0.00
65536                 210.04            106.95            102.21              0.00
131072                430.04            219.66            210.89              0.24
262144                951.16            479.17            460.37              0.00
524288               2557.91           1281.48           1232.61              0.00
1048576              6454.66           3203.84           3064.69              0.00
openmpi@1.10.7%gcc@10.2.0 osu_ibarrier

# OSU MPI Non-blocking Barrier Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

       Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
              7.58              3.99              2.95              0.00
openmpi@1.10.7%gcc@10.2.0 osu_iscatterv

# OSU MPI Non-blocking Scatterv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.01              2.12              1.51             41.10
2                       3.10              2.13              1.51             36.06
4                       3.13              2.18              1.50             36.32
8                       3.07              2.14              1.53             38.81
16                      3.19              2.28              1.56             41.80
32                      3.23              2.29              1.63             41.94
64                      3.25              2.28              1.70             43.20
128                     3.28              2.29              1.75             42.98
256                     3.87              2.80              2.00             46.45
512                     4.60              3.35              2.55             51.24
1024                    4.81              3.24              2.60             39.65
2048                    5.86              4.04              3.30             44.99
4096                   14.92              7.45              6.50              0.00
8192                   23.64             11.00              9.80              0.00
16384                  33.83             15.01             13.75              0.00
32768                  54.43             23.04             21.53              0.00
65536                 116.66             41.65             38.68              0.00
131072                273.56            122.39            112.48              0.00
262144                655.41            292.45            258.77              0.00
524288               1661.55            684.17            606.91              0.00
1048576              4600.37           1664.01           1532.18              0.00
openmpi@1.10.7%gcc@10.2.0 osu_alltoallv

# OSU MPI All-to-Allv Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.16
2                       3.56
4                       6.26
8                      10.29
16                      7.52
32                      3.51
64                      7.49
128                    16.17
256                     6.75
512                     5.68
1024                    7.42
2048                    9.04
4096                   25.37
8192                   28.31
16384                 167.31
32768                  78.18
65536                 197.92
131072                378.64
262144                912.30
524288               1867.35
1048576              7289.94
openmpi@1.10.7%gcc@10.2.0 osu_iallgatherv

# OSU MPI Non-blocking Allgatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       7.86              4.47              3.43              1.08
2                       7.75              4.31              3.32              0.00
4                       7.87              4.44              3.40              0.00
8                       8.62              4.43              3.53              0.00
16                      8.12              4.49              3.35              0.00
32                      7.97              4.43              3.70              4.42
64                      8.09              4.44              3.64              0.00
128                     8.20              4.47              3.70              0.00
256                     8.69              4.49              3.87              0.00
512                    10.55              5.56              4.95              0.00
1024                   14.85              7.73              7.06              0.00
2048                   18.61              9.89              8.77              0.59
4096                   35.41             16.55             14.79              0.00
8192                   60.05             27.54             25.52              0.00
16384                  79.30             38.69             36.64              0.00
32768                 125.99             62.12             59.28              0.00
65536                 232.05            118.04            113.04              0.00
131072                460.72            224.89            215.08              0.00
262144               1008.52            495.73            476.99              0.00
524288               2981.62           1331.77           1266.26              0.00
1048576              6140.39           3219.59           3097.35              5.70
openmpi@1.10.7%gcc@10.2.0 osu_ibcast

# OSU MPI Non-Blocking Broadcast Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.54              2.12              1.42              0.00
2                       3.87              2.15              1.47              0.00
4                       3.47              2.13              1.36              2.33
8                       3.46              2.13              1.40              4.23
16                      3.99              2.63              1.38              0.58
32                      3.57              2.26              1.48             11.71
64                      3.65              2.26              1.65             15.51
128                     3.71              2.27              1.67             14.10
256                     3.89              2.40              1.79             16.41
512                     4.26              2.81              2.10             30.73
1024                    5.05              3.10              2.46             20.31
2048                    5.98              3.75              2.87             22.27
4096                    9.85              6.21              5.14             29.19
8192                   14.21              8.28              7.42             20.02
16384                  21.48             11.96             11.15             14.55
32768                  34.63             18.08             16.72              1.07
65536                  80.26             44.58             42.27             15.58
131072                151.99             78.40             74.07              0.65
262144                280.96            148.29            141.98              6.56
524288                487.37            182.92            174.80              0.00
1048576               735.24            389.58            374.52              7.70
openmpi@1.10.7%gcc@10.2.0 osu_reduce

# OSU MPI Reduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.55
8                       0.52
16                      0.57
32                      0.55
64                      0.57
128                     0.66
256                     0.76
512                     0.99
1024                    1.35
2048                    1.91
4096                    3.32
8192                    6.01
16384                  12.10
32768                  21.11
65536                  43.35
131072                 87.42
262144                169.84
524288                284.46
1048576               544.85
openmpi@1.10.7%gcc@10.2.0 osu_barrier

# OSU MPI Barrier Latency Test v5.6.3
# Avg Latency(us)
             1.06
openmpi@1.10.7%gcc@10.2.0 osu_iallreduce

# OSU MPI Non-blocking Allreduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       9.57              3.47              2.79              0.00
8                       6.46              3.51              2.77              0.00
16                      7.44              3.93              2.86              0.00
32                      6.61              3.78              2.93              3.41
64                      7.92              4.75              3.88             18.16
128                     7.90              4.05              3.38              0.00
256                     8.47              4.60              3.81              0.00
512                    10.03              5.37              4.52              0.00
1024                   14.24              6.91              5.92              0.00
2048                   16.13              8.40              7.48              0.00
4096                   28.58             13.42             12.22              0.00
8192                   46.86             23.54             21.91              0.00
16384                  76.97             36.81             34.79              0.00
32768                 119.17             58.06             55.26              0.00
65536                 164.46             79.71             75.20              0.00
131072                224.23            114.24            109.18              0.00
262144                362.90            194.19            186.47              9.53
524288                635.76            323.36            310.72              0.00
1048576              1525.24            732.42            701.14              0.00
openmpi@1.10.7%gcc@10.2.0 osu_igather

# OSU MPI Non-blocking Gather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.89              1.72              1.00              0.00
2                       2.76              1.52              1.02              0.00
4                       2.77              1.52              1.00              0.00
8                       2.62              1.44              0.97              0.00
16                      2.62              1.43              0.95              0.00
32                      2.65              1.46              0.92              0.00
64                      2.59              1.43              0.97              0.00
128                     2.79              1.57              1.10              0.00
256                     2.95              1.60              1.09              0.00
512                     3.23              1.72              1.34              0.00
1024                    5.12              2.86              2.05              0.00
2048                    5.86              3.09              2.56              0.00
4096                   23.80              9.87              8.67              0.00
8192                   37.20             17.63             16.13              0.00
16384                  51.74             23.45             21.96              0.00
32768                  81.86             35.14             32.93              0.00
65536                 162.24             66.00             62.82              0.00
131072                372.06            155.62            148.96              0.00
262144                748.65            341.26            327.00              0.00
524288               1116.40            646.20            614.35             23.46
1048576              1737.21            844.58            812.76              0.00
openmpi@1.10.7%gcc@10.2.0 osu_reduce_scatter

# OSU MPI Reduce_scatter Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.82
8                       0.88
16                      1.19
32                      1.73
64                      1.79
128                     1.87
256                     1.58
512                     1.80
1024                    2.19
2048                    2.83
4096                    4.27
8192                    6.34
16384                  10.53
32768                  17.40
65536                  29.90
131072                135.72
262144                304.77
524288                212.78
1048576               498.00
openmpi@1.10.7%gcc@10.2.0 osu_acc_latency
# OSU MPI_Accumulate latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.03
2                       0.03
4                       0.03
8                       0.04
16                      0.04
32                      0.06
64                      0.09
128                     0.13
256                     0.23
512                     0.39
1024                    0.73
2048                    1.40
4096                    2.75
8192                    5.46
16384                  10.90
32768                  21.73
65536                  43.10
131072                 86.76
262144                175.33
524288                345.79
1048576               689.91
2097152              1397.83
4194304              2792.04
openmpi@1.10.7%gcc@10.2.0 osu_fop_latency
# OSU MPI_Fetch_and_op latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.03
openmpi@1.10.7%gcc@10.2.0 osu_get_bw
# OSU MPI_Get Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      46.35
2                      90.19
4                     150.20
8                     377.38
16                    753.14
32                   1478.48
64                   2501.82
128                  5144.53
256                  9324.45
512                 14613.76
1024                24809.77
2048                22938.63
4096                24855.62
8192                27888.89
16384               15915.32
32768               12391.10
65536               11789.70
131072              11417.36
262144               6113.86
524288               4660.28
1048576              4542.39
2097152              4564.62
4194304              4718.28
openmpi@1.10.7%gcc@10.2.0 osu_put_bibw
# OSU MPI_Put Bi-directional Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_post/start/complete/wait
# Size      Bandwidth (MB/s)
1                      52.10
2                      99.69
4                     228.26
8                     424.33
16                    844.38
32                   1751.69
64                   3494.45
128                  6978.51
256                 12665.97
512                 20618.20
1024                36698.70
2048                42498.10
4096                43202.59
8192                39435.47
16384               28548.51
32768               23833.68
65536               19828.53
131072              10806.46
262144               8649.39
524288               8605.78
1048576              8276.51
2097152              8251.78
4194304              8828.55
openmpi@1.10.7%gcc@10.2.0 osu_put_latency
# OSU MPI_Put Latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.05
2                       0.05
4                       0.06
8                       0.05
16                      0.05
32                      0.05
64                      0.05
128                     0.06
256                     0.06
512                     0.06
1024                    0.07
2048                    0.08
4096                    0.12
8192                    0.28
16384                   0.61
32768                   1.31
65536                   2.96
131072                  6.01
262144                 11.32
524288                 23.56
1048576                65.42
2097152               166.75
4194304               362.76
openmpi@1.10.7%gcc@10.2.0 osu_cas_latency
# OSU MPI_Compare_and_swap latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.03
openmpi@1.10.7%gcc@10.2.0 osu_get_acc_latency
# OSU MPI_Get_accumulate latency Test v5.6.3
# Window creation: MPI_Win_create
# Synchronization: MPI_Win_lock/unlock
# Size          Latency (us)
1                       1.55
2                       1.56
4                       1.55
8                       1.55
16                      1.58
32                      1.56
64                      1.68
128                     1.79
256                     1.99
512                     2.33
1024                    2.86
2048                    3.92
4096                    6.63
8192                   13.38
16384                  21.03
32768                  34.96
65536                  62.53
131072                117.63
262144                223.17
524288                448.44
1048576               929.77
2097152              1928.17
4194304              4044.93
openmpi@1.10.7%gcc@10.2.0 osu_get_latency
# OSU MPI_Get latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.03
2                       0.03
4                       0.03
8                       0.03
16                      0.03
32                      0.03
64                      0.03
128                     0.03
256                     0.03
512                     0.03
1024                    0.04
2048                    0.05
4096                    0.07
8192                    0.10
16384                   0.27
32768                   1.28
65536                   2.37
131072                  6.11
262144                  9.71
524288                 21.11
1048576                63.46
2097152               169.29
4194304               343.08
openmpi@1.10.7%gcc@10.2.0 osu_put_bw
# OSU MPI_Put Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      38.95
2                      88.01
4                     180.88
8                     381.58
16                    784.73
32                   1549.88
64                   3135.55
128                  5606.54
256                 11224.15
512                 16490.53
1024                22749.79
2048                22039.02
4096                23014.85
8192                26090.11
16384               15911.68
32768               12264.16
65536               11736.24
131072              11325.51
262144               6679.27
524288               4892.67
1048576              4377.06
2097152              4532.93
4194304              4298.66
-- linux-debian9-cascadelake / gcc@10.2.0 -----------------------
hwloc@1.11.11
libiconv@1.16
libpciaccess@0.16
libxml2@2.9.10
numactl@2.0.14
openmpi@2.1.6
osu-micro-benchmarks@5.6.3
xz@5.2.5
zlib@1.2.11
openmpi@2.1.6%gcc@10.2.0 osu_hello
# OSU MPI Hello World Test v5.6.3
This is a test with 2 processes
openmpi@2.1.6%gcc@10.2.0 osu_init
# OSU MPI Init Test v5.6.3
nprocs: 2, min: 8 ms, max: 10 ms, avg: 9 ms
openmpi@2.1.6%gcc@10.2.0 osu_bibw
# OSU MPI Bi-Directional Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                      11.46
2                      21.80
4                      37.37
8                      91.13
16                    137.72
32                    334.20
64                    456.99
128                   599.82
256                  1042.64
512                  1907.79
1024                 3568.33
2048                 5771.38
4096                 5726.59
8192                 8054.57
16384               10202.37
32768               10175.23
65536               10489.81
131072              10430.19
262144              10568.95
524288               9982.40
1048576             10348.05
2097152              9804.10
4194304              8916.60
openmpi@2.1.6%gcc@10.2.0 osu_latency
# OSU MPI Latency Test v5.6.3
# Size          Latency (us)
0                       0.22
1                       0.25
2                       0.26
4                       0.26
8                       0.25
16                      0.27
32                      0.27
64                      0.29
128                     0.39
256                     0.41
512                     0.60
1024                    0.65
2048                    0.85
4096                    1.49
8192                    2.36
16384                   4.13
32768                   7.05
65536                   9.85
131072                 16.15
262144                 29.09
524288                 59.95
1048576               119.48
2097152               247.05
4194304               520.28
openmpi@2.1.6%gcc@10.2.0 osu_latency_mt
MPI_Init_thread must return MPI_THREAD_MULTIPLE!
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[9554,1],0]
  Exit code:    1
--------------------------------------------------------------------------
openmpi@2.1.6%gcc@10.2.0 osu_multi_lat
# OSU MPI Multi Latency Test v5.6.3
# Size          Latency (us)
0                       0.21
1                       0.26
2                       0.26
4                       0.26
8                       0.26
16                      0.27
32                      0.27
64                      0.29
128                     0.38
256                     0.39
512                     0.57
1024                    0.65
2048                    0.87
4096                    1.53
8192                    2.37
16384                   4.11
32768                   6.59
65536                   9.88
131072                 16.00
262144                 28.83
524288                 55.41
1048576               117.02
2097152               250.13
4194304               517.14
openmpi@2.1.6%gcc@10.2.0 osu_bw
# OSU MPI Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       7.63
2                      15.72
4                      31.15
8                      62.75
16                    116.35
32                    256.29
64                    319.98
128                   458.15
256                   821.80
512                  1414.12
1024                 2513.89
2048                 4038.77
4096                 4162.06
8192                 6252.75
16384                7685.95
32768                8780.11
65536                8819.24
131072               8879.97
262144               9496.66
524288               9611.43
1048576              9716.53
2097152              9004.78
4194304              8136.99
openmpi@2.1.6%gcc@10.2.0 osu_latency_mp
# OSU MPI Multi-process Latency Test v5.6.3
# Number of forked processes in sender: 2
# Number of forked processes in receiver: 2
# Size          Latency (us)
0                       0.21
1                       0.26
2                       0.25
4                       0.26
8                       0.26
16                      0.26
32                      0.27
64                      0.30
128                     0.38
256                     0.40
512                     0.56
1024                    0.63
2048                    0.85
4096                    1.51
8192                    2.48
16384                   4.25
32768                   6.78
65536                  10.19
131072                 16.86
262144                 30.54
524288                 56.42
1048576               118.95
2097152               251.89
4194304               486.60
openmpi@2.1.6%gcc@10.2.0 osu_mbw_mr
# OSU MPI Multiple Bandwidth / Message Rate Test v5.6.3
# [ pairs: 1 ] [ window size: 64 ]
# Size                  MB/s        Messages/s
1                      10.28       10279753.38
2                      19.02        9509404.07
4                      38.35        9587095.93
8                      79.09        9885650.69
16                    155.97        9747924.86
32                    316.87        9902246.88
64                    428.01        6687649.02
128                   566.09        4422607.91
256                   881.76        3444361.93
512                  1540.60        3008992.12
1024                 2766.08        2701250.50
2048                 4750.79        2319719.71
4096                 4831.11        1179471.09
8192                 6303.96         769526.19
16384                7729.57         471775.72
32768                8242.29         251534.83
65536               10046.41         153296.04
131072              10289.05          78499.21
262144              10336.48          39430.54
524288              10233.95          19519.71
1048576              9958.87           9497.52
2097152              9564.20           4560.57
4194304              8576.27           2044.74
openmpi@2.1.6%gcc@10.2.0 osu_allgather

# OSU MPI Allgather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       1.20
2                       1.20
4                       1.21
8                       1.27
16                      1.30
32                      2.87
64                      1.69
128                     2.45
256                     2.69
512                     3.37
1024                    4.83
2048                    7.70
4096                   11.76
8192                   21.33
16384                  26.57
32768                  64.26
65536                  95.32
131072                222.05
262144                483.78
524288               1441.20
1048576              3033.51
openmpi@2.1.6%gcc@10.2.0 osu_bcast

# OSU MPI Broadcast Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.81
2                       0.84
4                       0.83
8                       0.83
16                      0.89
32                      0.94
64                      0.96
128                     1.18
256                     1.18
512                     1.85
1024                    6.32
2048                   10.65
4096                    9.04
8192                   13.34
16384                  18.16
32768                  39.09
65536                  68.08
131072                127.77
262144                517.37
524288                442.83
1048576               633.55
openmpi@2.1.6%gcc@10.2.0 osu_ialltoall

# OSU MPI Non-blocking All-to-All Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      30.65              9.65              6.73              0.00
2                      19.96              4.25              3.44              0.00
4                      10.04              6.30              5.29             29.40
8                      13.94              4.42              3.57              0.00
16                     26.25              4.51              3.64              0.00
32                     30.74             13.87             12.65              0.00
64                     16.61             13.06             11.65             69.50
128                     8.12              4.44              3.48              0.00
256                    15.50              8.40              7.54              5.81
512                    58.20             26.34             22.67              0.00
1024                   34.11             13.80             11.95              0.00
2048                   56.01             14.21             10.09              0.00
4096                  137.14             55.92             47.85              0.00
8192                  158.04             56.78             44.53              0.00
16384                  78.28             39.20             37.02              0.00
32768                 265.88            135.30            127.87              0.00
65536                 387.79            161.51            143.85              0.00
131072                834.33            374.98            343.33              0.00
262144               1773.62            957.63            921.32             11.43
524288               3608.53           1809.47           1742.67              0.00
1048576              7593.84           3736.83           3593.44              0.00
openmpi@2.1.6%gcc@10.2.0 osu_igatherv

# OSU MPI Non-blocking Gatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.60              1.73              0.96              9.65
2                       2.40              1.49              0.69              0.00
4                       2.37              1.48              0.75              0.00
8                       2.43              1.51              0.69              0.00
16                      2.32              1.46              0.75              0.00
32                      2.37              1.47              0.74              0.00
64                      2.36              1.46              0.72              0.00
128                     2.38              1.46              0.74              0.00
256                     2.45              1.48              0.76              0.00
512                     6.07              3.17              2.43              0.00
1024                    7.85              3.90              3.16              0.00
2048                    9.56              4.59              3.80              0.00
4096                   16.11              6.97              6.12              0.00
8192                   27.23             12.32             11.19              0.00
16384                  38.09             18.57             17.05              0.00
32768                  52.31             22.98             21.47              0.00
65536                  87.80             36.74             34.91              0.00
131072                248.59            111.50             72.97              0.00
262144               3092.99            784.50            753.74              0.00
524288                747.85            363.76            349.88              0.00
1048576              1653.45            800.54            771.44              0.00
openmpi@2.1.6%gcc@10.2.0 osu_scatter

# OSU MPI Scatter Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.72
2                       0.70
4                       0.69
8                       0.78
16                      0.78
32                      0.82
64                      0.84
128                     1.11
256                     1.16
512                     3.04
1024                    3.76
2048                    4.59
4096                    7.57
8192                   11.43
16384                  19.65
32768                  30.01
65536                  44.16
131072                 84.14
262144                168.03
524288                328.44
1048576               668.22
openmpi@2.1.6%gcc@10.2.0 osu_allgatherv

# OSU MPI Allgatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.35
2                       3.49
4                       3.38
8                       3.37
16                      3.45
32                      3.43
64                      3.58
128                     4.01
256                     4.78
512                     5.69
1024                    6.96
2048                    9.65
4096                   13.96
8192                   22.58
16384                  33.68
32768                  52.60
65536                 110.96
131072                244.78
262144                506.91
524288               1107.44
1048576              3152.99
openmpi@2.1.6%gcc@10.2.0 osu_gather

# OSU MPI Gather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.24
2                       0.24
4                       0.23
8                       0.23
16                      0.25
32                      0.25
64                      0.26
128                     0.28
256                     0.30
512                     1.22
1024                    1.66
2048                    2.90
4096                   19.78
8192                   10.69
16384                  15.66
32768                  21.87
65536                  33.22
131072                 74.20
262144                126.65
524288                244.11
1048576               524.79
openmpi@2.1.6%gcc@10.2.0 osu_ialltoallv

# OSU MPI Non-blocking All-to-Allv Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.30              5.19              4.12              0.33
2                       9.91              4.83              3.90              0.00
4                       8.93              4.82              3.89              0.00
8                       8.75              4.77              3.80              0.00
16                      8.98              5.02              3.94              0.00
32                      9.10              4.96              3.84              0.00
64                      8.03              4.44              3.48              0.00
128                     8.24              4.45              3.73              0.00
256                     8.67              4.48              3.88              0.00
512                    13.71              6.90              6.19              0.00
1024                   16.29              8.26              7.31              0.00
2048                   20.00              9.91              8.97              0.00
4096                   37.42             19.18             17.86              0.00
8192                   50.53             26.12             24.46              0.25
16384                  70.43             36.65             34.67              2.58
32768                 114.21             56.80             54.03              0.00
65536                 259.26            134.68            128.95              3.39
131072                523.04            257.03            246.35              0.00
262144               1487.95            726.66            699.09              0.00
524288               3736.15           1932.99           1859.58              3.03
1048576              7601.53           3867.84           3722.08              0.00
openmpi@2.1.6%gcc@10.2.0 osu_ireduce

# OSU MPI Non-blocking Reduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       2.86              1.73              1.07              0.00
8                       2.86              1.71              1.06              0.00
16                      2.98              1.86              1.12              0.00
32                      3.04              1.85              1.16              0.00
64                      3.03              1.86              1.17              0.00
128                     3.15              1.84              1.24              0.00
256                     3.33              1.99              1.33              0.00
512                     6.15              3.06              2.12              0.00
1024                    7.79              3.39              2.57              0.00
2048                   11.90              4.98              4.10              0.00
4096                   12.74              5.54              4.55              0.00
8192                   18.69              7.46              6.48              0.00
16384                  32.87             12.34             11.33              0.00
32768                  52.22             18.96             17.78              0.00
65536                  82.46             29.06             27.35              0.00
131072                151.19             51.08             48.07              0.00
262144                288.39             99.15             94.59              0.00
524288                547.06            184.30            176.16              0.00
1048576              1162.46            389.19            373.18              0.00
openmpi@2.1.6%gcc@10.2.0 osu_scatterv

# OSU MPI Scatterv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.72
2                       0.96
4                       0.71
8                       0.74
16                      0.77
32                      0.79
64                      0.83
128                     0.94
256                     1.17
512                     3.47
1024                    3.78
2048                    4.59
4096                    7.44
8192                   11.06
16384                  18.68
32768                  31.37
65536                  46.01
131072                 86.41
262144                172.73
524288                354.59
1048576               666.45
openmpi@2.1.6%gcc@10.2.0 osu_allreduce

# OSU MPI Allreduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       1.52
8                       1.53
16                      1.54
32                      1.53
64                      1.62
128                     1.88
256                     2.04
512                     3.17
1024                    3.99
2048                    5.50
4096                   12.07
8192                   15.59
16384                  22.13
32768                  50.17
65536                  66.35
131072                 98.82
262144                168.29
524288                335.62
1048576               860.49
openmpi@2.1.6%gcc@10.2.0 osu_gatherv

# OSU MPI Gatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.28
2                       0.71
4                       0.40
8                       0.49
16                      0.29
32                      0.28
64                      0.29
128                     0.33
256                     0.34
512                     1.48
1024                    1.75
2048                    2.42
4096                   30.19
8192                   24.91
16384                  30.76
32768                  71.36
65536                  61.64
131072                 78.42
262144                251.88
524288                472.20
1048576              1068.96
openmpi@2.1.6%gcc@10.2.0 osu_ialltoallw

# OSU MPI Non-blocking All-to-Allw Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      10.75              5.79              4.86              0.00
2                      27.67              7.19              5.71              0.00
4                      28.00             17.59             13.44             22.61
8                      21.57             16.39             13.79             62.42
16                     12.25              5.09              4.39              0.00
32                     18.28              6.67              4.89              0.00
64                     27.19             13.95              9.57              0.00
128                    17.81             12.49              9.94             46.40
256                    54.85             14.58             13.29              0.00
512                    30.31             11.57              9.79              0.00
1024                  105.32             37.04             29.33              0.00
2048                   58.40             20.56             16.20              0.00
4096                  164.80             66.73             57.69              0.00
8192                  161.15             69.06             61.85              0.00
16384                  76.93             37.85             35.43              0.00
32768                 116.08             58.62             55.80              0.00
65536                 256.86            123.79            117.44              0.00
131072                756.30            277.26            265.43              0.00
262144               4304.68            899.66            719.03              0.00
524288               3810.34           1922.90           1850.17              0.00
1048576              7599.37           3851.72           3703.62              0.00
openmpi@2.1.6%gcc@10.2.0 osu_iscatter

# OSU MPI Non-blocking Scatter Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.48              2.46              1.85             45.15
2                       3.92              2.83              2.02             46.16
4                       3.97              2.92              2.00             47.48
8                       3.82              2.75              1.97             45.68
16                      3.91              2.83              2.01             45.90
32                      3.68              2.68              1.95             48.65
64                      3.27              2.28              1.70             41.48
128                     3.86              2.82              2.02             48.30
256                     4.28              3.01              2.15             40.97
512                     4.60              3.15              2.29             36.94
1024                    4.82              3.24              2.65             40.32
2048                    5.67              3.90              3.19             44.65
4096                   14.81              7.33              6.40              0.00
8192                   21.86             10.22              9.16              0.00
16384                  31.42             14.28             13.01              0.00
32768                  51.46             22.20             20.95              0.00
65536                  90.75             38.57             36.40              0.00
131072                247.03            119.78            114.68              0.00
262144                523.79            251.54            241.98              0.00
524288               1068.04            532.84            512.70              0.00
1048576              2162.91           1100.09           1058.30              0.00
openmpi@2.1.6%gcc@10.2.0 osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       4.05
2                       3.85
4                       4.28
8                       3.93
16                      3.90
32                      3.94
64                      5.79
128                     4.13
256                     4.16
512                     4.72
1024                    5.65
2048                    7.28
4096                   18.59
8192                   28.14
16384                  46.16
32768                  67.61
65536                 119.72
131072                257.36
262144                638.59
524288               1726.17
1048576              3526.70
openmpi@2.1.6%gcc@10.2.0 osu_iallgather

# OSU MPI Non-blocking Allgather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       7.50              4.16              3.25              0.00
2                      10.99              3.93              3.16              0.00
4                       6.92              3.64              3.11              0.00
8                       7.70              4.29              3.35              0.00
16                      9.01              5.48              3.74              5.59
32                      7.97              4.27              3.32              0.00
64                      7.79              4.36              3.23              0.00
128                     8.64              4.44              3.47              0.00
256                    11.12              7.01              5.66             27.26
512                    14.49              7.73              6.54              0.00
1024                   18.48              8.92              7.60              0.00
2048                   21.07             10.65              9.61              0.00
4096                   35.54             18.49             16.96              0.00
8192                   49.38             25.35             23.89              0.00
16384                  66.81             34.12             32.33              0.00
32768                 112.13             56.73             53.92              0.00
65536                 216.67            109.25            104.14              0.00
131072                444.63            222.20            213.28              0.00
262144                969.84            480.07            461.55              0.00
524288               2819.58           1409.99           1352.75              0.00
1048576              6288.33           3206.81           3085.62              0.13
openmpi@2.1.6%gcc@10.2.0 osu_ibarrier

# OSU MPI Non-blocking Barrier Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

       Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
              6.50              3.54              3.03              2.43
openmpi@2.1.6%gcc@10.2.0 osu_iscatterv

# OSU MPI Non-blocking Scatterv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.48              2.47              1.73             41.76
2                       3.48              2.46              1.62             37.67
4                       3.44              2.44              1.64             39.37
8                       3.44              2.47              1.69             42.71
16                      3.43              2.47              1.76             45.86
32                      3.35              2.38              1.71             43.12
64                      3.82              2.43              1.76             21.50
128                     3.91              2.72              1.89             37.03
256                     3.97              2.87              2.04             46.44
512                     4.53              3.12              2.30             38.68
1024                    4.63              3.14              2.58             42.39
2048                    5.81              4.01              3.29             45.20
4096                   15.52              7.14              6.14              0.00
8192                   24.80             11.88             10.75              0.00
16384                  31.11             13.93             12.61              0.00
32768                  53.56             22.42             20.91              0.00
65536                  91.46             38.54             36.46              0.00
131072                237.53            115.32            110.29              0.00
262144                507.14            246.44            236.78              0.00
524288               1077.66            545.32            524.99              0.00
1048576              2346.48           1150.63           1108.01              0.00
openmpi@2.1.6%gcc@10.2.0 osu_alltoallv

# OSU MPI All-to-Allv Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.01
2                       2.97
4                       2.90
8                       2.93
16                      3.09
32                      3.34
64                      3.36
128                     3.86
256                     4.13
512                     6.36
1024                    7.66
2048                    9.39
4096                   18.82
8192                   29.10
16384                  43.70
32768                  73.06
65536                 127.98
131072                261.26
262144                792.74
524288               1747.53
1048576              3749.26
openmpi@2.1.6%gcc@10.2.0 osu_iallgatherv

# OSU MPI Non-blocking Allgatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       8.53              4.67              3.78              0.00
2                       7.98              4.57              3.73              8.57
4                       8.18              4.49              3.28              0.00
8                       8.95              4.88              3.89              0.00
16                      8.74              4.96              4.11              7.92
32                      8.00              4.44              3.34              0.00
64                      8.02              4.43              3.49              0.00
128                     8.15              4.43              3.60              0.00
256                     8.93              4.46              3.98              0.00
512                    13.99              7.74              6.72              7.01
1024                   16.85              7.99              7.24              0.00
2048                   21.93             10.89              9.89              0.00
4096                   50.11             29.96             27.87             27.71
8192                   54.18             26.88             25.29              0.00
16384                  68.61             34.44             32.53              0.00
32768                 109.05             55.91             53.14              0.00
65536                 305.08            136.72            129.72              0.00
131072                540.72            258.33            243.40              0.00
262144               1088.05            549.11            528.49              0.00
524288               6852.17           1893.20           1588.97              0.00
1048576              9338.73           5859.23           5630.38             38.20
openmpi@2.1.6%gcc@10.2.0 osu_ibcast

# OSU MPI Non-Blocking Broadcast Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.47              2.11              1.44              5.21
2                       3.56              2.19              1.46              6.00
4                       3.59              2.19              1.41              0.95
8                       3.60              2.18              1.47              3.23
16                      3.61              2.30              1.53             14.32
32                      3.61              2.30              1.52             13.68
64                      3.67              2.30              1.61             14.89
128                     3.79              2.30              1.73             14.23
256                     4.00              2.42              1.79             11.60
512                     5.48              3.16              2.30              0.00
1024                    5.65              3.42              2.66             16.23
2048                    6.25              3.65              3.05             14.69
4096                   10.74              6.30              5.30             16.08
8192                   14.26              8.06              7.25             14.57
16384                  21.35             11.33             10.39              3.55
32768                  32.05             17.46             16.24             10.15
65536                  50.98             26.01             24.65              0.00
131072                 97.16             60.14             57.30             35.40
262144                184.12            110.42            105.70             30.27
524288                359.54            235.01            225.77             44.84
1048576               732.77            473.08            455.11             42.94
openmpi@2.1.6%gcc@10.2.0 osu_reduce

# OSU MPI Reduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.55
8                       0.56
16                      0.62
32                      0.56
64                      0.61
128                     0.75
256                     2.85
512                     1.53
1024                    1.70
2048                    2.96
4096                    5.94
8192                   11.90
16384                  12.35
32768                  20.83
65536                  44.50
131072                 94.72
262144                157.60
524288                319.43
1048576               574.11
openmpi@2.1.6%gcc@10.2.0 osu_barrier

# OSU MPI Barrier Latency Test v5.6.3
# Avg Latency(us)
             1.05
openmpi@2.1.6%gcc@10.2.0 osu_iallreduce

# OSU MPI Non-blocking Allreduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       6.32              3.63              2.75              2.07
8                       6.32              3.65              2.78              4.03
16                      6.31              3.70              2.81              6.83
32                      6.43              3.47              2.76              0.00
64                      7.64              3.94              3.12              0.00
128                     7.91              4.18              3.45              0.00
256                     8.35              4.46              3.64              0.00
512                    10.61              5.53              4.79              0.00
1024                   15.75              6.61              5.70              0.00
2048                   42.27             11.70              9.41              0.00
4096                   32.48             17.16             15.93              3.83
8192                   43.53             21.04             19.61              0.00
16384                  75.55             35.13             32.73              0.00
32768                 132.49             62.10             58.82              0.00
65536                 182.49             99.88             95.24             13.26
131072                235.03            115.61            110.80              0.00
262144                380.69            190.03            182.58              0.00
524288                650.02            332.13            319.12              0.38
1048576              1274.55            640.49            614.32              0.00
openmpi@2.1.6%gcc@10.2.0 osu_igather

# OSU MPI Non-blocking Gather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.32              1.46              0.68              0.00
2                       2.57              1.32              0.66              0.00
4                       2.20              1.33              0.67              0.00
8                       2.17              1.33              0.66              0.00
16                      2.17              1.32              0.68              0.00
32                      2.33              1.45              0.68              0.00
64                      2.35              1.46              0.72              0.00
128                     2.39              1.46              0.71              0.00
256                     2.42              1.46              0.75              0.00
512                     6.20              3.24              2.50              0.00
1024                    7.58              4.02              3.40              0.00
2048                    9.33              4.44              3.63              0.00
4096                   15.79              6.78              5.83              0.00
8192                   26.00             12.10             11.00              0.00
16384                  37.86             16.90             15.53              0.00
32768                  50.42             22.18             20.68              0.00
65536                  86.98             37.95             35.60              0.00
131072                195.93             92.45             88.51              0.00
262144                373.61            164.50            157.87              0.00
524288                790.38            370.57            356.53              0.00
1048576              1710.67            842.16            811.01              0.00
openmpi@2.1.6%gcc@10.2.0 osu_reduce_scatter

# OSU MPI Reduce_scatter Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.80
8                       0.88
16                      1.08
32                      1.59
64                      1.52
128                     1.60
256                     1.86
512                     2.08
1024                    2.69
2048                    3.24
4096                    4.37
8192                    6.84
16384                  10.87
32768                  18.61
65536                  28.87
131072                135.27
262144                305.60
524288                220.70
1048576               486.83
openmpi@2.1.6%gcc@10.2.0 osu_acc_latency
# OSU MPI_Accumulate latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.05
2                       0.03
4                       0.03
8                       0.04
16                      0.04
32                      0.06
64                      0.09
128                     0.13
256                     0.21
512                     0.38
1024                    1.01
2048                    1.39
4096                    2.75
8192                    5.47
16384                  10.87
32768                  21.83
65536                  43.22
131072                 85.71
262144                171.48
524288                348.17
1048576               746.51
2097152              1386.52
4194304              2785.81
openmpi@2.1.6%gcc@10.2.0 osu_fop_latency
# OSU MPI_Fetch_and_op latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.03
openmpi@2.1.6%gcc@10.2.0 osu_get_bw
# OSU MPI_Get Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      46.75
2                      80.73
4                     181.87
8                     368.03
16                    612.61
32                   1530.32
64                   3073.14
128                  5582.09
256                 11004.25
512                 17054.06
1024                25564.45
2048                25224.88
4096                28248.82
8192                29383.20
16384               15725.93
32768               12038.50
65536               11143.25
131072               9610.11
262144               5178.92
524288               4684.03
1048576              4579.67
2097152              4541.13
4194304              4648.36
openmpi@2.1.6%gcc@10.2.0 osu_put_bibw
# OSU MPI_Put Bi-directional Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_post/start/complete/wait
# Size      Bandwidth (MB/s)
1                      46.46
2                      98.39
4                     206.92
8                     421.55
16                    756.45
32                   1524.96
64                   3343.78
128                  6528.17
256                 13450.90
512                 21293.27
1024                31489.83
2048                43959.63
4096                50256.61
8192                42087.96
16384               29302.05
32768               24489.98
65536               21788.43
131072              13381.50
262144               9432.10
524288               8617.91
1048576              8536.77
2097152              8782.18
4194304              8493.13
openmpi@2.1.6%gcc@10.2.0 osu_put_latency
# OSU MPI_Put Latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.03
2                       0.03
4                       0.03
8                       0.03
16                      0.03
32                      0.03
64                      0.03
128                     0.03
256                     0.03
512                     0.03
1024                    0.04
2048                    0.05
4096                    0.07
8192                    0.21
16384                   0.42
32768                   1.48
65536                   3.01
131072                  5.85
262144                 11.37
524288                 23.34
1048576                65.92
2097152               171.35
4194304               356.53
openmpi@2.1.6%gcc@10.2.0 osu_cas_latency
# OSU MPI_Compare_and_swap latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.05
openmpi@2.1.6%gcc@10.2.0 osu_get_acc_latency
# OSU MPI_Get_accumulate latency Test v5.6.3
# Window creation: MPI_Win_create
# Synchronization: MPI_Win_lock/unlock
# Size          Latency (us)
1                       3.04
2                       2.92
4                       2.81
8                       3.05
16                      2.77
32                      2.78
64                      2.84
128                     3.01
256                     3.18
512                     3.54
1024                    4.15
2048                    5.27
4096                    8.00
8192                   11.80
16384                  20.48
32768                  35.71
65536                  63.81
131072                118.17
262144                222.33
524288                460.60
1048576               938.85
2097152              1873.17
4194304              3987.59
openmpi@2.1.6%gcc@10.2.0 osu_get_latency
# OSU MPI_Get latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.03
2                       0.03
4                       0.03
8                       0.03
16                      0.03
32                      0.03
64                      0.03
128                     0.03
256                     0.03
512                     0.03
1024                    0.04
2048                    0.05
4096                    0.07
8192                    0.11
16384                   0.32
32768                   1.33
65536                   2.65
131072                  6.03
262144                 11.36
524288                 23.11
1048576                61.84
2097152               167.70
4194304               355.02
openmpi@2.1.6%gcc@10.2.0 osu_put_bw
# OSU MPI_Put Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      32.75
2                      86.05
4                     169.91
8                     354.63
16                    708.43
32                   1400.66
64                   2747.15
128                  5083.02
256                 10221.73
512                 13630.96
1024                20845.25
2048                26307.37
4096                27608.96
8192                25141.52
16384               14986.51
32768               12450.49
65536               11713.93
131072              11124.89
262144               4874.58
524288               4249.05
1048576              4654.14
2097152              4581.97
4194304              4625.34
-- linux-debian9-cascadelake / gcc@10.2.0 -----------------------
hwloc@1.11.11
libiconv@1.16
libpciaccess@0.16
libxml2@2.9.10
numactl@2.0.14
openmpi@3.1.6
osu-micro-benchmarks@5.6.3
xz@5.2.5
zlib@1.2.11
openmpi@3.1.6%gcc@10.2.0 osu_hello
# OSU MPI Hello World Test v5.6.3
This is a test with 2 processes
openmpi@3.1.6%gcc@10.2.0 osu_init
# OSU MPI Init Test v5.6.3
nprocs: 2, min: 7 ms, max: 8 ms, avg: 7 ms
openmpi@3.1.6%gcc@10.2.0 osu_bibw
# OSU MPI Bi-Directional Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                      10.71
2                      22.26
4                      43.02
8                      73.44
16                    149.41
32                    313.12
64                    469.75
128                   644.61
256                  1069.37
512                  2045.60
1024                 3298.10
2048                 6033.42
4096                 5810.37
8192                 7814.66
16384               10226.64
32768               11071.40
65536               10180.34
131072              10363.88
262144              10665.06
524288              10494.68
1048576             10270.73
2097152              9613.02
4194304              8574.05
openmpi@3.1.6%gcc@10.2.0 osu_latency
# OSU MPI Latency Test v5.6.3
# Size          Latency (us)
0                       0.24
1                       0.29
2                       0.28
4                       0.28
8                       0.27
16                      0.29
32                      0.29
64                      0.31
128                     0.39
256                     0.42
512                     0.59
1024                    0.68
2048                    0.86
4096                    1.54
8192                    2.39
16384                   4.14
32768                   6.48
65536                   9.62
131072                 15.93
262144                 28.19
524288                 54.01
1048576               114.54
2097152               243.51
4194304               499.80
openmpi@3.1.6%gcc@10.2.0 osu_latency_mt
# Number of Sender threads: 1
# Number of Receiver threads: 2
# OSU MPI Multi-threaded Latency Test v5.6.3
# Size          Latency (us)
0                       0.34
1                       0.40
2                       0.40
4                       0.41
8                       0.40
16                      0.41
32                      0.40
64                      0.41
128                     0.49
256                     0.52
512                     0.86
1024                    0.90
2048                    1.03
4096                    5.05
8192                    3.35
16384                   4.47
32768                   6.81
65536                   9.87
131072                 16.57
262144                 54.03
524288                 87.24
1048576               235.66
2097152               447.99
4194304              1039.95
openmpi@3.1.6%gcc@10.2.0 osu_multi_lat
# OSU MPI Multi Latency Test v5.6.3
# Size          Latency (us)
0                       0.22
1                       0.26
2                       0.26
4                       0.27
8                       0.26
16                      0.28
32                      0.27
64                      0.29
128                     0.37
256                     0.40
512                     0.53
1024                    0.66
2048                    0.85
4096                    1.49
8192                    2.31
16384                   4.02
32768                   6.44
65536                   9.49
131072                 15.55
262144                 28.20
524288                 54.42
1048576               117.06
2097152               239.44
4194304               507.83
openmpi@3.1.6%gcc@10.2.0 osu_bw
# OSU MPI Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       8.48
2                      14.31
4                      31.82
8                      82.19
16                    146.33
32                    324.54
64                    453.11
128                   571.73
256                   913.46
512                  1592.26
1024                 2732.62
2048                 4917.61
4096                 5655.95
8192                 7374.81
16384                9384.18
32768                9880.13
65536               10298.12
131072              10021.31
262144               9101.63
524288               9882.02
1048576              9271.19
2097152              8978.31
4194304              8215.02
openmpi@3.1.6%gcc@10.2.0 osu_latency_mp
# OSU MPI Multi-process Latency Test v5.6.3
# Number of forked processes in sender: 2
# Number of forked processes in receiver: 2
# Size          Latency (us)
0                       0.22
1                       0.26
2                       0.28
4                       0.27
8                       0.29
16                      0.28
32                      0.28
64                      0.30
128                     0.38
256                     0.41
512                     0.56
1024                    0.66
2048                    0.84
4096                    1.50
8192                    2.36
16384                   4.11
32768                   6.55
65536                   9.62
131072                 15.70
262144                 27.97
524288                 53.63
1048576               114.09
2097152               240.30
4194304               501.71
openmpi@3.1.6%gcc@10.2.0 osu_mbw_mr
# OSU MPI Multiple Bandwidth / Message Rate Test v5.6.3
# [ pairs: 1 ] [ window size: 64 ]
# Size                  MB/s        Messages/s
1                       8.67        8669408.69
2                      21.51       10753645.32
4                      38.63        9656893.86
8                      83.53       10441018.72
16                    161.02       10063557.88
32                    316.71        9897085.15
64                    435.24        6800696.77
128                   552.41        4315681.22
256                   900.92        3519207.94
512                  1621.66        3167300.81
1024                 2894.41        2826567.99
2048                 4958.76        2421269.02
4096                 5623.32        1372879.71
8192                 7415.10         905163.09
16384                9143.50         558074.69
32768               10065.25         307167.08
65536               10307.43         157278.86
131072               9920.15          75684.76
262144               9920.67          37844.35
524288               9595.17          18301.34
1048576              9653.32           9206.12
2097152              8948.77           4267.10
4194304              7929.76           1890.60
openmpi@3.1.6%gcc@10.2.0 osu_allgather

# OSU MPI Allgather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       1.31
2                       1.47
4                       1.36
8                       1.40
16                      1.50
32                      1.57
64                      1.66
128                     2.13
256                     2.64
512                     3.18
1024                    4.97
2048                    7.63
4096                   11.96
8192                   19.11
16384                  28.41
32768                  48.65
65536                  99.83
131072                212.66
262144                446.89
524288               1072.84
1048576              3142.74
openmpi@3.1.6%gcc@10.2.0 osu_bcast

# OSU MPI Broadcast Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.77
2                       0.78
4                       1.30
8                       0.83
16                      0.88
32                      0.90
64                      0.98
128                     1.14
256                     1.22
512                     1.66
1024                    2.05
2048                    4.07
4096                    6.62
8192                   11.27
16384                  19.55
32768                  37.01
65536                  70.61
131072                141.57
262144                262.11
524288                248.10
1048576               521.57
openmpi@3.1.6%gcc@10.2.0 osu_ialltoall

# OSU MPI Non-blocking All-to-All Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       8.12              4.35              3.37              0.00
2                       7.87              4.43              3.29              0.00
4                       7.89              4.37              3.32              0.00
8                       7.92              4.43              3.31              0.00
16                      7.62              4.19              3.28              0.00
32                      7.90              4.43              3.30              0.00
64                      7.98              4.44              3.40              0.00
128                     8.09              4.44              3.67              0.46
256                     8.56              4.58              3.97              0.00
512                    13.03              6.77              6.09              0.00
1024                   14.73              7.73              6.55              0.00
2048                   28.41             14.41             13.20              0.00
4096                   34.86             18.02             16.77              0.00
8192                   50.01             24.38             22.80              0.00
16384                  68.51             35.13             32.97              0.00
32768                 110.34             56.26             53.47              0.00
65536                 218.04            111.70            106.85              0.47
131072                507.57            241.86            232.01              0.00
262144               1434.19            704.61            676.17              0.00
524288               5826.79           1794.99           1636.06              0.00
1048576              7247.20           3644.16           3504.11              0.00
openmpi@3.1.6%gcc@10.2.0 osu_igatherv

# OSU MPI Non-blocking Gatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.12              1.29              0.65              0.00
2                       2.16              1.30              0.63              0.00
4                       2.15              1.32              0.64              0.00
8                       2.16              1.33              0.65              0.00
16                      2.12              1.29              0.64              0.00
32                      2.19              1.30              0.65              0.00
64                      2.82              1.31              0.66              0.00
128                     2.43              1.46              0.69              0.00
256                     2.33              1.42              0.71              0.00
512                     8.90              3.23              2.20              0.00
1024                    7.28              3.41              2.65              0.00
2048                    9.40              4.44              3.28              0.00
4096                   15.37              6.50              5.45              0.00
8192                   24.70             11.71             10.49              0.00
16384                  34.73             15.45             14.25              0.00
32768                  56.18             23.73             22.22              0.00
65536                 105.58             43.57             39.42              0.00
131072                197.49             85.87             81.26              0.00
262144                334.13            161.06            153.53              0.00
524288                702.53            327.25            308.92              0.00
1048576              1543.25            743.05            714.88              0.00
openmpi@3.1.6%gcc@10.2.0 osu_scatter

# OSU MPI Scatter Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.75
2                       0.76
4                       0.76
8                       1.50
16                      0.82
32                      0.86
64                      0.87
128                     1.04
256                     1.15
512                     2.99
1024                    3.76
2048                    4.59
4096                    7.24
8192                   10.88
16384                  18.50
32768                  30.54
65536                  45.77
131072                 81.83
262144                171.46
524288                335.36
1048576               711.35
openmpi@3.1.6%gcc@10.2.0 osu_allgatherv

# OSU MPI Allgatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.60
2                       3.75
4                       3.58
8                       3.56
16                      3.76
32                      3.91
64                      3.67
128                     4.20
256                     4.98
512                     6.05
1024                    7.72
2048                   10.52
4096                   15.31
8192                   23.09
16384                  31.29
32768                  58.85
65536                  95.74
131072                194.71
262144                434.84
524288               1162.47
1048576              2980.85
openmpi@3.1.6%gcc@10.2.0 osu_gather

# OSU MPI Gather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.27
2                       0.26
4                       0.27
8                       0.27
16                      0.27
32                      0.30
64                      0.31
128                     0.33
256                     0.35
512                     1.51
1024                    1.91
2048                    2.57
4096                    8.57
8192                   13.30
16384                  16.77
32768                  29.83
65536                  34.64
131072                 77.36
262144                131.21
524288                248.62
1048576               519.49
openmpi@3.1.6%gcc@10.2.0 osu_ialltoallv

# OSU MPI Non-blocking All-to-Allv Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.15              4.98              4.18              0.48
2                       8.85              5.13              4.13              9.70
4                       8.06              4.48              3.52              0.00
8                       7.96              4.43              3.37              0.00
16                      7.98              4.44              3.45              0.00
32                      8.22              4.45              3.49              0.00
64                      8.05              4.45              3.63              0.83
128                     8.12              4.44              3.44              0.00
256                     8.48              4.44              3.84              0.00
512                    13.91              7.05              6.27              0.00
1024                   14.76              7.73              6.75              0.00
2048                   18.25              9.35              8.40              0.00
4096                   34.58             17.54             15.85              0.00
8192                   49.86             23.89             22.32              0.00
16384                  81.14             37.82             35.21              0.00
32768                 126.24             63.61             60.57              0.00
65536                 255.45            126.89            121.73              0.00
131072                619.41            291.76            276.65              0.00
262144               1498.35            758.80            729.33              0.00
524288               3440.50           1799.47           1732.27              5.27
1048576              8212.76           4007.56           3834.15              0.00
openmpi@3.1.6%gcc@10.2.0 osu_ireduce

# OSU MPI Non-blocking Reduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       3.55              2.16              1.46              4.54
8                       9.73              2.15              1.39              0.00
16                      9.46              2.16              1.43              0.00
32                     11.99              7.14              6.10             20.57
64                      7.14              1.97              1.35              0.00
128                     7.16              5.68              4.87             69.61
256                     4.03              2.32              1.77              3.37
512                     6.31              3.39              2.46              0.00
1024                    8.98              5.16              2.89              0.00
2048                   21.47              8.42              7.44              0.00
4096                   41.12             15.85             12.55              0.00
8192                   58.09             22.06             18.94              0.00
16384                  98.22             35.94             28.29              0.00
32768                 170.54             40.25             32.80              0.00
65536                  87.19             31.00             29.11              0.00
131072                631.98            225.79            163.57              0.00
262144                734.04            172.08            134.14              0.00
524288                898.08            346.39            310.10              0.00
1048576              1260.22            426.84            410.56              0.00
openmpi@3.1.6%gcc@10.2.0 osu_scatterv

# OSU MPI Scatterv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.81
2                       0.81
4                       0.87
8                       0.84
16                      0.91
32                      0.88
64                      0.89
128                     1.15
256                     1.19
512                     3.23
1024                    3.87
2048                    4.60
4096                    7.37
8192                   11.06
16384                  18.89
32768                  30.76
65536                  45.62
131072                 84.68
262144                176.30
524288                343.70
1048576               848.51
openmpi@3.1.6%gcc@10.2.0 osu_allreduce

# OSU MPI Allreduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       1.28
8                       1.34
16                      1.42
32                      6.82
64                     41.53
128                    14.47
256                    31.93
512                     9.04
1024                   15.59
2048                    8.52
4096                   28.71
8192                   26.18
16384                  20.05
32768                  42.29
65536                  65.43
131072                 98.89
262144                167.21
524288                326.14
1048576               796.10
openmpi@3.1.6%gcc@10.2.0 osu_gatherv

# OSU MPI Gatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.33
2                       0.30
4                       0.29
8                       0.28
16                      0.29
32                      0.31
64                      0.32
128                     0.35
256                     0.38
512                     1.40
1024                    1.83
2048                    2.67
4096                    8.40
8192                   12.61
16384                  19.34
32768                  29.48
65536                  53.33
131072                 78.97
262144                141.19
524288                276.30
1048576               584.22
openmpi@3.1.6%gcc@10.2.0 osu_ialltoallw

# OSU MPI Non-blocking All-to-Allw Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      10.15              5.60              4.62              1.72
2                      10.26              5.32              4.48              0.00
4                      11.14              6.29              5.23              7.17
8                       9.61              5.37              4.62              8.36
16                      8.93              5.03              4.33              9.94
32                      8.63              4.77              3.98              2.75
64                      8.37              4.43              3.51              0.00
128                     8.32              4.41              3.78              0.00
256                    10.18              5.38              4.71              0.00
512                    13.91              7.07              6.27              0.00
1024                   16.54              8.35              7.45              0.00
2048                   21.79             11.55             10.59              3.31
4096                   38.49             18.08             16.39              0.00
8192                   49.18             25.25             23.70              0.00
16384                  69.52             34.50             32.60              0.00
32768                 110.54             55.65             52.82              0.00
65536                 222.07            108.58            103.83              0.00
131072                662.63            262.01            251.46              0.00
262144               1609.37            848.34            816.75              6.82
524288               3728.41           2000.01           1925.84             10.25
1048576              7590.04           3966.85           3808.00              4.85
openmpi@3.1.6%gcc@10.2.0 osu_iscatter

# OSU MPI Non-blocking Scatter Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.02              2.13              1.34             33.71
2                       2.99              2.14              1.38             38.40
4                       2.98              2.13              1.37             37.60
8                       2.98              2.14              1.38             38.98
16                      3.05              2.12              1.42             33.87
32                      3.02              2.13              1.47             39.43
64                      3.07              2.15              1.57             41.00
128                     3.22              2.27              1.73             44.94
256                     3.82              2.79              1.91             46.07
512                     4.30              2.95              2.09             35.55
1024                    4.58              3.13              2.40             39.73
2048                    5.76              3.99              3.04             41.88
4096                   13.54              6.77              5.94              0.00
8192                   20.39              9.56              8.57              0.00
16384                  30.10             13.74             12.59              0.00
32768                  53.09             22.67             20.95              0.00
65536                  89.13             37.26             35.00              0.00
131072                224.41            108.70            104.11              0.00
262144                493.60            238.95            228.90              0.00
524288               1024.09            509.18            481.57              0.00
1048576              2096.12           1034.27            993.28              0.00
openmpi@3.1.6%gcc@10.2.0 osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       4.09
2                       3.98
4                       3.99
8                       3.71
16                      3.35
32                      3.41
64                      3.39
128                     3.86
256                     3.70
512                     4.62
1024                    5.44
2048                    7.34
4096                   19.39
8192                   29.66
16384                  45.53
32768                  69.95
65536                 129.17
131072                258.70
262144                619.97
524288               1697.91
1048576              3889.55
openmpi@3.1.6%gcc@10.2.0 osu_iallgather

# OSU MPI Non-blocking Allgather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       8.98              4.77              3.79              0.00
2                       8.21              4.58              3.74              3.19
4                       8.34              4.42              3.31              0.00
8                       7.74              4.30              3.25              0.00
16                      7.63              4.17              3.19              0.00
32                      7.86              4.30              3.27              0.00
64                      8.36              4.60              3.62              0.00
128                     8.46              4.61              3.88              0.85
256                     8.48              4.53              3.72              0.00
512                    12.83              6.65              5.77              0.00
1024                   26.69              9.55              7.48              0.00
2048                   17.71              9.18              8.29              0.00
4096                   33.42             17.37             15.86              0.00
8192                   82.63             47.99             45.48             23.85
16384                  72.75             34.80             32.51              0.00
32768                 110.13             54.92             52.27              0.00
65536                 238.06            109.06            100.51              0.00
131072                447.17            238.44            228.79              8.77
262144               1002.91            529.16            508.46              6.83
524288               2454.07           1199.99           1154.24              0.00
1048576              5574.25           2763.56           2661.05              0.00
openmpi@3.1.6%gcc@10.2.0 osu_ibarrier

# OSU MPI Non-blocking Barrier Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

       Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
              7.27              4.32              3.32             11.39
openmpi@3.1.6%gcc@10.2.0 osu_iscatterv

# OSU MPI Non-blocking Scatterv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.54              2.45              1.85             41.33
2                       3.48              2.47              1.78             43.10
4                       3.77              2.83              2.16             56.38
8                       2.97              2.13              1.43             40.61
16                      2.99              2.11              1.48             40.74
32                      3.00              2.12              1.49             40.61
64                      3.04              2.13              1.64             44.59
128                     3.96              2.69              1.91             33.54
256                     3.53              2.53              1.89             47.18
512                     4.16              2.81              2.00             32.57
1024                    4.50              3.07              2.29             37.90
2048                    5.35              3.65              2.89             41.27
4096                   14.55              6.92              6.01              0.00
8192                   23.51             12.31             11.17              0.00
16384                  30.92             13.48             12.30              0.00
32768                  50.33             21.34             20.19              0.00
65536                  86.66             36.54             34.56              0.00
131072                231.38            112.59            107.68              0.00
262144                503.85            244.01            222.28              0.00
524288               1035.76            525.33            505.28              0.00
1048576              2691.18           1153.55           1045.43              0.00
openmpi@3.1.6%gcc@10.2.0 osu_alltoallv

# OSU MPI All-to-Allv Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.37
2                       2.94
4                       3.20
8                       5.11
16                     11.33
32                      3.27
64                      7.99
128                     4.00
256                    12.23
512                    13.27
1024                    8.09
2048                    8.98
4096                   43.23
8192                   64.32
16384                 147.00
32768                  72.95
65536                 135.25
131072                512.00
262144               2083.12
524288               5241.90
1048576              3994.67
openmpi@3.1.6%gcc@10.2.0 osu_iallgatherv

# OSU MPI Non-blocking Allgatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.72              5.64              4.87             16.18
2                       9.34              5.28              4.43              8.38
4                       9.28              5.20              4.29              4.97
8                       9.43              5.29              4.33              4.53
16                      9.94              5.23              4.38              0.00
32                      8.44              4.65              3.69              0.00
64                      8.57              4.76              4.14              8.12
128                     8.44              4.73              3.90              4.73
256                     8.72              4.50              3.77              0.00
512                    15.39              7.40              6.54              0.00
1024                   16.22              8.43              7.45              0.00
2048                   18.30              9.62              8.52              0.00
4096                   33.22             16.85             15.66              0.00
8192                   48.25             24.98             23.52              1.03
16384                  64.97             33.13             31.24              0.00
32768                 119.10             53.97             51.47              0.00
65536                 259.99            111.55            103.27              0.00
131072                470.93            237.30            224.49              0.00
262144                935.04            454.72            437.22              0.00
524288               2415.32           1178.93           1131.47              0.00
1048576              5713.87           2876.62           2767.26              0.00
openmpi@3.1.6%gcc@10.2.0 osu_ibcast

# OSU MPI Non-Blocking Broadcast Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.47              2.23              1.45             14.64
2                       3.46              2.21              1.43             12.51
4                       3.43              2.21              1.38             11.50
8                       3.47              2.25              1.43             14.75
16                      3.48              2.21              1.40              8.53
32                      3.54              2.23              1.45              9.59
64                      3.56              2.21              1.49              9.07
128                     3.80              2.21              1.71              7.19
256                     3.71              2.21              1.73             13.01
512                     5.28              3.18              2.22              5.94
1024                    5.59              3.33              2.52             10.64
2048                    6.23              3.64              2.97             12.65
4096                   10.28              6.38              5.60             30.33
8192                   14.01              7.78              7.10             12.25
16384                  20.64             11.56             10.53             13.77
32768                  34.26             17.71             16.29              0.00
65536                  53.62             26.80             25.19              0.00
131072                 99.17             60.28             57.42             32.28
262144                182.75            110.56            105.82             31.78
524288                364.79            230.21            220.99             39.10
1048576               744.72            483.42            464.19             43.71
openmpi@3.1.6%gcc@10.2.0 osu_reduce

# OSU MPI Reduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.55
8                       0.55
16                      0.62
32                      0.61
64                      0.64
128                     0.78
256                     0.82
512                     1.44
1024                    1.80
2048                    2.96
4096                    5.76
8192                   11.76
16384                  13.34
32768                  21.35
65536                  43.65
131072                 98.48
262144                165.65
524288                311.25
1048576               556.71
openmpi@3.1.6%gcc@10.2.0 osu_barrier

# OSU MPI Barrier Latency Test v5.6.3
# Avg Latency(us)
             1.10
openmpi@3.1.6%gcc@10.2.0 osu_iallreduce

# OSU MPI Non-blocking Allreduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       6.61              3.48              2.71              0.00
8                       6.10              3.39              2.63              0.00
16                      6.97              3.81              3.10              0.00
32                      7.60              3.86              3.16              0.00
64                      7.54              4.01              3.42              0.00
128                     8.37              4.59              3.69              0.00
256                     8.78              4.88              3.97              1.70
512                    11.68              5.99              5.18              0.00
1024                   12.66              6.96              6.12              6.85
2048                   14.93              7.72              7.05              0.00
4096                   27.75             13.56             12.42              0.00
8192                   42.13             20.29             19.08              0.00
16384                  72.35             33.86             32.03              0.00
32768                 127.22             65.69             62.15              0.99
65536                 157.31             72.85             67.14              0.00
131072                208.78            104.30             99.67              0.00
262144                340.61            169.02            162.00              0.00
524288                640.11            326.05            313.15              0.00
1048576              1318.72            655.88            631.11              0.00
openmpi@3.1.6%gcc@10.2.0 osu_igather

# OSU MPI Non-blocking Gather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.60              1.63              0.74              0.00
2                       2.26              1.35              0.70              0.00
4                       2.24              1.34              0.70              0.00
8                       2.39              1.48              0.73              0.00
16                      2.22              1.33              0.69              0.00
32                      2.25              1.33              0.69              0.00
64                      2.23              1.32              0.70              0.00
128                     2.38              1.46              0.73              0.00
256                     2.42              1.47              0.78              0.00
512                     6.70              2.96              2.32              0.00
1024                    9.73              5.70              4.76             15.37
2048                    9.32              4.07              3.36              0.00
4096                   15.72              6.60              5.60              0.00
8192                   25.37             11.70             10.54              0.00
16384                  36.84             16.41             15.09              0.00
32768                  49.44             21.59             20.03              0.00
65536                  81.02             33.10             31.12              0.00
131072                160.11             66.30             63.27              0.00
262144                359.29            166.62            156.44              0.00
524288                677.45            322.22            309.52              0.00
1048576              1455.97            697.43            670.34              0.00
openmpi@3.1.6%gcc@10.2.0 osu_reduce_scatter

# OSU MPI Reduce_scatter Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.89
8                       0.94
16                      1.15
32                      1.65
64                      1.68
128                     2.00
256                     2.02
512                     2.08
1024                    2.75
2048                    3.80
4096                    5.00
8192                    7.59
16384                  10.87
32768                  17.79
65536                  28.71
131072                147.23
262144                336.29
524288                247.52
1048576               497.85
openmpi@3.1.6%gcc@10.2.0 osu_acc_latency
# OSU MPI_Accumulate latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.03
2                       0.03
4                       0.04
8                       0.04
16                      0.04
32                      0.07
64                      0.09
128                     0.13
256                     0.21
512                     0.40
1024                    0.72
2048                    1.43
4096                    2.74
8192                    5.39
16384                  13.35
32768                  21.43
65536                  43.03
131072                 85.67
262144                171.10
524288                347.49
1048576               693.47
2097152              1378.35
4194304              2822.11
openmpi@3.1.6%gcc@10.2.0 osu_fop_latency
# OSU MPI_Fetch_and_op latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.03
openmpi@3.1.6%gcc@10.2.0 osu_get_bw
# OSU MPI_Get Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      45.81
2                      90.58
4                     173.53
8                     347.07
16                    737.96
32                   1399.36
64                   3012.27
128                  5492.97
256                  7865.99
512                 17275.23
1024                26859.68
2048                23381.37
4096                26835.12
8192                28267.82
16384               15025.35
32768               11920.58
65536               11296.91
131072               9919.57
262144               5230.26
524288               4494.62
1048576              4556.52
2097152              4551.25
4194304              4429.65
openmpi@3.1.6%gcc@10.2.0 osu_put_bibw
# OSU MPI_Put Bi-directional Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_post/start/complete/wait
# Size      Bandwidth (MB/s)
1                      53.04
2                     115.38
4                     207.42
8                     445.67
16                    856.08
32                   1667.49
64                   3581.84
128                  7027.20
256                 13069.98
512                 21083.92
1024                32407.79
2048                40915.76
4096                51353.36
8192                40620.69
16384               30331.74
32768               24694.95
65536               21952.93
131072              12457.99
262144               8612.87
524288               8453.22
1048576              8613.34
2097152              8746.99
4194304              8890.20
openmpi@3.1.6%gcc@10.2.0 osu_put_latency
# OSU MPI_Put Latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.03
2                       0.03
4                       0.03
8                       0.03
16                      0.03
32                      0.03
64                      0.03
128                     0.03
256                     0.03
512                     0.03
1024                    0.04
2048                    0.05
4096                    0.07
8192                    0.21
16384                   0.44
32768                   1.50
65536                   3.04
131072                  6.17
262144                  9.64
524288                 22.16
1048576                61.07
2097152               168.86
4194304               367.77
openmpi@3.1.6%gcc@10.2.0 osu_cas_latency
# OSU MPI_Compare_and_swap latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.03
openmpi@3.1.6%gcc@10.2.0 osu_get_acc_latency
# OSU MPI_Get_accumulate latency Test v5.6.3
# Window creation: MPI_Win_create
# Synchronization: MPI_Win_lock/unlock
# Size          Latency (us)
1                       2.77
2                       2.80
4                       2.78
8                       2.82
16                      2.80
32                      2.79
64                      2.86
128                     3.00
256                     3.19
512                     3.48
1024                    4.10
2048                    5.15
4096                    8.27
8192                   11.63
16384                  20.35
32768                  35.59
65536                  63.22
131072                117.09
262144                220.38
524288                450.34
1048576               913.66
2097152              1874.84
4194304              3952.80
openmpi@3.1.6%gcc@10.2.0 osu_get_latency
# OSU MPI_Get latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.03
2                       0.03
4                       0.03
8                       0.03
16                      0.03
32                      0.03
64                      0.03
128                     0.03
256                     0.03
512                     0.03
1024                    0.04
2048                    0.05
4096                    0.08
8192                    0.09
16384                   0.26
32768                   1.08
65536                   2.28
131072                  6.30
262144                 10.24
524288                 22.45
1048576                62.13
2097152               171.28
4194304               346.94
openmpi@3.1.6%gcc@10.2.0 osu_put_bw
# OSU MPI_Put Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      43.98
2                      90.24
4                     177.13
8                     359.72
16                    730.13
32                   1507.43
64                   2995.90
128                  5341.99
256                 10860.12
512                 15061.32
1024                21256.32
2048                25676.08
4096                28501.63
8192                20509.37
16384               15540.18
32768               12336.74
65536               11117.25
131072              10491.17
262144               5841.03
524288               4569.30
1048576              4333.03
2097152              4495.22
4194304              4567.21
-- linux-debian9-cascadelake / gcc@10.2.0 -----------------------
hwloc@2.2.0
libiconv@1.16
libpciaccess@0.16
libxml2@2.9.10
numactl@2.0.14
openmpi@4.0.5
osu-micro-benchmarks@5.6.3
xz@5.2.5
zlib@1.2.11
openmpi@4.0.5%gcc@10.2.0 osu_hello
# OSU MPI Hello World Test v5.6.3
This is a test with 2 processes
openmpi@4.0.5%gcc@10.2.0 osu_init
# OSU MPI Init Test v5.6.3
nprocs: 2, min: 17 ms, max: 18 ms, avg: 17 ms
openmpi@4.0.5%gcc@10.2.0 osu_bibw
# OSU MPI Bi-Directional Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       9.89
2                      20.41
4                      41.31
8                      84.96
16                    118.93
32                    322.12
64                    360.51
128                   472.54
256                   845.30
512                  1575.65
1024                 3259.49
2048                 5762.85
4096                 5522.38
8192                 7734.41
16384               10140.16
32768               10569.12
65536               10508.05
131072              10898.85
262144              11212.97
524288              11456.90
1048576             11467.16
2097152             11621.04
4194304             12800.15
openmpi@4.0.5%gcc@10.2.0 osu_latency
# OSU MPI Latency Test v5.6.3
# Size          Latency (us)
0                       0.23
1                       0.27
2                       0.26
4                       0.26
8                       0.27
16                      0.28
32                      0.28
64                      0.29
128                     0.36
256                     0.40
512                     0.57
1024                    0.68
2048                    0.83
4096                    1.85
8192                    2.97
16384                   4.58
32768                   7.26
65536                  13.23
131072                 25.64
262144                 50.80
524288                101.71
1048576               218.91
2097152               447.10
4194304               961.68
openmpi@4.0.5%gcc@10.2.0 osu_latency_mt
# Number of Sender threads: 1
# Number of Receiver threads: 2
# OSU MPI Multi-threaded Latency Test v5.6.3
# Size          Latency (us)
0                       0.37
1                       0.40
2                       0.39
4                       0.40
8                       0.39
16                      0.42
32                      0.42
64                      0.44
128                     0.52
256                     0.54
512                     0.87
1024                    0.93
2048                    3.46
4096                    3.22
8192                    3.92
16384                   4.88
32768                   7.53
65536                  13.60
131072                 42.32
262144                104.60
524288                226.28
1048576               460.92
2097152               884.42
4194304              2257.64
openmpi@4.0.5%gcc@10.2.0 osu_multi_lat
# OSU MPI Multi Latency Test v5.6.3
# Size          Latency (us)
0                       0.22
1                       0.26
2                       0.26
4                       0.26
8                       0.26
16                      0.28
32                      0.28
64                      0.29
128                     0.37
256                     0.41
512                     0.59
1024                    0.67
2048                    0.82
4096                    1.84
8192                    2.87
16384                   6.08
32768                   7.41
65536                  13.61
131072                 26.57
262144                 52.64
524288                105.97
1048576               222.40
2097152               465.10
4194304              1012.47
openmpi@4.0.5%gcc@10.2.0 osu_bw
# OSU MPI Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                      10.28
2                      19.34
4                      42.26
8                      83.46
16                    164.13
32                    324.85
64                    485.20
128                   555.57
256                   780.44
512                  1966.57
1024                 3317.96
2048                 5698.26
4096                 5435.04
8192                 7341.09
16384                9374.84
32768                9604.81
65536               10343.82
131072              10547.65
262144              10633.56
524288              10821.02
1048576             11198.21
2097152             11255.25
4194304             11245.19
openmpi@4.0.5%gcc@10.2.0 osu_latency_mp
# OSU MPI Multi-process Latency Test v5.6.3
# Number of forked processes in sender: 2
# Number of forked processes in receiver: 2
# Size          Latency (us)
0                       0.34
1                       0.26
2                       0.27
4                       0.27
8                       0.26
16                      0.28
32                      0.27
64                      0.29
128                     0.35
256                     0.39
512                     0.56
1024                    0.62
2048                    0.82
4096                    1.83
8192                    2.84
16384                   4.51
32768                   7.05
65536                  13.04
131072                 25.40
262144                 49.67
524288                101.44
1048576               213.60
2097152               449.50
4194304              1015.41
openmpi@4.0.5%gcc@10.2.0 osu_mbw_mr
# OSU MPI Multiple Bandwidth / Message Rate Test v5.6.3
# [ pairs: 1 ] [ window size: 64 ]
# Size                  MB/s        Messages/s
1                      10.37       10365194.97
2                      20.89       10445449.46
4                      42.01       10502615.81
8                      82.53       10316124.75
16                    134.42        8401133.10
32                    326.40       10199918.40
64                    482.15        7533639.47
128                   541.72        4232202.43
256                   844.77        3299868.88
512                  1934.74        3778796.35
1024                 3358.12        3279409.26
2048                 5631.86        2749933.51
4096                 5249.47        1281609.19
8192                 7565.36         923506.13
16384                9630.88         587822.14
32768                9627.43         293805.86
65536               10377.71         158351.34
131072              10713.18          81735.07
262144              10763.38          41059.05
524288              11098.16          21168.05
1048576             11137.85          10621.89
2097152             11165.99           5324.36
4194304             11589.32           2763.11
openmpi@4.0.5%gcc@10.2.0 osu_allgather

# OSU MPI Allgather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       1.22
2                       1.24
4                       1.33
8                       1.37
16                      1.41
32                      1.50
64                      1.65
128                     2.28
256                     2.74
512                     3.21
1024                    5.18
2048                    8.39
4096                   15.13
8192                   23.96
16384                  37.69
32768                  60.45
65536                 105.05
131072                207.32
262144                454.50
524288               1079.18
1048576              3381.93
openmpi@4.0.5%gcc@10.2.0 osu_bcast

# OSU MPI Broadcast Latency Test v5.6.3
# Size       Avg Latency(us)
1                      10.88
2                       0.77
4                       0.81
8                       0.83
16                      0.92
32                      0.89
64                      0.98
128                     1.19
256                     1.20
512                     1.68
1024                    2.01
2048                    6.14
4096                   13.43
8192                   19.31
16384                  19.47
32768                  55.10
65536                 108.51
131072                156.39
262144                276.45
524288                487.15
1048576               906.07
openmpi@4.0.5%gcc@10.2.0 osu_ialltoall

# OSU MPI Non-blocking All-to-All Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       8.30              4.50              3.42              0.00
2                       8.24              4.49              3.67              0.00
4                      25.49             18.46             13.60             48.33
8                      23.05             14.98             10.66             24.30
16                      8.97              5.07              4.23              7.95
32                     10.61              4.91              3.88              0.00
64                     27.07             18.55             15.58             45.26
128                    23.25             14.27             11.68             23.16
256                     8.55              4.46              3.90              0.00
512                    43.50             10.83              9.86              0.00
1024                   38.27             19.17             17.50              0.00
2048                   71.23             24.41             18.78              0.00
4096                   67.93             23.86             20.61              0.00
8192                  105.64             51.58             46.25              0.00
16384                  96.12             38.05             35.83              0.00
32768                 226.13            106.48             95.77              0.00
65536                 380.82            170.28            151.67              0.00
131072                456.47            231.07            220.91              0.00
262144               1153.73            586.89            564.01              0.00
524288               3277.99           1581.95           1522.76              0.00
1048576              9185.23           3890.75           3611.96              0.00
openmpi@4.0.5%gcc@10.2.0 osu_igatherv

# OSU MPI Non-blocking Gatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.23              1.33              0.66              0.00
2                       2.28              1.36              0.65              0.00
4                       2.18              1.33              0.66              0.00
8                       2.19              1.32              0.64              0.00
16                      2.21              1.33              0.66              0.00
32                      2.42              1.45              0.69              0.00
64                      2.51              1.49              0.75              0.00
128                     2.49              1.49              0.78              0.00
256                     2.49              1.49              0.77              0.00
512                     5.71              2.81              2.12              0.00
1024                    7.37              3.54              2.71              0.00
2048                    9.00              4.24              3.31              0.00
4096                   16.90              7.42              6.61              0.00
8192                   24.49             10.17              9.27              0.00
16384                  33.33             13.94             12.70              0.00
32768                  61.92             31.80             29.79              0.00
65536                 135.08             60.33             57.12              0.00
131072                240.65            115.20            109.98              0.00
262144                400.93            200.72            192.61              0.00
524288                821.35            402.81            387.20              0.00
1048576              1868.55            879.32            845.82              0.00
openmpi@4.0.5%gcc@10.2.0 osu_scatter

# OSU MPI Scatter Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.70
2                       0.87
4                       0.70
8                       0.69
16                      0.85
32                      0.87
64                      0.93
128                     1.18
256                     1.23
512                     3.27
1024                    3.73
2048                    4.71
4096                    8.76
8192                   13.62
16384                  20.87
32768                  34.03
65536                  61.70
131072                128.97
262144                272.85
524288                583.17
1048576              1186.23
openmpi@4.0.5%gcc@10.2.0 osu_allgatherv

# OSU MPI Allgatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.76
2                       3.21
4                       3.24
8                       3.25
16                      3.36
32                      3.48
64                      3.62
128                     4.52
256                     4.49
512                     5.10
1024                    7.20
2048                   10.59
4096                   16.67
8192                   23.66
16384                  32.38
32768                  60.61
65536                 108.58
131072                213.62
262144                453.97
524288               1062.41
1048576              3206.85
openmpi@4.0.5%gcc@10.2.0 osu_gather

# OSU MPI Gather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.26
2                       0.27
4                       0.26
8                       0.26
16                      0.27
32                      0.28
64                      0.30
128                     0.32
256                     0.35
512                     1.42
1024                    1.88
2048                    2.86
4096                    8.35
8192                   11.79
16384                  15.98
32768                  22.12
65536                  41.15
131072                 89.97
262144                156.22
524288                306.21
1048576               654.95
openmpi@4.0.5%gcc@10.2.0 osu_ialltoallv

# OSU MPI Non-blocking All-to-Allv Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       8.24              4.59              3.68              0.95
2                       8.17              4.46              3.45              0.00
4                       8.24              4.52              3.49              0.00
8                       8.44              4.66              3.72              0.00
16                     30.31              4.96              3.95              0.00
32                      8.78              4.66              3.70              0.00
64                      8.97              4.68              3.73              0.00
128                     8.40              4.45              3.69              0.00
256                     8.74              4.48              4.03              0.00
512                    13.39              6.70              6.00              0.00
1024                   16.20              8.17              7.27              0.00
2048                   19.35              9.97              8.83              0.00
4096                   42.78             21.57             20.01              0.00
8192                   53.73             27.47             25.86              0.00
16384                  74.41             37.45             35.50              0.00
32768                 141.15             69.91             66.37              0.00
65536                 220.19            107.57            102.85              0.00
131072                516.92            228.66            219.46              0.00
262144               1192.13            575.24            553.27              0.00
524288               3419.14           1659.57           1590.75              0.00
1048576              7370.70           3757.83           3610.54              0.00
openmpi@4.0.5%gcc@10.2.0 osu_ireduce

# OSU MPI Non-blocking Reduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       3.09              1.84              1.04              0.00
8                       2.84              1.70              1.00              0.00
16                      2.84              1.71              1.03              0.00
32                      2.89              1.71              1.04              0.00
64                      3.12              1.82              1.14              0.00
128                     3.13              1.84              1.23              0.00
256                     3.34              1.98              1.30              0.00
512                     6.08              3.05              2.02              0.00
1024                    6.26              3.09              2.42              0.00
2048                    7.41              3.38              2.73              0.00
4096                   14.47              5.95              5.26              0.00
8192                   21.27              8.53              7.53              0.00
16384                  33.06             12.51             11.32              0.00
32768                  55.10             20.12             18.76              0.00
65536                  99.88             36.25             34.29              0.00
131072                190.84             68.70             65.49              0.00
262144                439.96            152.08            141.73              0.00
524288                828.87            304.98            292.90              0.00
1048576              1624.38            576.41            554.37              0.00
openmpi@4.0.5%gcc@10.2.0 osu_scatterv

# OSU MPI Scatterv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.72
2                       0.75
4                       0.72
8                       0.82
16                      0.87
32                      0.95
64                      1.02
128                     1.13
256                     1.26
512                     3.23
1024                    3.67
2048                    4.51
4096                    8.79
8192                   13.14
16384                  20.30
32768                  32.95
65536                  60.65
131072                129.12
262144                267.51
524288                558.60
1048576              1145.98
openmpi@4.0.5%gcc@10.2.0 osu_allreduce

# OSU MPI Allreduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       1.44
8                       1.33
16                      1.40
32                      1.40
64                      1.47
128                     2.03
256                     1.96
512                     3.08
1024                    3.67
2048                    4.78
4096                    9.95
8192                   17.82
16384                  24.85
32768                  55.51
65536                 195.47
131072                212.41
262144                530.38
524288                497.26
1048576              1622.75
openmpi@4.0.5%gcc@10.2.0 osu_gatherv

# OSU MPI Gatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.25
2                       0.26
4                       0.27
8                       0.26
16                      0.29
32                      0.29
64                      0.32
128                     0.34
256                     0.36
512                     2.89
1024                    1.94
2048                    2.51
4096                   14.23
8192                   33.35
16384                  18.84
32768                  34.41
65536                  62.91
131072                120.43
262144                536.44
524288                744.09
1048576              1875.46
openmpi@4.0.5%gcc@10.2.0 osu_ialltoallw

# OSU MPI Non-blocking All-to-Allw Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.42              5.07              4.16              0.00
2                      14.57              5.48              4.53              0.00
4                       9.32              4.43              3.39              0.00
8                       9.01              5.13              3.82              0.00
16                     11.14              5.49              4.52              0.00
32                      9.92              5.55              4.41              0.96
64                     13.61              6.03              5.03              0.00
128                     9.65              4.42              3.77              0.00
256                   161.06            146.03            138.85             89.17
512                    14.67              7.50              6.53              0.00
1024                   20.05             10.87              9.44              2.79
2048                   23.36             11.83             10.64              0.00
4096                   45.09             22.59             21.02              0.00
8192                   50.71             25.86             24.31              0.00
16384                  69.34             35.11             33.06              0.00
32768                 119.00             61.18             58.16              0.59
65536                 222.23            108.96            104.37              0.00
131072                462.69            229.00            219.45              0.00
262144               1182.68            574.61            550.97              0.00
524288               3394.05           1681.42           1614.47              0.00
1048576              7347.68           3784.38           3637.75              2.05
openmpi@4.0.5%gcc@10.2.0 osu_iscatter

# OSU MPI Non-blocking Scatter Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.18              2.20              1.40             29.89
2                       3.09              2.17              1.49             38.01
4                       3.12              2.19              1.45             35.92
8                       3.08              2.16              1.46             36.58
16                      3.10              2.18              1.61             42.69
32                      3.18              2.22              1.58             39.40
64                      3.44              2.43              1.75             42.18
128                     4.02              2.92              2.06             46.57
256                     4.51              3.26              2.51             50.38
512                     5.70              3.98              2.93             41.42
1024                    5.74              4.05              3.15             46.14
2048                    6.33              4.48              3.76             50.86
4096                   13.13              6.06              5.24              0.00
8192                   20.06              8.74              7.63              0.00
16384                  28.86             12.29             11.27              0.00
32768                  55.80             27.48             25.83              0.00
65536                  99.19             49.40             46.91              0.00
131072                614.78            198.16            190.01              0.00
262144                469.50            236.78            227.11              0.00
524288               1042.14            539.52            511.67              1.77
1048576              1996.32            990.83            954.38              0.00
openmpi@4.0.5%gcc@10.2.0 osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.72
2                       3.42
4                       3.34
8                       3.53
16                      3.53
32                      3.51
64                      3.44
128                     3.91
256                     3.85
512                     4.81
1024                    6.01
2048                    6.96
4096                   26.77
8192                   34.19
16384                  48.61
32768                  80.26
65536                 131.80
131072                249.33
262144                574.35
524288               1885.06
1048576              3904.07
openmpi@4.0.5%gcc@10.2.0 osu_iallgather

# OSU MPI Non-blocking Allgather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       8.62              4.79              4.15              7.65
2                       8.47              4.73              3.87              3.45
4                       7.74              4.30              3.23              0.00
8                       7.80              4.43              3.47              2.83
16                      7.91              4.44              3.26              0.00
32                      8.49              4.99              4.09             14.44
64                      8.05              4.43              3.38              0.00
128                     8.18              4.46              3.65              0.00
256                     8.88              4.74              4.09              0.00
512                    12.73              6.65              5.74              0.00
1024                   15.78              8.28              7.37              0.00
2048                   18.54              9.92              8.89              3.02
4096                   48.96             22.78             21.10              0.00
8192                   53.55             28.65             26.80              7.07
16384                  66.01             32.96             31.07              0.00
32768                 117.23             60.99             58.08              3.17
65536                 213.07            109.03            104.35              0.30
131072                428.04            231.48            222.19             11.54
262144                825.62            409.05            392.77              0.00
524288               2317.95           1121.12           1078.53              0.00
1048576              5630.99           2959.13           2845.89              6.11
openmpi@4.0.5%gcc@10.2.0 osu_ibarrier

# OSU MPI Non-blocking Barrier Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

       Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
              7.21              3.76              3.03              0.00
openmpi@4.0.5%gcc@10.2.0 osu_iscatterv

# OSU MPI Non-blocking Scatterv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.41              2.43              1.49             34.28
2                       3.31              2.31              1.56             35.77
4                       3.29              2.30              1.52             34.40
8                       3.27              2.29              1.55             36.79
16                      3.30              2.30              1.59             37.67
32                      4.06              3.05              1.61             37.14
64                      3.31              2.29              1.63             37.31
128                     3.63              2.58              1.90             44.92
256                     4.21              3.13              1.97             44.84
512                     4.17              2.74              2.03             29.51
1024                    4.65              3.15              2.28             34.15
2048                    5.60              3.85              2.98             41.11
4096                   14.61              6.55              5.76              0.00
8192                   20.31              8.84              7.78              0.00
16384                  29.24             12.76             11.58              0.00
32768                  55.95             27.74             26.05              0.00
65536                 102.89             50.38             47.91              0.00
131072                219.79            110.21            105.43              0.00
262144                472.65            241.59            231.85              0.34
524288                948.40            470.37            448.87              0.00
1048576              1873.14            943.41            907.49              0.00
openmpi@4.0.5%gcc@10.2.0 osu_alltoallv

# OSU MPI All-to-Allv Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.20
2                       2.87
4                       2.92
8                       2.99
16                      3.18
32                      3.12
64                      3.36
128                     3.98
256                     4.42
512                     7.60
1024                   12.15
2048                    8.59
4096                   27.04
8192                   42.99
16384                  51.07
32768                  87.07
65536                 145.29
131072                261.75
262144                648.33
524288               1802.77
1048576              3712.81
openmpi@4.0.5%gcc@10.2.0 osu_iallgatherv

# OSU MPI Non-blocking Allgatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       7.95              4.47              3.42              0.00
2                       9.72              4.44              3.31              0.00
4                       7.99              4.45              3.33              0.00
8                       8.48              4.84              3.70              1.81
16                      8.19              4.47              3.37              0.00
32                      8.31              4.44              3.42              0.00
64                      8.25              4.58              3.84              4.56
128                     8.37              4.55              3.64              0.00
256                     8.61              4.60              4.04              0.72
512                    12.92              6.90              6.12              1.62
1024                   15.69              8.11              7.17              0.00
2048                   19.17              9.86              8.68              0.00
4096                   42.95             22.44             20.84              1.60
8192                   59.71             26.03             23.53              0.00
16384                  70.19             37.00             34.80              4.63
32768                 129.68             67.75             64.29              3.67
65536                 246.75            135.33            129.61             14.04
131072                426.09            216.07            206.75              0.00
262144               2569.53            505.61            385.29              0.00
524288               5755.63           2266.28           2028.62              0.00
1048576              7544.05           3784.44           3576.31              0.00
openmpi@4.0.5%gcc@10.2.0 osu_ibcast

# OSU MPI Non-Blocking Broadcast Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.54              2.20              1.40              4.51
2                       3.34              2.14              1.36             12.25
4                       3.46              2.16              1.37              4.72
8                       3.38              2.15              1.42             13.39
16                      3.39              2.14              1.44             13.50
32                      3.43              2.15              1.47             12.63
64                      3.57              2.20              1.56             12.12
128                     3.99              2.50              1.74             15.07
256                     4.54              2.86              2.29             26.80
512                     5.77              3.20              2.44              0.00
1024                    5.59              3.29              2.67             14.17
2048                    6.33              3.73              2.94             11.72
4096                   11.04              6.36              5.37             12.76
8192                   15.66              8.34              7.37              0.72
16384                  22.62             12.17             10.97              4.69
32768                  38.44             22.14             20.74             21.42
65536                  64.75             36.79             34.68             19.36
131072                119.87             68.82             65.56             22.14
262144                325.43            177.37            127.86              0.00
524288                454.86            260.44            250.31             22.33
1048576               992.71            554.17            533.54             17.81
openmpi@4.0.5%gcc@10.2.0 osu_reduce

# OSU MPI Reduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.65
8                       0.63
16                      0.66
32                      0.72
64                      0.70
128                     0.89
256                     0.91
512                     1.57
1024                    1.96
2048                    3.55
4096                    6.73
8192                   13.47
16384                  12.60
32768                  22.60
65536                  48.43
131072                112.42
262144                171.72
524288                318.26
1048576               711.84
openmpi@4.0.5%gcc@10.2.0 osu_barrier

# OSU MPI Barrier Latency Test v5.6.3
# Avg Latency(us)
             1.13
openmpi@4.0.5%gcc@10.2.0 osu_iallreduce

# OSU MPI Non-blocking Allreduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       7.33              3.70              2.85              0.00
8                       7.87              3.92              2.94              0.00
16                      7.94              3.91              3.14              0.00
32                      8.04              4.03              3.20              0.00
64                      8.29              4.03              3.25              0.00
128                     8.45              4.91              4.05             12.57
256                     8.94              4.88              3.52              0.00
512                    11.17              6.15              4.91              0.00
1024                   15.70              7.00              5.69              0.00
2048                   17.20              8.88              7.75              0.00
4096                   33.54             16.28             14.86              0.00
8192                   48.31             23.10             21.58              0.00
16384                  77.78             38.94             36.31              0.00
32768                 131.49             63.41             60.15              0.00
65536                 200.65             78.73             75.13              0.00
131072                264.26            132.59            126.86              0.00
262144                406.62            191.79            183.94              0.00
524288                819.28            431.90            414.77              6.60
1048576              1500.57            715.71            684.27              0.00
openmpi@4.0.5%gcc@10.2.0 osu_igather

# OSU MPI Non-blocking Gather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.36              1.47              0.74              0.00
2                       2.33              1.46              0.68              0.00
4                       2.17              1.33              0.66              0.00
8                       2.32              1.44              0.66              0.00
16                      2.17              1.32              0.67              0.00
32                      2.16              1.32              0.66              0.00
64                      2.19              1.31              0.70              0.00
128                     2.33              1.45              0.69              0.00
256                     2.42              1.48              0.75              0.00
512                     6.00              3.24              2.41              0.00
1024                    7.42              3.54              2.78              0.00
2048                    9.03              4.07              3.37              0.00
4096                   17.11              7.28              6.40              0.00
8192                   23.25              9.74              8.81              0.00
16384                  33.54             14.18             13.12              0.00
32768                 117.31             30.61             28.54              0.00
65536                 110.75             55.95             53.17              0.00
131072                206.94            102.17             97.51              0.00
262144                399.76            199.75            191.90              0.00
524288                817.04            393.96            378.83              0.00
1048576              1852.96            863.81            823.29              0.00
openmpi@4.0.5%gcc@10.2.0 osu_reduce_scatter

# OSU MPI Reduce_scatter Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.90
8                       0.90
16                      1.14
32                      1.62
64                      1.63
128                     1.66
256                     1.82
512                     2.03
1024                    2.74
2048                    3.46
4096                    4.37
8192                    6.77
16384                  11.18
32768                  17.90
65536                  29.37
131072                143.40
262144                311.43
524288                238.13
1048576               485.78
openmpi@4.0.5%gcc@10.2.0 osu_acc_latency
# OSU MPI_Accumulate latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.04
2                       0.04
4                       0.04
8                       0.04
16                      0.04
32                      0.06
64                      0.09
128                     0.13
256                     0.21
512                     0.38
1024                    0.73
2048                    1.40
4096                    2.76
8192                    5.47
16384                  10.88
32768                  21.78
65536                  43.19
131072                 85.75
262144                171.81
524288                344.80
1048576               705.78
2097152              1382.28
4194304              2898.16
openmpi@4.0.5%gcc@10.2.0 osu_fop_latency
# OSU MPI_Fetch_and_op latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.03
openmpi@4.0.5%gcc@10.2.0 osu_get_bw
# OSU MPI_Get Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      38.06
2                      83.71
4                     166.62
8                     347.79
16                    590.09
32                   1436.99
64                   2685.48
128                  5189.77
256                  8934.36
512                 17054.05
1024                26622.58
2048                25854.96
4096                28298.44
8192                29196.62
16384               15138.39
32768               12197.15
65536               11307.57
131072               9081.13
262144               5538.31
524288               4620.61
1048576              4389.38
2097152              4537.80
4194304              4620.11
openmpi@4.0.5%gcc@10.2.0 osu_put_bibw
# OSU MPI_Put Bi-directional Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_post/start/complete/wait
# Size      Bandwidth (MB/s)
1                      40.22
2                      93.56
4                     196.51
8                     387.98
16                    794.01
32                   1548.98
64                   3051.44
128                  4762.35
256                  9543.23
512                 16965.54
1024                28947.00
2048                37793.66
4096                42299.34
8192                37253.95
16384               24669.79
32768               21559.68
65536               19357.82
131072              12349.99
262144               8401.48
524288               8055.50
1048576              8819.54
2097152              8568.92
4194304              8722.11
openmpi@4.0.5%gcc@10.2.0 osu_put_latency
# OSU MPI_Put Latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.03
2                       0.03
4                       0.03
8                       0.03
16                      0.03
32                      0.03
64                      0.03
128                     0.03
256                     0.03
512                     0.03
1024                    0.04
2048                    0.05
4096                    0.07
8192                    0.22
16384                   0.42
32768                   1.48
65536                   2.99
131072                  5.88
262144                 11.86
524288                 23.80
1048576                66.13
2097152               170.82
4194304               354.49
openmpi@4.0.5%gcc@10.2.0 osu_cas_latency
# OSU MPI_Compare_and_swap latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.03
openmpi@4.0.5%gcc@10.2.0 osu_get_acc_latency
# OSU MPI_Get_accumulate latency Test v5.6.3
# Window creation: MPI_Win_create
# Synchronization: MPI_Win_lock/unlock
# Size          Latency (us)
1                       3.43
2                       3.41
4                       3.34
8                       2.88
16                      2.83
32                      2.89
64                      2.87
128                     2.97
256                     3.19
512                     3.59
1024                    4.24
2048                    5.57
4096                    8.76
8192                   13.11
16384                  21.57
32768                  36.95
65536                  64.06
131072                121.12
262144                228.07
524288                461.47
1048576               930.96
2097152              1864.70
4194304              4036.32
openmpi@4.0.5%gcc@10.2.0 osu_get_latency
# OSU MPI_Get latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.03
2                       0.03
4                       0.03
8                       0.03
16                      0.03
32                      0.04
64                      0.03
128                     0.03
256                     0.03
512                     0.03
1024                    0.04
2048                    0.05
4096                    0.07
8192                    0.10
16384                   0.35
32768                   1.23
65536                   2.43
131072                  4.09
262144                 11.28
524288                 21.28
1048576                63.26
2097152               168.99
4194304               344.35
openmpi@4.0.5%gcc@10.2.0 osu_put_bw
# OSU MPI_Put Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      44.78
2                      75.72
4                     179.78
8                     343.52
16                    657.44
32                   1539.76
64                   3093.73
128                  5080.63
256                 11024.46
512                 15065.95
1024                21198.15
2048                26584.51
4096                24428.03
8192                20434.38
16384               12665.05
32768               10903.49
65536               10134.39
131072               7697.89
262144               4732.12
524288               4208.56
1048576              4339.57
2097152              4590.90
4194304              4664.82
-- linux-debian9-cascadelake / gcc@10.2.0 -----------------------
hwloc@2.2.0
libiconv@1.16
libpciaccess@0.16
libxml2@2.9.10
mpich@3.3.2
osu-micro-benchmarks@5.6.3
xz@5.2.5
zlib@1.2.11
mpich@3.3.2%gcc@10.2.0 osu_hello
# OSU MPI Hello World Test v5.6.3
This is a test with 2 processes
mpich@3.3.2%gcc@10.2.0 osu_init
# OSU MPI Init Test v5.6.3
nprocs: 2, min: 13 ms, max: 13 ms, avg: 13 ms
mpich@3.3.2%gcc@10.2.0 osu_bibw
# OSU MPI Bi-Directional Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       4.49
2                       8.90
4                      16.60
8                      24.44
16                     51.20
32                    101.86
64                    184.96
128                   367.82
256                   729.47
512                  1344.61
1024                 2305.73
2048                 4078.16
4096                 7323.97
8192                10087.03
16384               11819.53
32768               11794.57
65536                8770.44
131072              10813.23
262144              12659.09
524288              12620.10
1048576             10952.85
2097152              9800.37
4194304              9503.74
mpich@3.3.2%gcc@10.2.0 osu_latency
# OSU MPI Latency Test v5.6.3
# Size          Latency (us)
0                       0.28
1                       0.28
2                       0.31
4                       0.30
8                       0.30
16                      0.35
32                      0.35
64                      0.37
128                     0.39
256                     0.46
512                     0.49
1024                    0.61
2048                    0.84
4096                    1.21
8192                    2.10
16384                   3.78
32768                   5.94
65536                   7.78
131072                 13.82
262144                 21.09
524288                 42.15
1048576                95.25
2097152               210.11
4194304               453.47
mpich@3.3.2%gcc@10.2.0 osu_latency_mt
# Number of Sender threads: 1
# Number of Receiver threads: 2
# OSU MPI Multi-threaded Latency Test v5.6.3
# Size          Latency (us)
0                       2.70
1                       2.90
2                       2.80
4                       2.70
8                       2.98
16                      3.04
32                      2.42
64                      2.31
128                     2.07
256                     1.98
512                     2.36
1024                    3.26
2048                    4.05
4096                    4.84
8192                    6.26
16384                   7.54
32768                   9.54
65536                   9.98
131072                 13.44
262144                 24.09
524288                 45.24
1048576               103.14
2097152               223.08
4194304               461.02
mpich@3.3.2%gcc@10.2.0 osu_multi_lat
# OSU MPI Multi Latency Test v5.6.3
# Size          Latency (us)
0                       0.25
1                       0.28
2                       0.32
4                       0.29
8                       0.30
16                      0.31
32                      0.32
64                      0.34
128                     0.36
256                     0.40
512                     0.45
1024                    0.59
2048                    0.82
4096                    1.20
8192                    2.05
16384                   3.75
32768                   6.07
65536                   7.69
131072                 12.26
262144                 20.87
524288                 42.67
1048576               101.50
2097152               215.64
4194304               447.52
mpich@3.3.2%gcc@10.2.0 osu_bw
# OSU MPI Bandwidth Test v5.6.3
# Size      Bandwidth (MB/s)
1                       4.18
2                       7.82
4                      16.27
8                      32.82
16                     71.54
32                    139.06
64                    269.07
128                   544.18
256                   915.65
512                  1801.53
1024                 2857.66
2048                 4417.84
4096                 6795.50
8192                 8578.21
16384               10321.63
32768               10772.92
65536                7741.17
131072               9056.12
262144              10136.30
524288              10994.34
1048576             10755.14
2097152              9759.75
4194304              9511.89
mpich@3.3.2%gcc@10.2.0 osu_latency_mp
# OSU MPI Multi-process Latency Test v5.6.3
# Number of forked processes in sender: 2
# Number of forked processes in receiver: 2
# Size          Latency (us)
0                       0.26
1                       0.29
2                       0.32
4                       0.32
8                       0.30
16                      0.32
32                      0.33
64                      0.37
128                     0.38
256                     0.45
512                     0.46
1024                    0.59
2048                    0.83
4096                    1.19
8192                    2.11
16384                   3.76
32768                   6.02
65536                   7.76
131072                 12.19
262144                 21.91
524288                 42.63
1048576                96.19
2097152               211.39
4194304               449.45
mpich@3.3.2%gcc@10.2.0 osu_mbw_mr
# OSU MPI Multiple Bandwidth / Message Rate Test v5.6.3
# [ pairs: 1 ] [ window size: 64 ]
# Size                  MB/s        Messages/s
1                       4.49        4485886.63
2                       8.87        4436216.43
4                      17.41        4352772.11
8                      35.54        4442089.29
16                     69.52        4345021.95
32                    142.38        4449452.28
64                    232.44        3631923.37
128                   547.30        4275811.66
256                   999.27        3903380.19
512                  1755.51        3428732.35
1024                 3150.46        3076624.14
2048                 4603.55        2247826.63
4096                 6324.48        1544063.60
8192                 7803.77         952608.17
16384                9880.14         603036.02
32768               10430.69         318319.27
65536                7493.02         114334.40
131072               8603.82          65641.93
262144               9912.07          37811.54
524288              10608.73          20234.56
1048576              9969.63           9507.78
2097152              9630.93           4592.39
4194304              9571.84           2282.11
mpich@3.3.2%gcc@10.2.0 osu_allgather

# OSU MPI Allgather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       2.10
2                       1.93
4                       1.93
8                       1.95
16                      1.93
32                      1.98
64                      2.22
128                     2.34
256                     3.85
512                     3.10
1024                    3.75
2048                    5.41
4096                    8.08
8192                   18.11
16384                  27.05
32768                  51.94
65536                 128.91
131072                251.03
262144                505.62
524288               1191.15
1048576              3284.28
mpich@3.3.2%gcc@10.2.0 osu_bcast

# OSU MPI Broadcast Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.63
2                       0.50
4                       0.45
8                       0.44
16                      0.46
32                      0.56
64                      0.57
128                     0.65
256                     0.74
512                     0.99
1024                    1.13
2048                    1.51
4096                    2.40
8192                    4.04
16384                  11.16
32768                  15.25
65536                  25.74
131072                 44.46
262144                 88.25
524288                182.65
1048576               345.93
mpich@3.3.2%gcc@10.2.0 osu_ialltoall

# OSU MPI Non-blocking All-to-All Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                      11.86              5.30              4.35              0.00
2                      13.12              5.36              4.57              0.00
4                       9.76              5.20              4.31              0.00
8                      11.48              5.71              4.77              0.00
16                     11.46              5.84              4.92              0.00
32                     12.72              6.43              5.38              0.00
64                     12.86              6.83              5.87              0.00
128                    13.08              6.71              5.54              0.00
256                    14.53              7.76              6.67              0.00
512                    10.83              5.74              5.04              0.00
1024                   12.46              6.69              5.70              0.00
2048                   14.76              7.75              6.80              0.00
4096                   19.77             10.31              9.34              0.00
8192                   27.85             14.50             13.36              0.03
16384                  47.76             24.46             22.89              0.00
32768                  93.24             47.29             44.77              0.00
65536                 279.60            143.58            137.31              0.94
131072                514.41            255.85            245.12              0.00
262144               1543.09            800.32            768.22              3.31
524288               6544.62           2290.38           2104.51              0.00
1048576             14153.85           6504.75           6124.84              0.00
mpich@3.3.2%gcc@10.2.0 osu_igatherv

# OSU MPI Non-blocking Gatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.22              1.32              0.70              0.00
2                       2.16              1.30              0.67              0.00
4                       2.15              1.30              0.68              0.00
8                       2.18              1.31              0.67              0.00
16                      2.19              1.30              0.67              0.00
32                      2.23              1.35              0.67              0.00
64                      2.21              1.30              0.67              0.00
128                     2.26              1.32              0.73              0.00
256                     2.49              1.48              0.83              0.00
512                     2.66              1.58              0.95              0.00
1024                    2.71              1.54              1.04              0.00
2048                    3.26              1.78              1.33              0.00
4096                    4.50              2.59              1.76              0.00
8192                    6.01              3.43              2.63              1.82
16384                   8.71              4.99              4.21             11.62
32768                  14.77              8.27              7.30             10.93
65536                  85.81             32.40             30.45              0.00
131072                154.24             56.93             54.04              0.00
262144                320.71            113.30            108.19              0.00
524288                987.62            295.25            282.88              0.00
1048576              1518.50            576.84            553.84              0.00
mpich@3.3.2%gcc@10.2.0 osu_scatter

# OSU MPI Scatter Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.62
2                       0.64
4                       0.60
8                       0.51
16                      0.53
32                      0.61
64                      0.70
128                     0.91
256                     1.04
512                     1.29
1024                    1.87
2048                    3.16
4096                    5.12
8192                    7.75
16384                  13.87
32768                  26.43
65536                  48.55
131072                 95.42
262144                195.97
524288                406.03
1048576               895.37
mpich@3.3.2%gcc@10.2.0 osu_allgatherv

# OSU MPI Allgatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       2.24
2                       2.27
4                       2.14
8                       2.18
16                      2.04
32                      2.08
64                      2.24
128                     2.52
256                     2.54
512                     3.26
1024                    3.80
2048                    5.27
4096                    8.13
8192                   14.61
16384                  44.02
32768                  64.99
65536                 125.98
131072                239.96
262144                529.89
524288               1254.22
1048576              3427.11
mpich@3.3.2%gcc@10.2.0 osu_gather

# OSU MPI Gather Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.78
2                       0.94
4                       0.86
8                       0.81
16                      0.90
32                      0.87
64                      1.01
128                     1.00
256                     1.01
512                     1.10
1024                    1.85
2048                    2.19
4096                    3.65
8192                    4.84
16384                   7.76
32768                  13.41
65536                  24.27
131072                 44.66
262144                 87.32
524288                181.90
1048576               413.42
mpich@3.3.2%gcc@10.2.0 osu_ialltoallv

# OSU MPI Non-blocking All-to-Allv Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       9.62              4.46              3.61              0.00
2                       8.31              4.53              3.62              0.00
4                       8.44              4.60              3.73              0.00
8                       8.42              4.46              3.56              0.00
16                      8.44              4.53              3.61              0.00
32                      8.68              4.68              3.81              0.00
64                      8.61              4.64              3.88              0.00
128                     9.08              4.88              4.14              0.00
256                    11.00              5.52              4.38              0.00
512                    10.61              5.61              4.78              0.00
1024                   12.97              7.00              6.16              2.99
2048                   15.01              7.89              6.83              0.00
4096                   19.65             10.29              9.30              0.00
8192                   27.82             14.60             13.34              0.94
16384                  49.62             25.88             24.29              2.24
32768                  92.01             46.99             44.47              0.00
65536                 245.40            112.99            106.84              0.00
131072                413.54            206.82            197.99              0.00
262144               1244.46            586.62            562.27              0.00
524288               3307.64           1604.42           1538.53              0.00
1048576              6879.18           3513.09           3377.55              0.34
mpich@3.3.2%gcc@10.2.0 osu_ireduce

# OSU MPI Non-blocking Reduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       3.70              2.21              1.55              3.68
8                       3.60              2.16              1.55              7.59
16                      3.56              2.32              1.61             23.16
32                      3.45              2.03              1.39              0.00
64                      3.28              2.00              1.40              8.18
128                     3.28              1.98              1.42              8.66
256                     3.73              2.01              1.45              0.00
512                     3.85              2.23              1.57              0.00
1024                    3.86              2.27              1.86             14.84
2048                    4.86              2.98              2.16             12.98
4096                   12.66              6.19              5.23              0.00
8192                   15.42              7.54              6.42              0.00
16384                  24.17             11.47             10.32              0.00
32768                  65.37             24.19             17.88              0.00
65536                 105.37             44.80             38.48              0.00
131072                317.46            147.65            141.41              0.00
262144                674.73            330.52            317.21              0.00
524288               1383.13            686.76            659.48              0.00
1048576              2939.86           1460.83           1403.84              0.00
mpich@3.3.2%gcc@10.2.0 osu_scatterv

# OSU MPI Scatterv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.76
2                       0.83
4                       0.79
8                       0.77
16                      0.78
32                      0.83
64                      0.89
128                     1.03
256                     1.43
512                     1.71
1024                    2.45
2048                    2.38
4096                    3.42
8192                    6.09
16384                   9.19
32768                  16.26
65536                  33.40
131072                 72.61
262144                148.60
524288                301.68
1048576               610.50
mpich@3.3.2%gcc@10.2.0 osu_allreduce

# OSU MPI Allreduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       2.69
8                       2.52
16                      2.77
32                      3.07
64                      3.10
128                     3.51
256                     3.51
512                     3.91
1024                    4.82
2048                    5.94
4096                   10.01
8192                   13.16
16384                  23.33
32768                  35.17
65536                  60.99
131072                192.26
262144                400.61
524288                964.30
1048576              2236.22
mpich@3.3.2%gcc@10.2.0 osu_gatherv

# OSU MPI Gatherv Latency Test v5.6.3
# Size       Avg Latency(us)
1                       0.33
2                       0.31
4                       0.30
8                       0.55
16                      0.36
32                      0.36
64                      0.34
128                     0.36
256                     0.37
512                     0.44
1024                    0.56
2048                    0.78
4096                    1.24
8192                    2.03
16384                   3.65
32768                   6.30
65536                  34.87
131072                 55.14
262144                104.25
524288                230.24
1048576               532.37
mpich@3.3.2%gcc@10.2.0 osu_ialltoallw

# OSU MPI Non-blocking All-to-Allw Personalized Exchange Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       8.78              4.69              3.99              0.00
2                       8.21              4.46              3.71              0.00
4                       8.13              4.44              3.53              0.00
8                       8.11              4.46              3.69              1.15
16                      8.17              4.45              3.47              0.00
32                      8.27              4.45              3.51              0.00
64                      8.42              4.45              3.72              0.00
128                     8.97              4.63              4.02              0.00
256                    10.43              5.56              4.52              0.00
512                    10.65              5.57              4.77              0.00
1024                   12.44              6.61              5.62              0.00
2048                   14.61              7.72              6.77              0.00
4096                   19.49             10.26              9.27              0.49
8192                   28.86             14.56             13.20              0.00
16384                  52.84             27.45             25.65              1.01
32768                  98.40             50.96             48.11              1.41
65536                 238.88            120.07            114.12              0.00
131072                529.91            238.25            228.36              0.00
262144               1294.57            672.54            645.06              3.57
524288               3235.24           1657.43           1586.57              0.55
1048576             12858.31           6992.70           6514.55              9.96
mpich@3.3.2%gcc@10.2.0 osu_iscatter

# OSU MPI Non-blocking Scatter Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       3.27              1.73              1.10              0.00
2                       3.24              1.85              1.14              0.00
4                       3.22              1.67              1.05              0.00
8                       3.40              1.72              1.08              0.00
16                      3.46              1.79              1.12              0.00
32                      3.45              1.80              1.13              0.00
64                      3.59              1.96              1.21              0.00
128                     3.87              2.28              1.37              0.00
256                     3.93              2.27              1.51              0.00
512                     4.19              2.30              1.76              0.00
1024                    5.73              3.38              2.38              0.89
2048                    7.38              4.26              3.52             11.42
4096                   10.65              6.48              5.52             24.47
8192                   15.76              9.32              8.27             22.03
16384                  30.90             16.00             14.75              0.00
32768                  57.36             29.42             27.64              0.00
65536                 105.65             54.03             51.05              0.00
131072                206.99            102.52             97.78              0.00
262144                432.66            215.39            206.32              0.00
524288                906.60            433.50            412.88              0.00
1048576              2148.85           1014.97            975.13              0.00
mpich@3.3.2%gcc@10.2.0 osu_alltoall

# OSU MPI All-to-All Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.60
2                       3.55
4                       3.51
8                       3.61
16                      3.59
32                      3.91
64                      4.22
128                     4.51
256                     4.83
512                     4.82
1024                    5.33
2048                    6.59
4096                    9.26
8192                   13.92
16384                  23.08
32768                  43.13
65536                 134.81
131072                243.82
262144                690.86
524288               1829.36
1048576              3572.97
mpich@3.3.2%gcc@10.2.0 osu_iallgather

# OSU MPI Non-blocking Allgather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       7.30              4.27              3.54             14.46
2                       6.09              3.18              2.49              0.00
4                       5.77              2.97              2.29              0.00
8                       5.86              3.01              2.34              0.00
16                      5.88              3.11              2.40              0.00
32                      6.01              3.13              2.39              0.00
64                      6.17              3.26              2.59              0.00
128                     6.36              3.36              2.63              0.00
256                     7.06              3.58              2.88              0.00
512                     8.16              4.23              3.38              0.00
1024                    8.87              4.66              3.95              0.00
2048                   11.92              6.34              5.38              0.00
4096                   16.38              8.58              7.55              0.00
8192                   25.75             13.38             12.11              0.00
16384                  54.03             27.90             26.05              0.00
32768                 106.03             53.30             50.60              0.00
65536                 266.42            134.70            128.64              0.00
131072                654.05            314.21            289.63              0.00
262144                997.22            505.84            485.46              0.00
524288               2395.54           1199.42           1151.49              0.00
1048576              6936.01           3562.05           3423.95              1.46
mpich@3.3.2%gcc@10.2.0 osu_ibarrier

# OSU MPI Non-blocking Barrier Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

       Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
              4.48              2.33              1.72              0.00
mpich@3.3.2%gcc@10.2.0 osu_iscatterv

# OSU MPI Non-blocking Scatterv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.63              1.79              1.05             20.01
2                       2.32              1.48              0.90              6.13
4                       2.32              1.50              0.89              8.39
8                       2.30              1.46              0.87              3.66
16                      2.38              1.52              0.89              4.31
32                      2.38              1.53              0.94              9.22
64                      2.61              1.63              0.99              1.68
128                     2.77              1.80              1.14             14.15
256                     3.08              2.06              1.28             20.45
512                     3.29              2.18              1.50             26.38
1024                    3.83              2.63              1.79             32.61
2048                    4.51              3.16              2.35             42.68
4096                    5.81              4.14              3.32             49.89
8192                    8.61              6.36              5.41             58.33
16384                  13.54             10.27              9.14             64.26
32768                  23.28             17.74             16.34             66.07
65536                  91.06             38.22             36.01              0.00
131072                184.70             77.67             73.30              0.00
262144                376.54            154.71            147.89              0.00
524288                740.44            300.20            288.01              0.00
1048576              1552.16            621.65            597.37              0.00
mpich@3.3.2%gcc@10.2.0 osu_alltoallv

# OSU MPI All-to-Allv Personalized Exchange Latency Test v5.6.3
# Size       Avg Latency(us)
1                       3.47
2                       3.19
4                       3.31
8                       3.28
16                      3.17
32                      3.28
64                      3.86
128                     3.89
256                     3.98
512                     9.07
1024                    5.42
2048                    7.28
4096                    9.22
8192                   16.20
16384                  24.02
32768                  45.70
65536                 105.23
131072                216.67
262144                631.46
524288               1595.45
1048576              3317.13
mpich@3.3.2%gcc@10.2.0 osu_iallgatherv

# OSU MPI Non-blocking Allgatherv Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       7.22              3.50              2.62              0.00
2                       6.77              3.37              2.60              0.00
4                       5.69              3.24              2.42              0.00
8                       5.73              3.24              2.41              0.00
16                      5.71              3.25              2.45              0.00
32                      6.37              3.44              2.67              0.00
64                      7.49              3.74              2.58              0.00
128                     6.19              3.37              2.68              0.00
256                     6.67              3.53              2.92              0.00
512                     8.36              4.39              3.53              0.00
1024                   10.23              5.38              4.53              0.00
2048                   12.61              6.65              5.76              0.00
4096                   18.05              9.27              8.36              0.00
8192                   30.04             15.57             14.39              0.00
16384                  73.21             36.37             34.34              0.00
32768                 138.55             69.74             66.34              0.00
65536                 262.40            135.37            129.50              1.90
131072                504.78            256.32            245.44              0.00
262144                983.77            498.75            478.32              0.00
524288               2414.12           1170.60           1120.90              0.00
1048576             10485.22           6416.96           6169.70             34.06
mpich@3.3.2%gcc@10.2.0 osu_ibcast

# OSU MPI Non-Blocking Broadcast Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.69              1.51              0.93              0.00
2                       2.65              1.35              0.90              0.00
4                       2.66              1.33              0.88              0.00
8                       2.69              1.42              0.89              0.00
16                      2.76              1.42              0.96              0.00
32                      2.87              1.67              1.03              0.00
64                      2.97              1.71              1.04              0.00
128                     3.08              1.78              1.06              0.00
256                     3.58              2.22              1.34              0.00
512                     3.63              2.18              1.33              0.00
1024                    3.99              2.36              1.53              0.00
2048                    4.44              2.57              2.03              7.90
4096                    6.35              3.63              2.84              4.22
8192                    8.08              5.29              4.44             37.15
16384                  21.44             13.02             11.89             29.16
32768                  28.99             16.66             15.34             19.59
65536                  54.84             33.68             31.70             33.26
131072                123.12             65.84             62.24              7.98
262144                236.39            120.00            113.60              0.00
524288                487.29            241.24            231.36              0.00
1048576               943.45            394.75            368.10              0.00
mpich@3.3.2%gcc@10.2.0 osu_reduce

# OSU MPI Reduce Latency Test v5.6.3
# Size       Avg Latency(us)
4                       0.87
8                       0.87
16                      0.89
32                      0.91
64                      5.43
128                     1.20
256                     4.18
512                     4.26
1024                    4.34
2048                    1.83
4096                    6.82
8192                    6.43
16384                  21.64
32768                  42.90
65536                  27.31
131072                335.66
262144                531.78
524288               1180.43
1048576              2281.46
mpich@3.3.2%gcc@10.2.0 osu_barrier

# OSU MPI Barrier Latency Test v5.6.3
# Avg Latency(us)
             2.30
mpich@3.3.2%gcc@10.2.0 osu_iallreduce

# OSU MPI Non-blocking Allreduce Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
4                       7.55              4.29              3.47              6.08
8                      17.09              3.45              2.88              0.00
16                      7.96              3.43              2.94              0.00
32                      9.64              5.63              3.31              0.00
64                     13.04              5.07              3.38              0.00
128                    32.84             18.09             12.92              0.00
256                    10.48              5.19              3.70              0.00
512                     9.89              5.52              4.53              3.45
1024                   21.47              7.14              5.14              0.00
2048                   16.41              8.12              7.30              0.00
4096                   28.99             10.90              9.79              0.00
8192                   54.51             19.84             16.84              0.00
16384                  63.65             31.91             29.38              0.00
32768                  99.32             47.94             44.09              0.00
65536                 164.39             87.11             82.77              6.63
131072                443.97            227.88            218.44              1.08
262144                901.29            475.06            456.01              6.53
524288               2143.51           1030.55            990.23              0.00
1048576              4784.04           2284.08           2182.68              0.00
mpich@3.3.2%gcc@10.2.0 osu_igather

# OSU MPI Non-blocking Gather Latency Test v5.6.3
# Overall = Coll. Init + Compute + MPI_Test + MPI_Wait

# Size           Overall(us)       Compute(us)    Pure Comm.(us)        Overlap(%)
1                       2.96              1.88              1.20              9.82
2                       2.90              1.82              1.18              8.37
4                       2.91              1.81              1.16              5.87
8                       2.94              1.83              1.22              9.29
16                      3.12              1.91              1.20              0.00
32                      3.18              1.97              1.31              7.09
64                      3.28              2.07              1.34              9.71
128                     3.25              2.02              1.36              9.79
256                     3.50              2.11              1.45              3.73
512                     3.35              2.09              1.48             15.26
1024                    4.02              2.43              1.79             11.37
2048                    4.77              2.86              2.21             13.64
4096                    6.25              3.78              3.11             20.52
8192                    9.71              5.74              4.73             16.16
16384                  14.78              8.51              7.60             17.45
32768                  32.45             15.15             13.90              0.00
65536                  76.38             26.25             24.61              0.00
131072                141.06             48.39             45.84              0.00
262144                278.40             93.23             88.96              0.00
524288                579.10            194.50            186.11              0.00
1048576              1287.54            420.27            395.93              0.00
mpich@3.3.2%gcc@10.2.0 osu_reduce_scatter

# OSU MPI Reduce_scatter Latency Test v5.6.3
# Size       Avg Latency(us)
4                       1.14
8                       1.21
16                      2.33
32                      2.67
64                      2.18
128                     2.39
256                     2.38
512                     2.40
1024                    2.73
2048                    3.24
4096                    3.94
8192                    7.65
16384                   8.99
32768                  15.48
65536                  32.05
131072                130.75
262144                308.14
524288                185.08
1048576               370.71
mpich@3.3.2%gcc@10.2.0 osu_acc_latency
# OSU MPI_Accumulate latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.09
2                       0.08
4                       0.08
8                       0.09
16                      0.09
32                      0.10
64                      0.12
128                     0.16
256                     0.23
512                     0.37
1024                    0.65
2048                    1.19
4096                    2.27
8192                    4.45
16384                   8.84
32768                  17.69
65536                  35.46
131072                 71.56
262144                143.75
524288                298.43
1048576               591.35
2097152              1172.46
4194304              2393.00
mpich@3.3.2%gcc@10.2.0 osu_fop_latency
# OSU MPI_Fetch_and_op latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.08
mpich@3.3.2%gcc@10.2.0 osu_get_bw
# OSU MPI_Get Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      38.13
2                      79.42
4                     123.28
8                     259.99
16                    450.68
32                    933.69
64                   2025.93
128                  4004.63
256                  7999.94
512                 14467.26
1024                22586.52
2048                31814.57
4096                36049.56
8192                24828.08
16384               14883.41
32768                9228.45
65536                8972.86
131072              10539.92
262144               5770.03
524288               4713.69
1048576              4522.97
2097152              4409.72
4194304              4697.17
mpich@3.3.2%gcc@10.2.0 osu_put_bibw
# OSU MPI_Put Bi-directional Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_post/start/complete/wait
# Size      Bandwidth (MB/s)
1                      43.05
2                      93.53
4                     168.96
8                     323.17
16                    564.76
32                   1244.02
64                   2639.00
128                  4690.75
256                  9624.58
512                 18155.74
1024                29878.03
2048                46062.49
4096                53258.01
8192                39686.40
16384               30446.84
32768               24001.89
65536               22145.25
131072              13003.20
262144               8911.61
524288               8335.37
1048576              8694.08
2097152              8772.80
4194304              8862.47
mpich@3.3.2%gcc@10.2.0 osu_put_latency
# OSU MPI_Put Latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.06
2                       0.06
4                       0.06
8                       0.07
16                      0.06
32                      0.06
64                      0.07
128                     0.06
256                     0.07
512                     0.06
1024                    0.07
2048                    0.08
4096                    0.11
8192                    0.14
16384                   0.29
32768                   1.35
65536                   2.36
131072                  5.16
262144                  9.94
524288                 17.40
1048576                66.09
2097152               176.45
4194304               385.61
mpich@3.3.2%gcc@10.2.0 osu_cas_latency
# OSU MPI_Compare_and_swap latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
8                       0.10
mpich@3.3.2%gcc@10.2.0 osu_get_acc_latency
# OSU MPI_Get_accumulate latency Test v5.6.3
# Window creation: MPI_Win_create
# Synchronization: MPI_Win_lock/unlock
# Size          Latency (us)
1                       1.50
2                       1.53
4                       1.53
8                       1.47
16                      1.55
32                      1.55
64                      1.61
128                     1.83
256                     1.94
512                     2.33
1024                    2.82
2048                    3.71
4096                    5.69
8192                    9.84
16384                  17.65
32768                  30.99
65536                  62.35
131072                142.65
262144                245.20
524288                459.43
1048576               872.21
2097152              1870.86
4194304              3706.58
mpich@3.3.2%gcc@10.2.0 osu_get_latency
# OSU MPI_Get latency Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size          Latency (us)
1                       0.06
2                       0.06
4                       0.07
8                       0.05
16                      0.07
32                      0.06
64                      0.08
128                     0.08
256                     0.08
512                     0.08
1024                    0.08
2048                    0.09
4096                    0.10
8192                    0.15
16384                   0.48
32768                   1.02
65536                   2.19
131072                  5.25
262144                  8.94
524288                 18.20
1048576                64.99
2097152               173.01
4194304               363.54
mpich@3.3.2%gcc@10.2.0 osu_put_bw
# OSU MPI_Put Bandwidth Test v5.6.3
# Window creation: MPI_Win_allocate
# Synchronization: MPI_Win_flush
# Size      Bandwidth (MB/s)
1                      38.13
2                      66.94
4                     144.90
8                     254.14
16                    356.13
32                   1014.16
64                   2131.50
128                  3771.65
256                  7099.12
512                 12574.47
1024                23881.66
2048                31704.49
4096                37551.63
8192                25983.97
16384               15426.33
32768               11752.41
65536               11390.95
131072              10790.10
262144               5701.38
524288               4483.51
1048576              4334.80
2097152              4369.55
4194304              4356.56
&lt;/code&gt;&lt;/pre&gt;</content><author><name></name></author><category term="ASC" /><summary type="html">引言</summary></entry><entry><title type="html">今日此时所想之事（二）</title><link href="http://localhost:4000/2021/02/11/%E4%BB%8A%E6%97%A5%E6%AD%A4%E6%97%B6%E6%89%80%E6%83%B3%E4%B9%8B%E4%BA%8B-%E4%BA%8C/" rel="alternate" type="text/html" title="今日此时所想之事（二）" /><published>2021-02-11T00:00:00+08:00</published><updated>2021-02-11T00:00:00+08:00</updated><id>http://localhost:4000/2021/02/11/%E4%BB%8A%E6%97%A5%E6%AD%A4%E6%97%B6%E6%89%80%E6%83%B3%E4%B9%8B%E4%BA%8B(%E4%BA%8C)</id><content type="html" xml:base="http://localhost:4000/2021/02/11/%E4%BB%8A%E6%97%A5%E6%AD%A4%E6%97%B6%E6%89%80%E6%83%B3%E4%B9%8B%E4%BA%8B-%E4%BA%8C/">&lt;h2 id=&quot;1&quot;&gt;1&lt;/h2&gt;

&lt;p&gt;二零二零年的大部分时间，我屏蔽了所有和保研相关的消息，也和家里产生了矛盾。想起高二暑假时的自己，因为想来广东最好的学校，把 SYSU 定成了接下来一年的目标和动力；现在的我又一次处在了相同的阶段。&lt;/p&gt;

&lt;p&gt;一个“成功”的保研人应该拿到一个更好学校的牛导 offer，但我对这一切都很抗拒。我觉得中大真是一所对我相性非常好的学校，有那么一些名气，但也不会让我太过自大。这让我产生了一些割裂感，因为大部分关系好的同学都在校外实习，或是准备其他学校的保研夏令营。无形中我对自己施加了很多压力：我主动选择了做一个“失败者”，被排除在了某一种可能性之外吗？答案很显然是否，但即使在漫长的失眠中思考，我也很难说上原因。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;定义你最终归宿的，一定是你能力和欲望综合的那个真实的你。…残酷的社会压力逼迫我们通过复制所谓的成功道路来获取社会资源，而完全忽视了&lt;strong&gt;人的内在动能才是决定你位置的根本因素&lt;/strong&gt;。&lt;/p&gt;

  &lt;p&gt;这些东西最终会决定你愿意为什么奋斗，最终会决定你对一份事业的热情和责任心，最终会决定你把自己放在什么样的位置是舒服的，最终会决定你人生的意义。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;十二月的一天，我翻到了一个&lt;a href=&quot;https://www.zhihu.com/question/23819007/answer/107332874&quot;&gt;两年前的回答&lt;/a&gt;，终于可以把长时间的这些想法表达出来。比起短跑，我更加喜欢和擅长长跑，我也相信不同人有不同的节拍。那么我可以远离学历上的鄙视链，唯一需要和同龄人比较的只有自己幸福感。我要找到最让自己舒服的位置，我要过的比所有人都快乐。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/12/26/pBT4SyzEKCicJ2O.gif&quot; alt=&quot;我啊，是真的想考音大吗？&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我啊，是真的想考音大吗？&lt;/p&gt;

&lt;h2 id=&quot;2&quot;&gt;2&lt;/h2&gt;

&lt;p&gt;当然，远离攀比也并不意味着我放弃了一切追求。保研前的最后一学期，无论如何我想证明一下自己仍然有和别人卷的能力。我已经提前算过，即使在这个学期把绩点拉满，只要前一名同学没有大的失误，仍然不会被我超过（最后确实只差了 0.13/100 分），因此自己的“内卷”不会影响任何人的保研名额。&lt;/p&gt;

&lt;p&gt;内卷是从选课的时候就要开始的，你要修的学分既不能太多（精力不够）也不能太少（GPA 将受限于一两门课程的给分，过于看脸）。对于数学基础好的同学，应当多选计算量多且难的课程；记忆力强的同学，则应该选要选期末考试且多是理论的课程；再或者像我一样多选实验分数占比高的课程。自己并没有很强的应试能力，那就只好在实验课上多卷一卷：要求给出一个基础版本及至少一个优化版本，我就&lt;a href=&quot;https://wu-kan.cn/_posts/2020-06-16-%E4%BD%BF%E7%94%A8-CUDA-%E4%BC%98%E5%8C%96%E4%BA%8C%E7%BB%B4%E4%BF%A1%E6%81%AF%E7%86%B5%E6%B1%82%E8%A7%A3/#%E8%AF%B7%E7%BB%99%E5%87%BA%E4%B8%80%E4%B8%AA%E5%9F%BA%E7%A1%80%E7%89%88%E6%9C%ACbaseline%E5%8F%8A%E8%87%B3%E5%B0%91%E4%B8%80%E4%B8%AA%E4%BC%98%E5%8C%96%E7%89%88%E6%9C%AC%E5%B9%B6%E5%88%86%E6%9E%90%E8%AF%B4%E6%98%8E%E6%AF%8F%E7%A7%8D%E4%BC%98%E5%8C%96%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D&quot;&gt;一共搞出了 15 个版本&lt;/a&gt;；要求识别代码中的实数，我就&lt;a href=&quot;https://wu-kan.cn/_posts/2020-05-14-%E4%BD%BF%E7%94%A8%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90%E5%99%A8-Flex-%E6%8F%90%E5%8F%96%E7%A8%8B%E5%BA%8F%E4%B8%AD%E7%9A%84%E6%95%B4%E6%95%B0%E5%92%8C%E6%B5%AE%E7%82%B9%E6%95%B0/#%E5%AE%9E%E9%AA%8C%E5%8E%9F%E7%90%86&quot;&gt;识别了所有 c 语言标准定义的格式，甚至考虑了排除 UNICODE 扩展中的数字&lt;/a&gt;；不要忘了还要在实验报告结尾提出几个工作量极大的“&lt;a href=&quot;https://wu-kan.cn/_posts/2020-05-14-%E4%BD%BF%E7%94%A8%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90%E5%99%A8-Flex-%E6%8F%90%E5%8F%96%E7%A8%8B%E5%BA%8F%E4%B8%AD%E7%9A%84%E6%95%B4%E6%95%B0%E5%92%8C%E6%B5%AE%E7%82%B9%E6%95%B0/#%E5%AE%9E%E9%AA%8C%E5%BF%83%E5%BE%97&quot;&gt;期待在日后的学习中加以解决的&lt;/a&gt;”问题，这会让老师觉得你对这个实验很有思考！当然也有翻车的地方，某门没有选的课程因为疫情的原因改为线上展示评分了，并且最后给出了人均 95+ 的高分。&lt;/p&gt;

&lt;p&gt;总之一个学期下来效果非常明显，三门计入保研排名的课程有两门都拿到了第一，年级排名也拉到了前五名。这甚至在&lt;a href=&quot;https://wu-kan.cn/_posts/2020-10-04-%E6%8E%A8%E5%85%8D%E7%AD%94%E8%BE%A9/&quot;&gt;推免面试&lt;/a&gt;上让老师（拿着我前三年的成绩单）特别关注了一下：“我看你前两年打了好多比赛，而学业成绩都不如大三的，是不是因为疫情没有比赛打？”当时无以言对，因为从客观上来看确实是这样，但我知道内在心态的转变和驱动才是真正的原因。&lt;/p&gt;

&lt;h2 id=&quot;3&quot;&gt;3&lt;/h2&gt;

&lt;p&gt;说到推免面试，我在英语提问环节拿到了&lt;a href=&quot;http://sdcs.sysu.edu.cn/content/5521&quot;&gt;前一百名里的最低分&lt;/a&gt;。我拿到的问题是，“你为什么喜欢 CS 专业”？我想即使是用中文，我也很难回答这个问题，因为 CS 专业只是我实现自己人生追求的一种手段，并不是喜爱本身。不过，也得到来自一位很棒的老师的肯定，这让我觉得很开心。有些遗憾我在这位老师的课上并不是学的最出彩的同学，但他的课真的非常不错。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/12/26/hw2tKTNW3bPSpnf.png&quot; alt=&quot;离谱&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4&quot;&gt;4&lt;/h2&gt;

&lt;p&gt;二零二零年对我来说是这样的不平淡，值得写上一篇文章纪念一下。我得到了去年这个时候在想的所有东西：侥幸不用考试就能继续有书念；在&lt;a href=&quot;https://wu-kan.github.io/resume/resume.pdf&quot;&gt;简历&lt;/a&gt;上又加了几项经历；作为队员参加的 ASC 20 取消之后，作为 advisor 带队和学弟们重新进了 ASC 21 的决赛；找了一份自己觉得还不错的&lt;a href=&quot;https://wu-kan.cn/_posts/2021-02-02-%E5%B9%B4%E8%BD%BB%E4%BA%BA%E7%9A%84%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95/&quot;&gt;实习&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;总之，我不再想给自己定下&lt;a href=&quot;https://wu-kan.cn/_posts/2020-01-24-%E4%BB%8A%E6%97%A5%E6%AD%A4%E6%97%B6%E6%89%80%E6%83%B3%E4%B9%8B%E4%BA%8B/&quot;&gt;像去年一样&lt;/a&gt;让人压力太大的愿望。今年我想看《佐贺 Ⅱ》，想看《京吹 Ⅲ》；想在生活中发掘更多的“自我”；想在科研上迈出第一步；想在 ASC 决赛拿一个不错的成绩，给在 SYSU 的本科生活画一个漂亮的句号。大二时把这样一句话送给了社团的学弟，现在我想再把它送给自己。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;心有猛虎，细嗅蔷薇。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;嗷呜~&lt;/p&gt;</content><author><name></name></author><category term="随笔" /><summary type="html">1</summary></entry><entry><title type="html">年轻人的第一次实习面试</title><link href="http://localhost:4000/2021/02/02/%E5%B9%B4%E8%BD%BB%E4%BA%BA%E7%9A%84%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95/" rel="alternate" type="text/html" title="年轻人的第一次实习面试" /><published>2021-02-02T00:00:00+08:00</published><updated>2021-02-02T00:00:00+08:00</updated><id>http://localhost:4000/2021/02/02/%E5%B9%B4%E8%BD%BB%E4%BA%BA%E7%9A%84%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95</id><content type="html" xml:base="http://localhost:4000/2021/02/02/%E5%B9%B4%E8%BD%BB%E4%BA%BA%E7%9A%84%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95/">&lt;h2 id=&quot;引子&quot;&gt;引子&lt;/h2&gt;

&lt;p&gt;大四保研之后除了打 ASC 之外并没有别的事情做，有亿点想在研究生之前找一段实习。正好上学期快接近期末的时候，和字节 AILab 的几位同学（悦航 &amp;amp; elfin）加了微信聊了一下，感觉他们需要做的东西和我研究生想做的方向比较符合，于是答应在 ASC 决赛名单出了之后考虑一下后面的时间安排再投一投简历。&lt;/p&gt;

&lt;p&gt;其实我本来是打算三月份左右再开始找一些暑期实习的，因此还没有准备好简历，临时在 GitHub 上拉了一个看起来不错的模板 &lt;a href=&quot;https://github.com/billryan/resume/tree/zh_CN&quot;&gt;billryan/resume&lt;/a&gt;。做简历的时候有点想吐槽自己好像没有什么能拿出手的论文或者项目经历，大部分都是课内作业和比赛项目…唯一有一点 star 的还是前端项目，也有丶离谱。&lt;/p&gt;

&lt;p&gt;19 号上午，ASC 赛会公布了决赛名单，我们还在上面，这意味着我在四月份到五月中的一段时间要同时准备决赛和毕业论文答辩。当天晚上，我把&lt;a href=&quot;https://wu-kan.github.io/resume/resume.pdf&quot;&gt;自己的简历&lt;/a&gt;发了过去。字节的流程非常快，当晚就有 HR 同学加我的微信。第二天上午，我在回家的高铁上和 HR 约了一面的时间在四天后的下午三点。&lt;/p&gt;

&lt;h2 id=&quot;一月二十四日一面&quot;&gt;一月二十四日，一面&lt;/h2&gt;

&lt;p&gt;我是在线上面试，点开邮件里的链接后会跳到牛客的线上面试平台。除了和面试官视频的窗口外，还有一个代码块编辑器和一键运行的按钮，显然面试会有线上 coding 环节。自我介绍完之后，果然马上就要编程，不过并不是考察算法，而是要写 CUDA。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;输入一个 $M\times N$ 的矩阵，输出其各行向量规约后得到的向量 $Y_i = \sum_{j=0}^{N-1}A_{i,j}$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;拿到这个问题有一点点出乎我的意料，最初和悦航加好友就是因为一年之前刚学 CUDA 时候写的两篇区间规约的博客（&lt;a href=&quot;https://wu-kan.cn/_posts/2020-02-25-%E7%94%A8Shuffle%E5%8A%A0%E9%80%9FCUDA%E4%B8%8A%E7%9A%84Reduce%E6%93%8D%E4%BD%9C/&quot;&gt;用 Shuffle 加速 CUDA 上的 Reduce 操作&lt;/a&gt;、&lt;a href=&quot;https://wu-kan.cn/_posts/2020-03-16-%E5%86%8D%E8%B0%88CUDA%E4%B8%8A%E7%9A%84Reduce%E6%93%8D%E4%BD%9C/&quot;&gt;再谈 CUDA 上的 Reduce 操作&lt;/a&gt;），但我确实有很长时间没有做过类似的操作了。所以在开始码代码的时候，回想之前是怎么做的同时，我也同时问了几个问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;数据范围大约是多大？（大约 $2048\times 2048$ 这个数量级）&lt;/li&gt;
  &lt;li&gt;被归约的数据类型是什么？问到一半，我想了想，顺手写了 &lt;code&gt;template&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;洗牌指令是 CUDA 6.5 引入的，但是现在的显卡基本都支持了，可不可以用？（可以）&lt;/li&gt;
  &lt;li&gt;可以通过多级规约/多次调用 cublas 的 asum 过程实现？（希望只启动一次 kernel ，减少多次启动的开销）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一边问一边写的差不多了。不过还是有些头大，有些 API（&lt;code&gt;__syncthreads&lt;/code&gt;）现场忘了，也不好去查文档。好在这里面试官说只要意思到了就可以了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;template &amp;lt;typename T, int WARPSIZE, int BLOCKSIZE&amp;gt;
__global__ void reduce(T *A, int lda, int n, T *y)
{
    A += lda * blockIdx.x;
    __shared__ T smem[BLOCKSIZE];
    {
        T val = 0;
        for (int id = threadIdx.x; id &amp;lt; n; id += BLOCKSIZE)
            val += A[id];
        smem[threadIdx.x] = val;
    }
#pragma unroll
    for (int offset = BLOCKSIZE &amp;gt;&amp;gt; 1; offset &amp;gt; (WARPSIZE &amp;gt;&amp;gt; 1); offset &amp;gt;&amp;gt;= 1)
    {
        __syncthreads();
        if (threadIdx.x &amp;lt; offset)
            smem[threadIdx.x] += smem[threadIdx.x ^ offset];
    }
    if (threadIdx.x &amp;lt; WARPSIZE)
    {
        T val = smem[threadIdx.x];
#pragma unroll
        for (int offset = WARPSIZE &amp;gt;&amp;gt; 1; offset &amp;gt; 0; offset &amp;gt;&amp;gt;= 1)
            val += __shfl_xor_sync(0xffffff, val, offset, WARPSIZE);
        if (threadIdx.x == 0)
            y[blockIdx.x] = val;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;码完之后，面试官又继续问了这个核函数启动时候的 &lt;code&gt;blockDim&lt;/code&gt; 和 &lt;code&gt;gridDim&lt;/code&gt; 怎么设置（前者 &lt;code&gt;(1024, 0, 0)&lt;/code&gt;，后者 &lt;code&gt;(m, 0, 0)&lt;/code&gt;）；答上来之后接着要我介绍了一下 grid，block，warp 等一些常见概念的意思（任务分发的最小单位，线程通信的最小单位，线程调度的最小单位）。&lt;/p&gt;

&lt;p&gt;然后面试官又继续问了一下，是否有更好的方案？比如这里用了很多的 shared memory，shared memory 过大会降低 SM 占用率。我算了一下，如果这里规约的类型是 8 字节的 &lt;code&gt;double&lt;/code&gt;，即使 &lt;code&gt;BLOCKSIZE&lt;/code&gt; 设置成 1024，每个 block 也只用了 8KB shared memory，按照 V100 96KB per SM 的容量来算，每个 SM 做多可以启动 12 个 block，而实际上每个 SM 最多只有 2048 个活跃线程，因此此处是完全不会影响 SM 占用率的。&lt;/p&gt;

&lt;p&gt;不过我好像会错意了，这里面试官的意思是说，使用 shared memory 的数量可以再少一些，可以每个 warp 之间用 shuffle 指令规约后，使用 shared memory 分享结果，再交给第一个 warp 规约，这样可以减少需要 shared memory 的数量减少 &lt;code&gt;WARPSIZE&lt;/code&gt; 倍。我觉得见仁见智，因为这样做实际上增大了线程的计算量（我写的代码里，下标大的线程在前几次规约后就可以空转了）；同时这也要求 &lt;code&gt;BLOCKSIZE == WARPSIZE * WARPSIZE&lt;/code&gt; ，并不具有很好的可移植性（在 ROCm 中 &lt;code&gt;WARPSIZE&lt;/code&gt; 通常是 64）。&lt;/p&gt;

&lt;p&gt;事后想一想，区间 reduce 这个案例确实是很适合面试现场问的一个问题，不像&lt;a href=&quot;https://wu-kan.cn/_posts/2019-11-29-CUDA%E7%9F%A9%E9%98%B5%E5%90%91%E9%87%8F%E4%B9%98%E7%9A%84%E5%A4%9A%E7%A7%8D%E4%BC%98%E5%8C%96/&quot;&gt;矩阵向量乘&lt;/a&gt;、&lt;a href=&quot;https://wu-kan.cn/_posts/2019-11-29-CUDA%E7%9F%A9%E9%98%B5%E5%90%91%E9%87%8F%E4%B9%98%E7%9A%84%E5%A4%9A%E7%A7%8D%E4%BC%98%E5%8C%96/&quot;&gt;矩阵乘法&lt;/a&gt;这样有太多细节容易写错，同时也有足够多优化的空间。&lt;/p&gt;

&lt;p&gt;再之后时间还有很多空余，就又来了一道传统的算法题。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;输入十进数 M &amp;lt; 1e8 , 输出最小的一个由 0、1 组成的十进数 N，满足 N &amp;gt; 0, N % M = 0， 如不存在输出 -1.
例子：
Input : 2 output : 10
Input : 3 output : 111&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一开始我在什么时候不存在这个解的情况纠结了一会儿。不过，面试官一直在引导我，先不考虑这些情况，直接去做呢？直接做的话那就是敲一个 BFS 了。太久没写 cpp，我甚至一开始忘记了 string 的下标是从前往后还是从后往前。此外，线上 coding 还是有点不习惯，因为全角半角括号没看出来导致一开始编译一直不过。&lt;/p&gt;

&lt;p&gt;跌跌撞撞总算是敲出了一个能跑的版本。之后，面试官问我，这个算法的空间复杂度上还能不能优化？我好像又把问题想复杂了（我回答“这里搜索状态是用 string 表示的一个大整数，在搜索队列拉长的时候状态也会变得很大。可以用一个整数表示这是当前搜索到的第几个大整数，然后从这个整数的二进制位来还原大整数，从而减少空间上的占用”）。面试官继续提示我，这里每次搜索结果只和我的余数 &lt;code&gt;sum&lt;/code&gt; 有关。于是我明白了他的意思，对这个余数判重就好了。不过确实，受数据范围 &lt;code&gt;M &amp;lt; 1e8&lt;/code&gt; 的影响，我一开始并没有往这个方向上想（判重需要上百 M 的空间，一些算法竞赛里的内存限制是 64M ~ 128M）。如果数据范围是 &lt;code&gt;M &amp;lt; 1e6&lt;/code&gt; 的话可能会好一些叭。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;lt;bits/stdc++.h&amp;gt;
using namespace std;
int main()
{
    int m;
    cin &amp;gt;&amp;gt; m;
    vector&amp;lt;char&amp;gt; flag(m, 0);
    for (deque&amp;lt;string&amp;gt; q(1, &quot;1&quot;); !q.empty(); q.pop_front())
    {
        string cur = q.front();
        int sum = 0;
        for (int i = 0; i &amp;lt; cur.size(); ++i)
        {
            sum = (sum * 10 + (cur[i] - '0')) % m;
        }
        if (sum == 0)
        {
            cout &amp;lt;&amp;lt; cur;
            return 0;
        }
        if (!flag[sum])
        {
            flag[sum] = 1;
            q.push_back(cur + &quot;0&quot;);
            q.push_back(cur + &quot;1&quot;);
        }
    }
    cout &amp;lt;&amp;lt; -1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再之后的面试基本上就以聊天为主，依稀记得有聊到，除了字节还想去哪里吗？我就老实说了，研究生还想去绿厂（NVIDIA）康康，毕竟要在 GPU 上做研究的话还是他家比较在行；不过在那之前我想先从“客户”的视角看看工业界是怎么用 GPU 的；接着又聊了聊字节这边的一些情况。面试结束后也有和面试官加微信，总体来说感觉人挺好 d。&lt;/p&gt;

&lt;h2 id=&quot;一月二十四日二面&quot;&gt;一月二十四日，二面&lt;/h2&gt;

&lt;p&gt;二面上来又是 coding ！&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$n, x, y, z$ 都是非负整数，求针对任意输入 $n$ ，满足 $n = 5x + 2y + z$ 的解 $(x, y, z)$ 的个数。
例如 $n = 5$ 的时候有如下四种情况：&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;n&lt;/th&gt;
        &lt;th&gt;x&lt;/th&gt;
        &lt;th&gt;y&lt;/th&gt;
        &lt;th&gt;z&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;5&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;3&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;2&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;5&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;p&gt;是我比较讨厌的推公式题（一般丢给队友做），不过真在面试碰上了也只能硬着头皮来了。实际上我自己做这类题一般的习惯是先写个暴力打表，然后找规律或者直接查 &lt;a href=&quot;http://oeis.org/?language=chineseT&quot;&gt;OEIS&lt;/a&gt;…感觉上这个题会有一个 $O(1)$ 的公式，不过不是很直观。只好先写一个 $O(\frac{n}{5})$ 枚举 $x$ 的代码。现场打了前 100 的表出来，我也并没有一眼看出来；组合数学的角度也没有好思路。&lt;/p&gt;

&lt;p&gt;这时候面试官提醒了一下，叠加的时候每次加的表达式都是一个只关于变量 x 的函数。大概明白意思了，把表达式里的公共项提出来即可，然后可以使用数列求和公式。不过 &lt;code&gt;5*x/2&lt;/code&gt; 这个式子里有整除，并不是等差数列，因此要按照 x 分奇偶讨论。写完奇数部分的代码已经很繁琐，让人不想再看了，偶数部分刚要开始写，面试官说可以了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;lt;iostream&amp;gt;
using namespace std;
int main()
{
    int n;
    cin &amp;gt;&amp;gt; n;
    //for(int n = 1; n &amp;lt; 100; ++n)
    //{
    long long ans = 0;
    /*
    for(int x=0; 5*x&amp;lt;=n; x+=2)
    {
        ans += n /2 - 5*x/2 + 1;
    }
    */
    int t = n / (5 * x) / 2 + 1;
    ans = (n / 2) * t + t + (0 + 5 * t) * t / 2;
    for (int x = 1; 5 * x &amp;lt;= n; x += 2)
    {
        //ans += (n - 5*x) /2 + 1;
        ans += (n - 1) / 2 - 5 * x / 2 + 1;
    }
    ans += ...;

    cout &amp;lt;&amp;lt; ans &amp;lt;&amp;lt; ' ';
    //}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再之后的面试感觉也是以聊为主。比如开始写 CUDA 有多少时间（从大三上的高性能计算课开始算的话是一年出头）；介绍一些自己写 CUDA 的经验（我记得我说了“CUDA 写起来结构感很强，因此如果要写出高性能的实现的话代码里一般会出现很多嵌套的 for 循环，包括 NVIDIA 自己给出来的 CUTLASS 库也是这样的；另外也建议多看 NVIDIA 的白皮书，练一些官方推荐的优化案例，比如矩阵向量乘、区间规约、矩阵乘法”）。又聊了些别的话题，记得有要我介绍自己做过的一个比较大的项目经历，就说了之前打 ASC 时候做 QuEST 优化的一些经历，感觉还行。然后话题又往超算队聊了一聊。&lt;/p&gt;

&lt;p&gt;二面将近结束的时候，面试官说，他之前已经加我微信和我聊过了（我楞了一下，问是悦航吗？是 elfin），可能是电脑外放太差我并没有从声音上听出来吧 hh 在之后又聊了聊入职之后想做什么，我说还是想做一些比较底层的东西，于是我们又提了一下之前聊到的 &lt;a href=&quot;https://docs.nvidia.com/cuda/nvrtc/index.html&quot;&gt;nvrtc&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;总之两轮技术面就这样过了，确实和传闻中的一样，到哪都要直接写代码。不过我觉得题目难度并不高（也许是因为我只是实习面试？），大概是 CF div2 B~C 题的难度，正常去做的话二十分钟应该可以做出来，不需要再特地去刷题（感觉最好还是提前练那么一两道找手感）。或者说，面试官在做题的时候其实也不会就在那里等着，会给一些提示和引导；刚好我做题时有这样那样啰里吧嗦的习惯，一眼没看出好的思路的时候也没有感觉特别尴尬。&lt;/p&gt;

&lt;h2 id=&quot;一月二十六日三面&quot;&gt;一月二十六日，三面&lt;/h2&gt;

&lt;p&gt;第二天，之前的 HR 同学就和我说过了技术面，之后还有 HR 面。于是约了在之后一天去面试。由于不需要线上编程，这次邮件点开之后是飞书的视频面试界面。&lt;/p&gt;

&lt;p&gt;嗯，三面感觉还是唠嗑。我记得聊的话题有，介绍一下简历上打的比赛（还是介绍了超算的一些比赛）；你在赛队里起到了什么作用（队长，内培/赛题分工/和学院老师联系/GPU 方向优化）；你感觉工作量最大的事情是什么（介绍了招新相关的一些事情）。总之，我觉得参加过超算比赛还是有挺多内容可以说的，倒是挺好的。然后 HR 和我说，前两面面试官反馈都挺好的，应该可以拿到 offer。于是也聊了聊入职之后的安排（三月底到五月中需要一个半月准备 ASC 决赛和毕业论文答辩；这部分可以请一个长假）。最后 HR 也有问我有什么想问的吗？我回答，之前和 elfin 聊的已经比较多了，对这个实习在做什么还有能学到什么已经有一个大致的估计了。&lt;/p&gt;

&lt;p&gt;总之，年轻人的第一次实习面试感觉来的太快，毫无准备。好在过了一两天，确定了拿到 offer，辣就完结撒花叭~&lt;/p&gt;

&lt;p&gt;&lt;del&gt;你们说我一个在广东上学的安徽人，怎么就到北京来实习了呢&lt;/del&gt;&lt;/p&gt;</content><author><name></name></author><category term="实习" /><summary type="html">引子</summary></entry><entry><title type="html">SYSU Collegiate Programming Contest 2020, Onsite</title><link href="http://localhost:4000/2020/11/29/SYSU-Collegiate-Programming-Contest-2020,-Onsite/" rel="alternate" type="text/html" title="SYSU Collegiate Programming Contest 2020, Onsite" /><published>2020-11-29T00:00:00+08:00</published><updated>2020-11-29T00:00:00+08:00</updated><id>http://localhost:4000/2020/11/29/SYSU%20Collegiate%20Programming%20Contest%202020,%20Onsite</id><content type="html" xml:base="http://localhost:4000/2020/11/29/SYSU-Collegiate-Programming-Contest-2020,-Onsite/">&lt;h1 id=&quot;中大校赛-2020&quot;&gt;中大校赛 2020&lt;/h1&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;a-undercover&quot;&gt;&lt;strong&gt;A&lt;/strong&gt; Undercover&lt;/h2&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;这题的关键是要知道有哪些人可能是卧底&lt;/li&gt;
  &lt;li&gt;也就是对于每个人，我要知道如果他是卧底，有多少人说了真话&lt;/li&gt;
  &lt;li&gt;统计每个人被说是卧底的次数 $X[i]$&lt;/li&gt;
  &lt;li&gt;每个人被说不是卧底的次数 $Y[i]$&lt;/li&gt;
  &lt;li&gt;说某人不是卧底的人的个数 $Z$&lt;/li&gt;
  &lt;li&gt;那么如果第 $i$ 个人是卧底，就有 $X[i]+(Z-Y[i])$ 个人说了真话&lt;/li&gt;
  &lt;li&gt;然后就是对于每个人，分类讨论了&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;b-orienteering&quot;&gt;&lt;strong&gt;B&lt;/strong&gt; Orienteering&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;命题： 吴坎&lt;/li&gt;
  &lt;li&gt;题意：询问 $n\le20$ 维超立方体上两点间的最短路。&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;E-立方选路算法
    &lt;ul&gt;
      &lt;li&gt;时间复杂度 $O(Tn)$&lt;/li&gt;
      &lt;li&gt;空间复杂度 $O(1)$&lt;/li&gt;
      &lt;li&gt;超算集群网络上的经典路由算法&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;超立方体
    &lt;ul&gt;
      &lt;li&gt;配图来自张永东老师课件&lt;/li&gt;
      &lt;li&gt;高度互联，弱可扩展的经直连典型网络构型&lt;/li&gt;
      &lt;li&gt;$n$ 维超立方体由两个 $n-1$ 维超立方体构成&lt;/li&gt;
      &lt;li&gt;任意两个相连的节点，其二进制表示仅有一位不同&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/11/21/6YQvAM7WmXyHUjR.jpg&quot; alt=&quot;三维超立方体&quot; /&gt;&lt;/p&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;路由计算
    &lt;ul&gt;
      &lt;li&gt;起点 $\overline{s_{n-1}\dots s_2s_1s_0}$&lt;/li&gt;
      &lt;li&gt;终点 $\overline{d_{n-1}\dots d_2d_1d_0}$&lt;/li&gt;
      &lt;li&gt;异或 $\overline{r_{n-1}\dots r_2r_1r_0}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;路由过程
    &lt;ul&gt;
      &lt;li&gt;$\overline{s_{n-1}\dots s_2s_1s_0}$&lt;/li&gt;
      &lt;li&gt;$\overline{s_{n-1}\dots s_2s_1\left(s_0\oplus r_0 \right)}$&lt;/li&gt;
      &lt;li&gt;$\overline{s_{n-1}\dots s_2\left(s_1\oplus r_1\right)d_0}$&lt;/li&gt;
      &lt;li&gt;$\dots$&lt;/li&gt;
      &lt;li&gt;$\overline{d_{n-1}\dots d_2d_1d_0}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/11/21/ZP7YxFrNIndMtgT.png&quot; alt=&quot;四维立方体上的示例&quot; /&gt;&lt;/p&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;E-立方选路算法有很多优美的性质，如
    &lt;ul&gt;
      &lt;li&gt;完成一个通信步的时间仅为网络节点数的对数级&lt;/li&gt;
      &lt;li&gt;可以多点同时通信且不造成阻塞&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;本题削减难度，只有单点通信
    &lt;ul&gt;
      &lt;li&gt;要跳过 E-立方算法得到结果中原地等待的步数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;欢迎大家参加两周后的计算机院定向越野！&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;c-compare&quot;&gt;&lt;strong&gt;C&lt;/strong&gt; Compare&lt;/h2&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;送分题&lt;/li&gt;
  &lt;li&gt;读入两个字符串&lt;/li&gt;
  &lt;li&gt;方法一：把后面多余的 0 去掉，然后比较字符串大小&lt;/li&gt;
  &lt;li&gt;方法二：在短串后面补 0 使其一样长，然后比较字符串大小&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;d-decode&quot;&gt;&lt;strong&gt;D&lt;/strong&gt; Decode&lt;/h2&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;模拟题，直接模拟即可&lt;/li&gt;
  &lt;li&gt;0 开头即为第一种，110 开头即为第二种，1110 开头即为第三种&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;e-reading&quot;&gt;&lt;strong&gt;E&lt;/strong&gt; Reading&lt;/h2&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;模拟题，模拟的时候要用到优先队列&lt;/li&gt;
  &lt;li&gt;用一个 &lt;code&gt;map&lt;/code&gt; 给每个字符串一个编号&lt;/li&gt;
  &lt;li&gt;用 &lt;code&gt;vector&lt;/code&gt; 存每本书有哪些单词，每个单词在哪些书里&lt;/li&gt;
  &lt;li&gt;用 &lt;code&gt;set&lt;/code&gt;（或 &lt;code&gt;priority_queue&lt;/code&gt;）维护以 &lt;code&gt;pair&amp;lt;生词数量,书本编号&amp;gt;&lt;/code&gt; 的小根堆&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;f-competition&quot;&gt;&lt;strong&gt;F&lt;/strong&gt; Competition&lt;/h2&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;先计算出每个队能解决的问题的最大难度：&lt;/li&gt;
  &lt;li&gt;维护一个堆，每次选出一个能力值最强的队伍&lt;/li&gt;
  &lt;li&gt;这个队伍能解决的问题的最大难度，就是“他现在的能力值”和“之前被选出的队伍能解决的问题的最大难度的最小值”的最小值&lt;/li&gt;
  &lt;li&gt;然后激励和他相连的队伍（注意维护优先队列）&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;g-sum&quot;&gt;&lt;strong&gt;G&lt;/strong&gt; Sum&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;简单的 DP，注意要开 &lt;code&gt;long long&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;h-sequence&quot;&gt;&lt;strong&gt;H&lt;/strong&gt; Sequence&lt;/h2&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;DP+线段树优化&lt;/li&gt;
  &lt;li&gt;给你一个长度为 N 的序列，你要修改最少的数字，然后分成 M 段，使得每一段都包含 1~K 的所有数字&lt;/li&gt;
  &lt;li&gt;用 F[i][j]表示把前 j 个数分成 i 份的答案&lt;/li&gt;
  &lt;li&gt;$F[i][j] = \min\lbrace F[i-1][p] + (K-g[p][j]) \vert p&amp;lt;=i-K\rbrace $&lt;/li&gt;
  &lt;li&gt;$g[p][j]$ 表示从 $p+1$ 到 $j$ 的不同数字个数&lt;/li&gt;
  &lt;li&gt;维护从 $j$ 往前，$1$ 到 $K$ 的每个数字的最右位置，从这位置往左，相当于让答案 $-1$&lt;/li&gt;
  &lt;li&gt;维护一棵线段树：支持区间加减、区间最小值查询&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;i-tianhe-iia&quot;&gt;&lt;strong&gt;I&lt;/strong&gt; TianHe-IIA&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;命题：吴坎&lt;/li&gt;
  &lt;li&gt;题意：维护区间上的欧拉变换、区间 x 方和等操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;区间欧拉变换曾经出现在 &lt;a href=&quot;https://wu-kan.cn/_posts/2019-12-15-SYSU-Novice-Programming-Contest-2019,-Online/&quot;&gt;19 年新手赛&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;快速收敛的区间变换操作&lt;/li&gt;
      &lt;li&gt;线段树上打标记&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;对曾经做过该题的同学有误导
    &lt;ul&gt;
      &lt;li&gt;存在区间赋值操作，不再收敛&lt;/li&gt;
      &lt;li&gt;数据范围加强，难以使用线段树完成&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;珂朵莉树
    &lt;ul&gt;
      &lt;li&gt;又称老司机树（Old Driver Tree, ODT）&lt;/li&gt;
      &lt;li&gt;在 &lt;code&gt;map&lt;/code&gt; 上以点代区间，暴力维护&lt;/li&gt;
      &lt;li&gt;2017 年一场 CF 比赛（896C）中提出的数据结构，题目背景主角是珂朵莉&lt;/li&gt;
      &lt;li&gt;一直没有正式比赛中见到过，于是搬到校赛上来&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/11/21/TDmdAHIezXr1tZY.png&quot; alt=&quot;珂朵莉&quot; /&gt;&lt;/p&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;在数据随机的时推平操作比较多
    &lt;ul&gt;
      &lt;li&gt;时间复杂度趋近于 $O(m\log n)$&lt;/li&gt;
      &lt;li&gt;用&lt;a href=&quot;https://arxiv.org/pdf/1408.3045v1.pdf&quot;&gt;这个数据结构&lt;/a&gt;可以把复杂度继续下降&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;非随机数下，出题人想要卡珂朵莉树时肯定会 T&lt;/li&gt;
  &lt;li&gt;标程 0.7s，时限 3s&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;使用场景
    &lt;ul&gt;
      &lt;li&gt;区间赋值操作&lt;/li&gt;
      &lt;li&gt;区间统计操作&lt;/li&gt;
      &lt;li&gt;最好用于随机数据&lt;/li&gt;
      &lt;li&gt;走投无路时冲一发暴力&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;j-traffic-lights&quot;&gt;&lt;strong&gt;J&lt;/strong&gt; Traffic Lights&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;这题的关键是要注意到：如果我们知道了第一次等红灯是在哪里，那么后面的情况也都知道了（在同一个位置等红灯的车，后面的情况都是一样的）&lt;/li&gt;
  &lt;li&gt;从后往前扫描，每个红灯要等待的时间段是一个区间（或者是环形区间的前面一段和后面一段）&lt;/li&gt;
  &lt;li&gt;问题就变成了区间覆盖问题，用数据结构维护即可&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;k-cards&quot;&gt;&lt;strong&gt;K&lt;/strong&gt; Cards&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;命题：左谭励&lt;/li&gt;
  &lt;li&gt;题意：$n \le 20$ 张卡牌，每次游戏胜利后随机刷出一张，每种牌出现的概率固定，求集齐两套牌期望要胜利多少次。&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;如果只是一套牌就很容易，可以状态压缩 DP，用 $F[S]$ 表示当前收集到的卡牌集合为 $S$ 时，收集一套卡牌&lt;strong&gt;还需要&lt;/strong&gt;的胜利次数&lt;/li&gt;
  &lt;li&gt;很容易写出递归方程 $F[S] = 1 + \sum_{i} p_i F[S \cup \lbrace i\rbrace ]$&lt;/li&gt;
  &lt;li&gt;为方便，用 $m(S) = \sum_{i\in S} 2^i$ 表示状态。
    &lt;ul&gt;
      &lt;li&gt;F[m(S)] = 1 + $\sum_{i\in S} p_i F[m(S)] + \sum_{i\notin S} p_i F[m(S)+2^i]$&lt;/li&gt;
      &lt;li&gt;直接计算即可&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;两套牌如果用 DP，状态数为 $O(3^n)$, $n = 20$ 的情况下计算时间太长。&lt;/li&gt;
  &lt;li&gt;考虑引入 min-max 容斥进行计算&lt;/li&gt;
  &lt;li&gt;$\max(S) = \sum_{T\subseteq S} (-1)^T \min(T)$&lt;/li&gt;
  &lt;li&gt;设 $a_1, a_2,\dots, a_n$ 为每种牌第二次出现的时间，那么本题就是在求 $\max_i a_i$&lt;/li&gt;
  &lt;li&gt;用 min-max 容斥，将题目转为，对于 1~n 的每个子集 T, 求这个子集中的元素第一次出现两次，期望游戏胜利的次数&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;p&gt;对于给定的 T，我们用动态规划求解 $\min(T)$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;F(S) 表示当前收集到的卡牌集合为 S 时，收集一套卡牌&lt;strong&gt;还需要&lt;/strong&gt;的胜利次数。
注意：这里 $S \subseteq T$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;F(S) = $\sum_{u\notin T} p(u) F(S) + \sum_{u\in T-S} p(u)f(S\cup u) + 1$&lt;/li&gt;
  &lt;li&gt;$[1 - \sum_{u\notin T}p(u)]F(S) = \sum_{u\in T-S}p(u)f(S\cup u) + 1$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;定义 $P(T) = \sum_{u\in T}p(u)$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$F(S) = \frac{1}{p(T)}\sum_{u\in T-S}p(u)f(S\cup u)+\color{#FF0000}{1}$&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;p&gt;$F(S) = \frac{1}{p(T)}\sum_{u\in T-S}p(u)f(S\cup u)+\color{#FF0000}{1}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;考虑每个 1 做的贡献，可得&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$F(\emptyset) = \sum_{u_1, u_2, …, u_k} \Pi_{i=1}^k \frac{p(u_i)}{P(T)} $, ($u_i \ne u_j$, $u_i \in T$)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;进一步简化，可得：&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$ F(\emptyset) = \sum\star{S\in T} \vert S\vert ! \Pi\star{u\in S} p(u) \times (\frac{1}{P(T)})^{\vert S\vert } $&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;定义 $G(T, j) = \sum_{S\in T,\vert S\vert =j} \vert S\vert ! \Pi_{u\in S} p(u)$,&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;结论：min(T) = $F(\emptyset) = \sum_{j=1}^{\vert T\vert } G(T, j) / p(T)^j$&lt;/p&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;p&gt;G 的计算：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$G(T, j) = \sum_{S\in T,\vert S\vert =j} \vert S\vert ! \Pi_{u\in S} p(u)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于任意的 $v\in T$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\sum_{S\in T,\vert S\vert =j,v\notin S} \vert S\vert ! \Pi_{u\in S} p(u) = G(T-v, j)$&lt;/li&gt;
  &lt;li&gt;$\sum_{S\in T,\vert S\vert =j,v\in S} \vert S\vert ! \Pi_{u\in S} p(u) = p(v) \cdot j \cdot G(T-v, j-1)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此 $G(T, j) = G(T - v, j) + p(v) \cdot j \cdot G(T-v, j-1)$, 预处理 G 的复杂度为 $O(n\cdot 2^n)$. 原问题计算所有 min(T) 的复杂度同样也是 $O(n\cdot 2^n)$.&lt;/p&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;l-teams&quot;&gt;&lt;strong&gt;L&lt;/strong&gt; Teams&lt;/h2&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;如果所有点的度数都很小，用并查集即可&lt;/li&gt;
  &lt;li&gt;对于每个点，log out 时让所在集合 size 减一&lt;/li&gt;
  &lt;li&gt;log in 时直接开一个新点&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;如果有的点度数很大怎么办？&lt;/li&gt;
  &lt;li&gt;把点分为两类，度数大于 $\sqrt{2M}$ 的为大度点，否则为小度点&lt;/li&gt;
  &lt;li&gt;大度点的个数不超过 $\sqrt{2M}$&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;考虑一个大度点，当它在线的时候，会把它的所有在线好友合并到同一个队伍里&lt;/li&gt;
  &lt;li&gt;当它 log out 之后，下次再 log in 的时候，还需要处理它的所有邻居吗？&lt;/li&gt;
  &lt;li&gt;不需要的话，需要处理的是哪些呢？&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;对于每个大度点，用 &lt;code&gt;unordered_set&lt;/code&gt; 维护两个集合：
    &lt;ol&gt;
      &lt;li&gt;和它相邻的在线的点的集合，记为 S1&lt;/li&gt;
      &lt;li&gt;自上次 log out 之后发生过 log in 事件的、相邻的在线的点的集合，记为 S2（S2 一定是 S1 的子集）&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;当大度点 log in 之后，只需要处理 S2 中的所有点，以及在 S1 中找到一个不在 S2 中的点（若没有即忽略）&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">中大校赛 2020</summary></entry><entry><title type="html">超算队内培：HPL作业总结</title><link href="http://localhost:4000/2020/11/25/%E8%B6%85%E7%AE%97%E9%98%9F%E5%86%85%E5%9F%B9-HPL%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93/" rel="alternate" type="text/html" title="超算队内培：HPL作业总结" /><published>2020-11-25T00:00:00+08:00</published><updated>2020-11-25T00:00:00+08:00</updated><id>http://localhost:4000/2020/11/25/%E8%B6%85%E7%AE%97%E9%98%9F%E5%86%85%E5%9F%B9:HPL%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93</id><content type="html" xml:base="http://localhost:4000/2020/11/25/%E8%B6%85%E7%AE%97%E9%98%9F%E5%86%85%E5%9F%B9-HPL%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93/">&lt;h2 id=&quot;跑分任务总结&quot;&gt;跑分任务总结&lt;/h2&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;(/TFLOPS)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Theoretical value&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Proposal&lt;/th&gt;
      &lt;th&gt;Yours&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HPL(4$\times$cpn)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12.4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8.45&lt;/td&gt;
      &lt;td&gt;8.76（姜智瀚）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HPL(2$\times$4 张 v100)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;62.4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;33.89&lt;/td&gt;
      &lt;td&gt;36.19（冯浚轩）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HPCG(4$\times$cpn)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12.4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.06&lt;/td&gt;
      &lt;td&gt;0.13（张景润，姜智瀚）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HPCG(2$\times$4 张 v100)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;62.4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.08&lt;/td&gt;
      &lt;td&gt;1.07（冯浚轩）&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!-- .slide --&gt;

&lt;h3 id=&quot;计算理论性能&quot;&gt;计算理论性能&lt;/h3&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ark.intel.com/content/www/cn/zh/ark/products/120490/intel-xeon-gold-6150-processor-24-75m-cache-2-70-ghz.html&quot;&gt;Intel Xeon Gold 6150&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;18 核 36 线程，TDP 165W&lt;/li&gt;
  &lt;li&gt;单核睿频 3.7 GHz，全核睿频 3.4Ghz，AVX2 全核睿频 3Ghz&lt;/li&gt;
  &lt;li&gt;AVX-512 只能运行在 2.7Ghz 以下，有 2 个 AVX-512 FMA 单元&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.sysu.tech/Benchmark/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97CPU%E7%AE%97%E5%8A%9B%E7%90%86%E8%AE%BA%E5%B3%B0%E5%80%BC/&quot;&gt;每时钟周期浮点运算数理论值&lt;/a&gt; $\frac{512}{64}\times 4=32$&lt;/li&gt;
  &lt;li&gt;HPL 是计算密集型程序，应当关闭超线程！&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[cpn233]$ cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c
     36  Intel(R) Xeon(R) Gold 6150 CPU @ 2.70GHz
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;$4\text{Nodes}\times 36\text{Cores}\times 2.7 \text{GHz}\times32\text{IPC}=12441.6 \text{GFLOPS}$&lt;/li&gt;
  &lt;li&gt;节点数 x 单节点物理核数 x 单核运行频率 x 每时钟周期浮点运算数&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;[gpu55]$ nvidia-smi --query-gpu=clocks.max.sm --format=csv --id=0
clocks.max.sm [MHz]
1530 MHz
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;V100 SXM2 单卡有 5120 个 CUDA 核心，2560 个 FP64 单元。&lt;/li&gt;
  &lt;li&gt;双精度性能为 $1530\text{MHz}\times2560\text{Cores}\times 2\approx 7.8\text{TFLOPS}$&lt;/li&gt;
  &lt;li&gt;ASC19 现场用的 V100 PCI-E 稍弱一些，$7 \text{TFLOPS}$&lt;/li&gt;
  &lt;li&gt;也可在&lt;a href=&quot;https://www.nvidia.cn/data-center/v100/&quot;&gt;官网&lt;/a&gt;上直接查到性能数据&lt;/li&gt;
  &lt;li&gt;明年比赛大概率会使用 &lt;a href=&quot;https://www.nvidia.cn/data-center/a100/&quot;&gt;A100 PCI-E&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;安培架构的 TensorCore 可做 FP64 矩阵乘法，要考虑进去！&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h3 id=&quot;选择针对硬件架构特别优化的软件软件包&quot;&gt;选择针对硬件架构特别优化的软件软件包&lt;/h3&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;反例：轩轩一开始使用英伟达官网上八年前的 hpl-2.0_FERMI_v15 进行跑分！
    &lt;ul&gt;
      &lt;li&gt;直接调用 cuBLAS，未对规模小的矩阵优化&lt;/li&gt;
      &lt;li&gt;只能跑到理论性能的十分之一&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;明年 A100 是新的 Ampere 架构，不能使用之前的二进制包（只支持到 Volta）！
    &lt;ul&gt;
      &lt;li&gt;单卡性能 19.5TFLOPS（TensorCore）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CPU 上的跑分，一般使用 MKL 中提供的即可。&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h3 id=&quot;优化通信&quot;&gt;优化通信&lt;/h3&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;blockquote&gt;
  &lt;p&gt;（使用两个节点后）…现在效率只有 58%了，怎么看这个数值都太感人了吧，可能是其它的小参数有限制，也可能是网络原因&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;引入多节点进行计算后会由于节点间延迟高通信的原因导致性能下降&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export I_MPI_FAVRICS=shm:dapl # 节点内共享内存，节点间用dapl
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;使用 IntelMPI 的时候可以配置 &lt;a href=&quot;http://blog.sysu.tech/MPI/Intel%20MPI/I_MPI_FABRICS/&quot;&gt;InfiniBand&lt;/a&gt; 来优化连接带宽。&lt;/li&gt;
  &lt;li&gt;Intel MPI 提供了名为 Direct Access Programming Library（DAPL）的中间层来支持多架构，兼容多种网络硬件及协议，优化网络互联。&lt;/li&gt;
  &lt;li&gt;其他 MPI 实现也有类似手段。可以参照去年永锋的 wiki：重新编译 openmpi + 不用 tcp 多机 GPU HPL。&lt;/li&gt;
  &lt;li&gt;总之，在网卡、交换机支持的情况下，尽量少用 TCP！&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h3 id=&quot;调参&quot;&gt;调参&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Linpack 算法可以说是最精妙的并行算法，算法本身的细节可以通过大量调整参数，应用于各种不同计算环境的 Benchmark（杜总）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;不同于机器学习中的“炼丹玄学”&lt;/li&gt;
  &lt;li&gt;讲武德，遵守基本法（则）&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-56-行-ns-问题规模&quot;&gt;第 5~6 行 Ns 问题规模&lt;/h4&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;Amdahl 定律
    &lt;ul&gt;
      &lt;li&gt;$\text{Speedup}=1/\left(\left(1-f\right)+\frac{f}{m}\right)$&lt;/li&gt;
      &lt;li&gt;$f$ 为问题的并行化比例，$m$ 为并行核数&lt;/li&gt;
      &lt;li&gt;问题规模 N 固定时，可并行化的比例是固定的，加速比有上限&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;Gustafson 定律
    &lt;ul&gt;
      &lt;li&gt;$\text{Speedup}=(1-f)+mf$&lt;/li&gt;
      &lt;li&gt;问题规模不固定时，问题并行化程度越高，加速比越接近于并行核数&lt;/li&gt;
      &lt;li&gt;对 Amdahl 模型的补充修正，重拾对大规模并行计算的信心&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;一般问题规模越大越好
    &lt;ul&gt;
      &lt;li&gt;不造成内存页交换&lt;/li&gt;
      &lt;li&gt;系统总内存（显存）的 80%?&lt;/li&gt;
      &lt;li&gt;系统空闲内存（显存）的 90%?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-78-行-nbs-分块大小计算粒度&quot;&gt;第 7~8 行 NBs 分块大小（计算粒度）&lt;/h4&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;矩阵被分成 $\text{NB}\times\text{NB}$ 的循环块，分配到各个进程当中去处理&lt;/li&gt;
  &lt;li&gt;取决于单进程所能调度的资源限制
    &lt;ul&gt;
      &lt;li&gt;CPU 核数&lt;/li&gt;
      &lt;li&gt;CPU 向量化指令同时操作的元素数&lt;/li&gt;
      &lt;li&gt;CPU 对应的缓存大小&lt;/li&gt;
      &lt;li&gt;GPU 单线程束的宽度&lt;/li&gt;
      &lt;li&gt;单进程占用内存（显存）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;一般 N 要微调成 NB 的整数倍，防止边缘处性能下降&lt;/li&gt;
  &lt;li&gt;$\text{NB}\times 8$ 一定是 Cache line 的倍数&lt;/li&gt;
  &lt;li&gt;对于 AVX-512 指令集优化的 benchmark，一般来说是 384
    &lt;ul&gt;
      &lt;li&gt;实际上还是都测一下&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;对于 GPU 上的 benchmark，一般至少要设置成 32 的整数倍
    &lt;ul&gt;
      &lt;li&gt;一个 Warp 的宽度&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NB 过大容易导致 Cache Miss&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-9-行-pmap-process-mapping-处理器阵列排布方式&quot;&gt;第 9 行 PMAP process mapping 处理器阵列排布方式&lt;/h4&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;按行的排列方式适用于节点数较少、每个节点内 CPU 数较多的系统&lt;/li&gt;
  &lt;li&gt;按列的排列方式适用于节点数较多、每个节点内 CPU 数较少的系统&lt;/li&gt;
  &lt;li&gt;一般在大规模集群系统上，按列的排列方式的性能远好于按行的排列方式&lt;/li&gt;
  &lt;li&gt;小型集群上，行优先略优&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-1112-行-p--q-二维进程映射&quot;&gt;第 11~12 行 P × Q 二维进程映射&lt;/h4&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;P × Q = 进程数
    &lt;ul&gt;
      &lt;li&gt;一般来说一个进程对于一个 CPU 可以得到最佳性能&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;P 的值尽量取得小一点（$P\le Q$），因为向量维度相同时，列向通信量（通信次数和通信数据量）要远大于横向通信。&lt;/li&gt;
  &lt;li&gt;P 不宜过小（1），不利于计算过程中通过 Lookahead 掩盖通信开销&lt;/li&gt;
  &lt;li&gt;$P = 2^n$，即 P 最好选择 2 的幂。
    &lt;ul&gt;
      &lt;li&gt;HPL 中，L 分解的列向通信可选二元交换法（Binary Exchange）。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-13-行-threshold-检验结果时的计算精度&quot;&gt;第 13 行 threshold 检验结果时的计算精度&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;不许改，使用默认的 16.0
    &lt;ul&gt;
      &lt;li&gt;不然结果非法就白跑了&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-1421-行-递归分解的方式&quot;&gt;第 14~21 行 递归分解的方式&lt;/h4&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;每次完成 NB 列的消元，然后更新后面的矩阵，NB 的消元就是 L 的分解&lt;/li&gt;
  &lt;li&gt;每次 L 的分解只在一列处理器中完成&lt;/li&gt;
  &lt;li&gt;PFACTs 和 RFACTs 存在三种方法，对应参数的含义详见参考论文
    &lt;ul&gt;
      &lt;li&gt;Left-looking&lt;/li&gt;
      &lt;li&gt;Crout-looking&lt;/li&gt;
      &lt;li&gt;Right-looking&lt;/li&gt;
      &lt;li&gt;对性能的影响不大，一般使用经验值（1 或 2）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NBMINs,NDIVS 取一样的值，经验值 2&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-23-行-bcasts-l-的横向广播方式&quot;&gt;第 23 行 BCASTs L 的横向广播方式&lt;/h4&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;前 4 种适合于快速网络&lt;/li&gt;
  &lt;li&gt;后 2 种采用将数据切割后传送的方式，主要适合于速度较慢的网络
    &lt;ul&gt;
      &lt;li&gt;一般不采用后两种方式。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;一般来说
    &lt;ul&gt;
      &lt;li&gt;在小规模系统中，选择 0 或 1&lt;/li&gt;
      &lt;li&gt;对于大规模系统，选择 3&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-25-行-depths-l-的横向通信的通信深度&quot;&gt;第 25 行 DEPTHs L 的横向通信的通信深度&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;依赖于机器的配置和问题规模的大小&lt;/li&gt;
  &lt;li&gt;经验值 1（优先）,0&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-2627-行-u-的广播算法&quot;&gt;第 26~27 行 U 的广播算法&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;U 的广播为列向广播，HPL 提供了 3 种 U 的广播算法：
    &lt;ul&gt;
      &lt;li&gt;二元交换（Binary Exchange）法&lt;/li&gt;
      &lt;li&gt;Long 法&lt;/li&gt;
      &lt;li&gt;混合法&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;推荐 Long 法&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-2829-行-l-和-u-的数据存放格式&quot;&gt;第 28~29 行 L 和 U 的数据存放格式&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;若选择“transposed”，则采用按列存放，否则按行存放。&lt;/li&gt;
  &lt;li&gt;影响 Cache Miss&lt;/li&gt;
  &lt;li&gt;GPU 上还要考虑合并访存的问题&lt;/li&gt;
  &lt;li&gt;推荐第一个 1，第二个 0&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-30-行-equilibration&quot;&gt;第 30 行 Equilibration&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;主要在回代中使用&lt;/li&gt;
  &lt;li&gt;对性能影响极小，可使结果更精确&lt;/li&gt;
  &lt;li&gt;建议打开&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h4 id=&quot;第-31-行-memory-alignment-in-double&quot;&gt;第 31 行 memory alignment in double&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;用于在内存分配中对齐地址&lt;/li&gt;
  &lt;li&gt;在向量化指令中有更佳表现&lt;/li&gt;
  &lt;li&gt;一般设为 8 的整数倍&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;presto-赛题探索&quot;&gt;PRESTO 赛题探索&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;甘家振
    &lt;ul&gt;
      &lt;li&gt;在天河上部署所有依赖，并完成 modulefile&lt;/li&gt;
      &lt;li&gt;在天河上成功运行两个算例&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;冯浚轩
    &lt;ul&gt;
      &lt;li&gt;在天河上部署所有依赖（使用 spack）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;吕天翔
    &lt;ul&gt;
      &lt;li&gt;在自己实验室的集群上运行两个算例&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;下一阶段任务&quot;&gt;下一阶段任务&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;自己有更多的想法可以自行探索&lt;/li&gt;
  &lt;li&gt;每个人的工作在 wiki 上展示出来的方式
    &lt;ul&gt;
      &lt;li&gt;争取做到“一个人做了等于大家都做了”的效果&lt;/li&gt;
      &lt;li&gt;切忌眼高手低&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;尽可能提高合作效率
    &lt;ul&gt;
      &lt;li&gt;避免无谓的重复试错&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;在天河上部署 PRESTO 环境依赖并成功跑通两个算例
    &lt;ul&gt;
      &lt;li&gt;后续在天河上测已优化程序的可扩展性&lt;/li&gt;
      &lt;li&gt;天河上已有安装好的 glib 和 fftw&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;分析两个官方算例，搞清楚一下几个问题
    &lt;ul&gt;
      &lt;li&gt;建议借助各种工具（advisor/vtune/aps，或自行探索其他工具）&lt;/li&gt;
      &lt;li&gt;这两个算例在算什么，有哪些流程？&lt;/li&gt;
      &lt;li&gt;每个步骤的耗时情况？&lt;/li&gt;
      &lt;li&gt;代码的热点、瓶颈在哪里（计算/访存/通信）？&lt;/li&gt;
      &lt;li&gt;各部分数据的依赖情况？&lt;/li&gt;
      &lt;li&gt;哪些地方可以并行？&lt;/li&gt;
      &lt;li&gt;整理一份初步的 proposal 草稿（中/英文）&lt;/li&gt;
      &lt;li&gt;做一些数据可视化工作&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;探索不同软件环境下的性能对比
    &lt;ul&gt;
      &lt;li&gt;不同编译器&lt;/li&gt;
      &lt;li&gt;不同编译选项&lt;/li&gt;
      &lt;li&gt;不同 MPI&lt;/li&gt;
      &lt;li&gt;python/ipython&lt;/li&gt;
      &lt;li&gt;最好能有一个自动化测试脚本，可以在决赛现场使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;尝试使用 mkl fftw 或 cufft 代替 fftw
    &lt;ul&gt;
      &lt;li&gt;一个可行方向，暂不清楚 fftw 在总时间里的占比&lt;/li&gt;
      &lt;li&gt;cufft 的接口和 fftw 接口略有差异，不过应该可以完成&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide vertical=true --&gt;

&lt;ul&gt;
  &lt;li&gt;阅读&lt;a href=&quot;https://wu-kan.github.io/sysu-thesis/main.pdf&quot;&gt;论文&lt;/a&gt;中对 HPL 算法的详细介绍&lt;/li&gt;
  &lt;li&gt;阅读 &lt;a href=&quot;https://enigmahuang.me/2017/12/27/HPCG_3_Notes/&quot;&gt;HPCG 3.0 reference implementation 阅读笔记&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;思考 HPCG 跑分时候如何分配进程到节点的映射&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;使用 3 机 12 卡，破 ASC19 HPL 跑分记录（50.21 TFlops）&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- .slide --&gt;

&lt;h2 id=&quot;下期内培预告&quot;&gt;下期内培预告&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;本周末，具体时间地点待定&lt;/li&gt;
  &lt;li&gt;By 黄承欢：选拔赛优化组必做题讲解+如何舒服地进行调优工作
    &lt;ul&gt;
      &lt;li&gt;优化至每轮迭代 1.4ms&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="ASC" /><summary type="html">跑分任务总结</summary></entry></feed>