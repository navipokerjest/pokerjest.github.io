<!DOCTYPE html><html><head><title> 图注意力网络 &middot; wu-kan</title><!-- Begin Jekyll SEO tag v2.7.1 --><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="图注意力网络" /><meta property="og:locale" content="en_US" /><meta name="description" content="图卷积网络（Graph Convolutional Network, GCN）告诉我们将局部的图结构和节点特征结合可以在节点分类任务中获得不错的表现；图注意力网络（Graph Attention Network, GAT）则在 GCN 的基础之上引入了注意力机制，从而克服了先前图卷积网络的短板。下面三篇论文有递进关系，这篇笔记着重整理其中第二篇论文：" /><meta property="og:description" content="图卷积网络（Graph Convolutional Network, GCN）告诉我们将局部的图结构和节点特征结合可以在节点分类任务中获得不错的表现；图注意力网络（Graph Attention Network, GAT）则在 GCN 的基础之上引入了注意力机制，从而克服了先前图卷积网络的短板。下面三篇论文有递进关系，这篇笔记着重整理其中第二篇论文：" /><link rel="canonical" href="http://localhost:4000/2020/06/07/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C/" /><meta property="og:url" content="http://localhost:4000/2020/06/07/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C/" /><meta property="og:site_name" content="wu-kan" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-06-07T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="图注意力网络" /><meta name="google-site-verification" content="YIKi1rBnyUaS-DMYiluseI5kZzTwjCkTFmKkSkMZDJk" /><meta name="baidu-site-verification" content="szbTSfUGAB" /> <script type="application/ld+json"> {"@type":"BlogPosting","url":"http://localhost:4000/2020/06/07/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C/","datePublished":"2020-06-07T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/06/07/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C/"},"headline":"图注意力网络","description":"图卷积网络（Graph Convolutional Network, GCN）告诉我们将局部的图结构和节点特征结合可以在节点分类任务中获得不错的表现；图注意力网络（Graph Attention Network, GAT）则在 GCN 的基础之上引入了注意力机制，从而克服了先前图卷积网络的短板。下面三篇论文有递进关系，这篇笔记着重整理其中第二篇论文：","dateModified":"2020-06-07T00:00:00+08:00","@context":"https://schema.org"}</script> <!-- End Jekyll SEO tag --><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" /><meta http-equiv="content-type" content="text/html; charset=utf-8" /><link rel="alternate" href="/feed.xml" title="RSS" type="application/rss+xml" /><link rel="apple-touch-icon-precomposed" href="https://gravatar.loli.net/avatar/289efba375d63424de3c49569c446744?s=320" /><link rel="shortcut icon" href="https://gravatar.loli.net/avatar/289efba375d63424de3c49569c446744?s=32" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/combine/gh/poole/lanyon@v1.1.0/public/css/poole.min.css,gh/poole/lanyon@v1.1.0/public/css/lanyon.min.css,gh/poole/lanyon@v1.1.0/public/css/syntax.min.css" /> <script async="async" src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/js/all.min.js" ></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Dreamer-Paul/Pio@2.4/static/pio.min.css" /><style> @media only print { .pio-container { display: none; } }</style><script async="async" src="https://cdn.jsdelivr.net/combine/gh/Dreamer-Paul/Pio@2.4/static/l2d.min.js,gh/Dreamer-Paul/Pio@2.4/static/pio.min.js" onload=' let pio_container = document.createElement("div"); pio_container.classList.add("pio-container"); pio_container.classList.add("right"); pio_container.style.bottom = "-2rem"; pio_container.style.zIndex = "1"; document.body.insertAdjacentElement("beforeend", pio_container); let pio_action = document.createElement("div"); pio_action.classList.add("pio-action"); pio_container.insertAdjacentElement("beforeend", pio_action); let pio_canvas = document.createElement("canvas"); pio_canvas.id = "pio"; pio_canvas.style.width = "14rem"; pio_canvas.width = "600"; pio_canvas.height = "800"; pio_container.insertAdjacentElement("beforeend", pio_canvas); let pio = new Paul_Pio({ "mode": "fixed", "hidden": true, "night": "for(let i=7; i<16; ++i) if(document.body.classList.contains(`theme-base-0`+i.toString(16))) { document.body.classList.remove(`theme-base-0`+i.toString(16)); document.body.classList.add(`theme-base-0`+((i-6)%9+7).toString(16)); break; }", "content": { "link": ["https:\/\/jekyll-theme-WuK.wu-kan.cn"], "skin": ["要换成我的朋友吗？", "让她放个假吧~"], "hidden": true, "custom": [{ "selector": "a", "type": "link", }, { "selector": ".sidebar-toggle", "text": "打开侧边栏叭~" }, { "selector": ".effect-info", "text": "哇，你发现了什么！" }, { "selector": "#sidebar-search-input", "text": "想搜索什么呢？很多干货哦！" }, { "selector": "#toc", "text": "这是目录~" }, { "selector": ".page-title", "text": "这是标题~" }, { "selector": ".v", "text": "评论没有审核，要对自己的发言负责哦~" }] }, "model": [ "https:\/\/cdn.jsdelivr.net/gh/imuncle/live2d/model/33/model.2018.bls-winter.json", "https:\/\/cdn.jsdelivr.net/gh/imuncle/live2d/model/platelet-2/model.json", "https:\/\/cdn.jsdelivr.net/gh/imuncle/live2d/model/xiaomai/xiaomai.model.json", "https:\/\/cdn.jsdelivr.net/gh/imuncle/live2d/model/mashiro/seifuku.model.json", "https:\/\/cdn.jsdelivr.net/gh/imuncle/live2d/model/Violet/14.json", "https:\/\/cdn.jsdelivr.net/gh/xiaoski/live2d_models_collection/Kobayaxi/Kobayaxi.model.json", "https:\/\/cdn.jsdelivr.net/gh/xiaoski/live2d_models_collection/mikoto/mikoto.model.json", "https:\/\/cdn.jsdelivr.net/gh/xiaoski/live2d_models_collection/uiharu/uiharu.model.json"] });' ></script> <script src='https://zz.bdstatic.com/linksubmit/push.js' async="async" ></script><style> .wrap { transition-property: width,background-size,transform; transition-duration: .3s; transition-timing-function: ease-in-out; min-height: 100%; display: inline-block; background-size: 100% auto; background-position: 0% 0%; background-repeat: no-repeat; background-attachment: fixed; background-image: url(https://Mizuno-Ai.wu-kan.cn/pixiv/74559485_p1.webp); } @media (min-aspect-ratio: 2400/1850) { .wrap { background-image: url(https://Mizuno-Ai.wu-kan.cn/pixiv/71932901_p0.webp); } } .sidebar-overlay #sidebar-checkbox:checked ~ .wrap { width: calc(100% - 14rem); background-size: calc(100% - 14rem) auto; transform: translateX(14rem); } .layout-reverse.sidebar-overlay #sidebar-checkbox:checked ~ .wrap { transform: translateX(0); }</style><style> .sidebar, html, h1, h2, h3, h4, h5, h6 { font-family: "Courier New", "Courier", "Hiragino Sans GB", "WenQuanYi Micro Hei", "Microsoft YaHei Light", "Microsoft JhengHei", monospace; }</style><style> td, th { padding: 0px; border: 0px; } table { border: 0px; } table tbody { display: block; overflow: scroll; } table thead, tbody tr { display: table; table-layout: fixed; width: 100%; }</style><style> img { display: inline-block; margin: 0; }</style><style> ::-webkit-scrollbar { width: 3px; height: 3px; } ::-webkit-scrollbar-thumb { background-image: linear-gradient(45deg, Cyan 0%, Magenta 50%, Yellow 100%); }</style><style> ::selection { color: White; background: Black; }</style><body class="theme-base-0d layout-reverse sidebar-overlay"> <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular styles, `#sidebar-checkbox` for behavior. --> <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" /> <!-- Toggleable sidebar --><div class="sidebar" id="sidebar"><div class="sidebar-item"><div class="effect effect-right_to_left"> <img class="effect-img" src="https://gravatar.loli.net/avatar/289efba375d63424de3c49569c446744?s=320" alt="img" /><div class="effect-info"> SYSU超算17级在读<br/> 水野爱<br/> HPC<br/> 田宫例四驱车<br/> <a href="mailto:i@wu-kan.cn"> <i class="fas fa-envelope"></i> </a> <a href="https://github.com/wu-kan"> <i class="fab fa-github"></i> </a> <a href="https://codeforces.com/profile/WuK"> <i class="fas fa-chart-bar"></i> </a> <a href="https://vjudge.net/user/WuK"> <i class="fas fa-smile"></i> </a> <a href="https://www.zhihu.com/people/wu.kan/activities"> <i class="fab fa-zhihu"></i> </a> <iframe src="https://music.163.com/outchain/player?type=0&id=155059595&auto=0&height=32" width=100% height=52 frameborder="no" border="0" marginwidth="0" marginheight="0" ></iframe></div></div></div><nav class="sidebar-nav"> <a class="sidebar-nav-item" href="/"> <i class="fas fa-home fa-fw"></i> 首页 </a> <a class="sidebar-nav-item" href="/comments/"> <i class="fas fa-comments fa-fw"></i> 留言 </a> <a class="sidebar-nav-item" href="/tags/"> <i class="fas fa-tags fa-fw"></i> 标签 </a> <a class="sidebar-nav-item" href="/archive/"> <i class="fas fa-archive fa-fw"></i> 归档 </a> <a class="sidebar-nav-item" href="/merger/"> <i class="fas fa-coffee fa-fw"></i> 打赏 </a></nav><div class="sidebar-item"><style> #sidebar-search-input { background: none; border: none; color: White; width: 100%; } #sidebar-search-results-container { overflow: auto auto; max-height: 66.6vh; }</style><input id="sidebar-search-input" placeholder="搜索博文" /><ol id="sidebar-search-results-container" ></ol><script src='https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.9.1' async='async' onload=' SimpleJekyllSearch({ json: "/assets/simple-jekyll-search/search.json", searchInput: document.getElementById("sidebar-search-input"), resultsContainer: document.getElementById("sidebar-search-results-container"), searchResultTemplate: `<li><a href="{url}">{title}</a>`, limit: 999, fuzzy: true })' ></script><style> #toc { overflow: auto auto; max-height: 66.6vh; }</style><ol id="toc"><li><a href="#%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D">方法介绍</a><ol><li><a href="#%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82graph-attention-layer">图注意力层（Graph Attention Layer）</a></ol><li><a href="#%E5%AE%9E%E9%AA%8C%E8%BF%87%E7%A8%8B">实验过程</a><ol><li><a href="#%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83">实验环境</a><li><a href="#%E5%AF%BC%E5%85%A5%E7%9B%B8%E5%85%B3%E5%8C%85%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86">导入相关包和数据集</a><li><a href="#%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82">图注意力层</a><li><a href="#%E6%A8%A1%E5%9E%8B">模型</a><li><a href="#%E8%AE%AD%E7%BB%83">训练</a><li><a href="#%E8%AF%84%E4%BC%B0%E6%95%88%E6%9E%9C">评估效果</a></ol><li><a href="#%E6%80%BB%E7%BB%93">总结</a><li><a href="#%E4%B8%BB%E8%A6%81%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">主要参考文献</a></ol><style> .sidebar-checkbox { display: none; } .sidebar-toggle { position: fixed; }</style><style> .sidebar { overflow: scroll; min-height: 101%; }</style><style> .effect { margin: 1rem; perspective: 900px; } .effect-info { text-align: center; position: absolute; top: 0; transform-style: preserve-3d; } .effect-img { z-index: 11; width: 100%; height: 100%; position: relative; transition: all 0.5s ease-in-out; } .effect-img:before { position: absolute; display: block; } .effect-right_to_left .effect-img { transform-origin: 0% 50%; } .effect-right_to_left:hover .effect-img { transform: rotate3d(0, 1, 0, -180deg); }</style><div> <i class="fas fa-cog fa-spin fa-fw"></i> <span id="run_time_day"> <i class="fas fa-spinner fa-pulse"></i> </span>天 <span id="run_time_hour"> <i class="fas fa-spinner fa-pulse"></i> </span>时 <span id="run_time_minute"> <i class="fas fa-spinner fa-pulse"></i> </span>分 <span id="run_time_second"> <i class="fas fa-spinner fa-pulse"></i> </span>秒 <script> setInterval(function (d,h,m,s,b) { function setzero(i) { return i < 10 ? "0" + i : i; } let BirthDay = new Date(b); let today = new Date(); let timeold = (today.getTime() - BirthDay.getTime()); let sectimeold = timeold / 1000; let secondsold = Math.floor(sectimeold); let msPerDay = 24 * 60 * 60 * 1000; let e_daysold = timeold / msPerDay; let daysold = Math.floor(e_daysold); let e_hrsold = (e_daysold - daysold) * 24; let hrsold = Math.floor(e_hrsold); let e_minsold = (e_hrsold - hrsold) * 60; let minsold = Math.floor((e_hrsold - hrsold) * 60); let seconds = Math.floor((e_minsold - minsold) * 60); d.textContent = daysold; h.textContent = setzero(hrsold); m.textContent = setzero(minsold); s.textContent = setzero(seconds); }, 1000, document.getElementById("run_time_day"), document.getElementById("run_time_hour"), document.getElementById("run_time_minute"), document.getElementById("run_time_second"), "10/04/2017 11:03:56")// 这是我第一篇CSDN博客的时间 </script></div><div><div> <i class="fas fa-eye fa-fw"></i> <span id="busuanzi_value_page_pv"> <i class="fas fa-spinner fa-pulse"></i> </span>次</div><div> <i class="fas fa-paw fa-fw"></i> <span id="busuanzi_value_site_pv"> <i class="fas fa-spinner fa-pulse"></i> </span>枚</div><div> <i class="fas fa-user-friends fa-fw"></i> <span id="busuanzi_value_site_uv"> <i class="fas fa-spinner fa-pulse"></i> </span>人</div><script src='https://cdn.jsdelivr.net/npm/busuanzi@2.3.0' async='async' ></script></div><div> <i class="fas fa-thumbs-up fa-fw"></i> <a href="https://jekyll-theme-WuK.wu-kan.cn"> jekyll-theme-WuK </a></div><div> <i class="fas fa-copyright fa-fw"></i> 2017-2021 WuK</div><div> <i class="fas fa-info-circle fa-fw"></i> <a href="http://beian.miit.gov.cn"> 粤ICP备 20024947号 </a></div><div> <img src="https://i.loli.net/2021/03/17/Y47tDZTrcy2xwRa.png" class="fa-fw"></img> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=34070202000407"> 皖公网安备 34070202000407号 </a></div></div></div><!-- Wrap is the content to shift when toggling the sidebar. We wrap the content to avoid any CSS collisions with our real content. --><div class="wrap"><style> @media only screen { pre { max-height: 66.6vh; overflow: auto; } }</style><style> .container { min-width: 66.6%; } @media only print { .container { min-width: 100%; } }</style><style> .container.content { padding: 2rem; box-shadow: 0 0 2rem rgba(255,255,255,0.9); background-color: rgba(255,255,255,0.9); animation-duration: 2s; animation-name: fadeIn; } @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }</style><div class="container content"><div class="page"><h1 class="page-title">图注意力网络</h1><div class="post"> <span class="post-date"> <i class="fas fa-calendar-day fa-fw"></i> 07 Jun 2020 <i class="fas fa-file-word fa-fw"></i> 9002字 <i class="fas fa-clock fa-fw"></i> 31分 <br/> <i class="fas fa-coffee fa-fw"></i> <a href="/merger/">如果这篇博客帮助到你，可以请我喝一杯咖啡~</a> <br/> <i class="fab fa-creative-commons-by fa-fw"></i> <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="license"> CC BY 4.0 </a> （除特别声明或转载文章外） </span><p>图卷积网络（Graph Convolutional Network, GCN）告诉我们将局部的图结构和节点特征结合可以在节点分类任务中获得不错的表现；图注意力网络（Graph Attention Network, GAT）则在 GCN 的基础之上引入了注意力机制，从而克服了先前图卷积网络的短板。下面三篇论文有递进关系，这篇笔记着重整理其中第二篇论文：</p><ol><li><a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a>，ICLR 2017，图卷积网络<li><a href="https://arxiv.org/abs/1710.10903">Graph Attention Network</a>，ICLR 2018，图注意力网络<li><a href="https://arxiv.org/abs/1904.05811">Relational Graph Attention Networks</a>，ICLR2019，关联性图注意力网络，整合了 GCN+Attention+Relational</ol><h2 id="方法介绍">方法介绍</h2><p>针对每一个节点运算相应的隐藏信息，在运算其相邻节点的时候引入注意力机制，有如下几个特性与优势：</p><ul><li>高效，与 GCN 的复杂度相当，并且可以并行运算。<li>针对有不同度数（degree）的节点，可以运用任意大小的权重与之对应。<li>可用于归纳学习（inductive learning），新来节点的时候，只需要考虑邻居的信息，对 GAT 的影响并不大（使用 GCN 时整个图矩阵就发生了变化，需要重新训练模型）</ul><p>在四个数据集（Cora、Citeseer、Pubmed、protein interaction）上达到 state of the art 的准确率。</p><h3 id="图注意力层graph-attention-layer">图注意力层（Graph Attention Layer）</h3><p>输入 $\mathbf{h}$ 为 $N$ 个节点的每个节点的 $F$ 个特征。</p>\[\mathbf{h}=\lbrace \vec{h_1},\vec{h_2},\dots,\vec{h_N}\rbrace ,\vec{h_i}\in \reals^F\]<p>输出 $\mathbf{h’}$ 为 $N$ 个节点的 $F’$ 个特征。</p>\[\mathbf{h'}=\lbrace \vec{h_1'},\vec{h_2'},\dots,\vec{h_N'}\rbrace ,\vec{h_i'}\in \reals^{F'}\]<p>为了得到相应的输入与输出的转换，需要根据输入的特征至少一次线性变换得到输出的特征，所以需要对所有节点训练一个权值矩阵 $\mathbf{W}\in\reals^{F\times F’}$，这个权值矩阵就是输入与输出的 $F$ 个特征与输出的 $F’$ 个特征之间的关系。</p><p>作者针对每个节点实行 self-attention 的注意力机制，机制为 $a:\reals^{F’}\times\reals^{F’}\to \reals$，注意力互相关系数（attention coefficients）为</p>\[e_{ij}=a(\mathbf{W}\vec{h_i},\mathbf{W}\vec{h_j})\]<p>表示节点 $j$ 对于节点 $i$ 的重要性（不考虑图结构性的信息）。</p><p>通过 masked attention（只计算节点 $i$ 相邻节点 $j\in \mathcal{N}_i$，其中 $\mathcal{N}_i$ 表示 $i$ 相邻节点构成的集合）将这个注意力机制引入图结构之中。为了使得互相关系数更容易计算和便于比较，引入了 $\text{softmax}$ 对所有的 $j\in \mathcal{N}_i$ 进行正则化：</p>\[\alpha_{ij}=\text{softmax}_j(e_{ij})=\frac{\exp(e_{ij})}{\sum_{k\in\mathcal{N}_i}\exp(e_{ik})}\]<p>实验之中，注意力机制 $a$ 是一个单层的前馈神经网络，通过权值向量来确定模型权重 $\vec{a}\in \reals^{2F’}$，并且加入了 $\text{LeakyRelu}$ 的非线性激活，小于零斜率为 0.2，大于 0 斜率为 0.1。于是需要求的注意力互相关系数的完整表达式如下：</p>\[\alpha_{ij}=\frac{ \exp( \text{LeakyRelu}(\vec{a}^T[\mathbf{W}\vec{h_i}\parallel\mathbf{W}\vec{h_j}]) ) }{ \sum_{k\in\mathcal{N}_i}\exp( \text{LeakyRelu}(\vec{a}^T[\mathbf{W}\vec{h_i}\parallel\mathbf{W}\vec{h_k}]) ) }\]<p>其中 $^T$ 代表转置，$\parallel$ 表示并列（concatenation）运算。</p><p><img src="https://cdn.jsdelivr.net/gh/dsgiitr/graph_nets/GAT/img/Attentional_Layer.jpg" alt="Attentional_Layer" /></p><p>公式含义就是权值矩阵与 $F’$ 个特征相乘，然后节点相乘后并列在一起，与权重相乘，$\text{LeakyRelu}$ 激活后指数操作得到 $\text{softmax}$ 的分子（分母同理）。</p><p>值得一提的是，这篇论文的早期版本里没有使用 $\text{LeakyRelu}$ 进行激活操作。对于对于每条边$(i,j)$，分别将节点 $i$ 和 $j$ 的特征变换为 1 个标量 $f(i)$ 和 $f(j)$ ，然后相加作为未归一化权重。然而后续进行 $\text{softmax}$ 时，传入函数的形式如下：</p>\[\frac{ \exp(f(i)+f(j)) }{ \sum_{k\in\mathcal{N}_i}\exp(f(i)+f(k)) }\]<p>约去 $\exp(f(i))$ 后得到</p>\[\frac{ \exp(f(j)) }{ \sum_{k\in\mathcal{N}_i}\exp(f(k)) }\]<p>这表明邻节点的重要性与当前节点无关，只与邻节点自身特征有关。这样忽视了节点本身，就容易导致矩阵在行上相似甚至相等，显然是有问题的。因此，我推测新版论文里加了激活函数可用于破坏上面的公式，使得 $i$ 不能约去。</p><p>通过上面，运算得到了正则化后的不同节点之间的注意力互相关系数（normalized attention coefficients），可以用来预测每个节点的输出特征：</p>\[\vec{h_i'}=\sigma(\sum_{j\in\mathcal{N}_i}\alpha_{ij}\mathbf{W}\vec{h_j})\]<p>这个公式的含义就是节点的输出特征是相邻的所有节点的线性和的非线性激活，线性和的系数是前面求得的注意力互相关系数。</p><p>在上面的输出特征加入计算 multi-head 的运算公式:</p>\[\vec{h_i'}=\parallel_{k=1}^K\sigma(\sum_{j\in \mathcal{N}_i}\alpha_{ij}^k\mathbf{W}^k\vec{h_j})\]<p>其中 concate 操作为 $\parallel$ ，$a_{ij}^k$ 是第 $k$ 个注意力机制 $(a_k)$ 算出的正则化注意力互相关系数，$\mathbf{W}^k$ 是对应的权重矩阵，共 $K$ 个注意力机制需要考虑，每个节点输出 $KF’$ 个特征。例如，$K=3$ 时结构如下，节点 1 在邻域中具有多端注意机制，不同的箭头样式表示独立的注意力计算，通过连接或平均每个 head 获取 $\vec{h_1’}$：</p><p><img src="https://cdn.jsdelivr.net/gh/dsgiitr/graph_nets/GAT/img/MultiHead_Attention.jpeg" alt="MultiHead_Attention" /></p><p>对于最终的输出，concate 操作可能不那么敏感了，所以作者直接用 K 平均来取代 concate 操作，得到最终的公式：</p>\[\vec{h_i'}=\sigma(\frac{1}{K}\sum_{k=1}^K\sum_{j\in\mathcal{N}_i}\alpha_{ij}^k\mathbf{W}^k\vec{h_j})\]<h2 id="实验过程">实验过程</h2><p>原论文中的实验分成两部分，半监督学习（transductive learning）和归纳学习（inductive learning）。限于文章篇幅，这里我只放出半监督学习的部分，归纳学习中对于论文中核心的图注意力层的原理是相同的，也可以查看文末参考链接部分原作者的代码。</p><h3 id="实验环境">实验环境</h3><p>使用的机器没有 cuda 环境，好在即使是在 cpu 上也能很快（1 分钟左右）训练完这个模型。</p><ul><li>Intel(R) Core(TM) i7-6567U CPU @3.30GHZ 3.31GHz<li>8.00GB RAM<li>Python 3.8.2 64-bit<ul><li>jupyter==1.0.0<li>numpy==1.18.4<li>torch==1.5.0+cpu<li>torch-scatter==2.0.4<li>torch-sparse==0.6.4<li>torch-cluster==1.5.4<li>torch-geometric==1.5.0</ul></ul><h3 id="导入相关包和数据集">导入相关包和数据集</h3><p>这里我没有选用原作者的代码（见参考部分），而是使用了 PyTorch Geometric 库，其优点是设计了一种新的表示图数据的存储结构，只会为存在的边进行计算，而完全不需要将计算浪费在不存在的边上。相对于原作者直接使用的邻接矩阵（浪费大量的时间为没有边的节点对计算权重，然而在第二步中被 Mask 消去），在我的机器上可以把训练模型的时间缩短到一分钟以内（Cora 数据集）。</p><pre><code class="language-python">import torch
from torch import nn
from torch_geometric.data import Data
from torch_geometric.datasets import Planetoid
from torch_geometric import transforms

torch.manual_seed(2020) # seed for reproducible numbers
name_data = 'Cora'
dataset = Planetoid(root= './dataset/' + name_data, name = name_data)
dataset.transform = transforms.NormalizeFeatures()

print(f"Number of Classes in {name_data}:", dataset.num_classes)
print(f"Number of Node Features in {name_data}:", dataset.num_node_features)
</code></pre><p>注意到下述代码会直接访问 <a href="https://github.com/kimiyoung/planetoid">kimiyoung/planetoid</a> 下载所需要的数据集到当前目录下，国内网络环境下有时需要挂上代理，否则会在运行时报“连接断开”的错误。</p><pre><code class="language-bash">Number of Classes in Cora: 7
Number of Node Features in Cora: 1433
</code></pre><h3 id="图注意力层">图注意力层</h3><p><code>torch_geometric</code>包已经实现了所需的图注意力图层，以下内容可以由一句 <code>from torch_geometric.nn import GATConv</code> 简单代替。但是作为对这篇论文本身的学习，还是有必要贴出对应的<a href="https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/gat_conv.py">源码</a>。</p><pre><code class="language-python"># https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/gat_conv.py
import torch
from torch.nn import Parameter, Linear
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.utils import remove_self_loops, add_self_loops, softmax

from torch_geometric.nn.inits import glorot, zeros

class GATConv(MessagePassing):
    def __init__(self, in_channels, out_channels, heads=1, concat=True,
                 negative_slope=0.2, dropout=0, bias=True, **kwargs):
        super(GATConv, self).__init__(aggr='add', **kwargs)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.heads = heads
        self.concat = concat
        self.negative_slope = negative_slope
        self.dropout = dropout

        self.__alpha__ = None

        self.lin = Linear(in_channels, heads * out_channels, bias=False)

        self.att_i = Parameter(torch.Tensor(1, heads, out_channels))
        self.att_j = Parameter(torch.Tensor(1, heads, out_channels))

        if bias and concat:
            self.bias = Parameter(torch.Tensor(heads * out_channels))
        elif bias and not concat:
            self.bias = Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('bias', None)

        self.reset_parameters()

    def reset_parameters(self):
        glorot(self.lin.weight)
        glorot(self.att_i)
        glorot(self.att_j)
        zeros(self.bias)

    def forward(self, x, edge_index, return_attention_weights=False):
        """"""

        if torch.is_tensor(x):
            x = self.lin(x)
            x = (x, x)
        else:
            x = (self.lin(x[0]), self.lin(x[1]))

        edge_index, _ = remove_self_loops(edge_index)
        edge_index, _ = add_self_loops(edge_index,
                                       num_nodes=x[1].size(self.node_dim))

        out = self.propagate(edge_index, x=x,
                             return_attention_weights=return_attention_weights)

        if self.concat:
            out = out.view(-1, self.heads * self.out_channels)
        else:
            out = out.mean(dim=1)

        if self.bias is not None:
            out = out + self.bias

        if return_attention_weights:
            alpha, self.__alpha__ = self.__alpha__, None
            return out, (edge_index, alpha)
        else:
            return out

    def message(self, x_i, x_j, edge_index_i, size_i,
                return_attention_weights):
        # Compute attention coefficients.
        x_i = x_i.view(-1, self.heads, self.out_channels)
        x_j = x_j.view(-1, self.heads, self.out_channels)

        alpha = (x_i * self.att_i).sum(-1) + (x_j * self.att_j).sum(-1)
        alpha = torch.nn.functional.leaky_relu(alpha, self.negative_slope)
        alpha = softmax(alpha, edge_index_i, size_i)

        if return_attention_weights:
            self.__alpha__ = alpha

        # Sample attention coefficients stochastically.
        alpha = torch.nn.functional.dropout(alpha, p=self.dropout, training=self.training)

        return x_j * alpha.view(-1, self.heads, 1)
</code></pre><p>我们只需要关注其 <code>forward</code> 过程中会调用的 <code>message</code> 方法：</p><pre><code class="language-python">alpha = (x_i * self.att_i).sum(-1) + (x_j * self.att_j).sum(-1)
alpha = torch.nn.functional.leaky_relu(alpha, self.negative_slope)
alpha = softmax(alpha, edge_index_i, size_i)

# Sample attention coefficients stochastically.
alpha = torch.nn.functional.dropout(alpha, p=self.dropout, training=self.training)

return x_j * alpha.view(-1, self.heads, 1)
</code></pre><p>这里算出了前面论文中提到的 $\alpha_{ij}$，然后进行 dropout 操作，相当于计算每个 node 位置的卷积时都是随机的选取了一部分近邻节点参与卷积，可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。最后返回 dropout 后卷积的结果。</p><h3 id="模型">模型</h3><ul><li>两层 GATLayer<li>第一层 8 head，$F’=8$，使用 elu（exponential linear unit）作为非线性函数<li>第二层为分类层，一个 attention head，特征数就是类别数，后面跟着 $\text{softmax}$ 函数<li>两层都采用 0.6 的 dropout</ul><pre><code class="language-python">class GAT(nn.Module):
    def __init__(self):
        super(GAT, self).__init__()
        self.hid = 8
        self.in_head = 8
        self.out_head = 1

        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)
        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False,
                             heads=self.out_head, dropout=0.6)


    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        # Dropout before the GAT layer is used to avoid overfitting in small datasets like Cora.
        # One can skip them if the dataset is sufficiently large.

        x = nn.functional.dropout(x, p=0.6, training=self.training)
        x = self.conv1(x, edge_index)
        x = nn.functional.elu(x)
        x = nn.functional.dropout(x, p=0.6, training=self.training)
        x = self.conv2(x, edge_index)

        return nn.functional.log_softmax(x, dim=1)
</code></pre><h3 id="训练">训练</h3><p>为了应对数据集小的问题，训练时使用了 <code>weight_decay=5e-4</code> 的 $L_2$ 正则化。</p><pre><code class="language-python">device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

data = dataset[0].to(device)
model = GAT().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)

model.train()
for epoch in range(1000):
    model.train()
    optimizer.zero_grad()
    out = model(data)
    loss = nn.functional.nll_loss(out[data.train_mask], data.y[data.train_mask])

    if epoch%200 == 0:
        print(loss)

    loss.backward()
    optimizer.step()
</code></pre><pre><code class="language-bash">tensor(1.9447, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.6848, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.6022, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.5589, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.4930, grad_fn=&lt;NllLossBackward&gt;)
</code></pre><h3 id="评估效果">评估效果</h3><pre><code class="language-python">model.eval()
_, pred = model(data).max(dim=1)
correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())
acc = correct / data.test_mask.sum().item()
print('Accuracy: {:.4f}'.format(acc))
</code></pre><pre><code class="language-bash">Accuracy: 0.8210
</code></pre><h2 id="总结">总结</h2><p>GAT 其实就是用注意力机制来计算聚合周边节点时的权重。受益于注意力机制，GAT 能够过滤噪音邻居，改进图卷积的缺点提升模型表现并可以对结果实现一定的解释。此外，GAT 不需要事先知道图结构，也可以应用在动态图上，无疑大大扩展了 GCN 的适用面。</p><h2 id="主要参考文献">主要参考文献</h2><ul><li><a href="https://arxiv.org/abs/1710.10903">Graph Attention Network</a><li><a href="https://github.com/Diego999/pyGAT">Diego999/pyGAT</a><li><a href="https://github.com/dsgiitr/graph_nets/blob/master/GAT/GAT.md">Understanding Graph Attention Networks (GAT)</a><li><a href="https://zhuanlan.zhihu.com/p/132497231">深入理解图注意力机制</a><li><a href="https://blog.csdn.net/weixin_36474809/article/details/89401552">图注意力网络(GAT) ICLR2018, Graph Attention Network 论文详解</a></ul></div><script repo="wu-kan/utterances-storage" src="https://utteranc.es/client.js" issue-term="url" theme="github-light" crossorigin="anonymous" async="async" ></script></div></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" /><style> .katex-display>.katex { white-space: normal; }</style><script src="https://cdn.jsdelivr.net/combine/npm/katex@0.13.11/dist/katex.min.js,npm/katex@0.13.11/dist/contrib/auto-render.min.js" async="async" onload='renderMathInElement(document.body, { delimiters: [{left: "$$", right: "$$", display: true}, { left: "$", right: "$", display: false }, {left: "\\(", right: "\\)", display: false}, {left: "\\[", right: "\\]", display: true}]})' ></script><style> pre.language-mermaid, code.language-mermaid { display: none; } @media only screen { .mermaid { overflow: auto auto; max-width: 100%; max-height: 66.6vh; } }</style><script src="https://cdn.jsdelivr.net/npm/mermaid@8.10.1/dist/mermaid.min.js" async="async" onload=' for(let x of document.getElementsByClassName("language-mermaid")) if(x.nodeName=="CODE") { let m = document.createElement("div"); m.classList.add("mermaid"); m.textContent = x.textContent; x.parentNode.insertAdjacentElement("beforebegin", m); }' ></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/combine/npm/prismjs@1.23.0/plugins/line-numbers/prism-line-numbers.min.css,npm/prismjs@1.23.0/plugins/toolbar/prism-toolbar.min.css,npm/prismjs@1.23.0/plugins/match-braces/prism-match-braces.min.css,npm/prism-themes@1.5.0/themes/prism-nord.min.css" /> <script src="https://cdn.jsdelivr.net/combine/npm/prismjs@1.23.0/components/prism-core.min.js,npm/prismjs@1.23.0/plugins/autoloader/prism-autoloader.min.js,npm/prismjs@1.23.0/plugins/line-numbers/prism-line-numbers.min.js,npm/prismjs@1.23.0/plugins/toolbar/prism-toolbar.min.js,npm/prismjs@1.23.0/plugins/match-braces/prism-match-braces.min.js" async="async" data-autoloader-path="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/components/" onload=' for(let x of document.getElementsByClassName("content")) x.classList.add("line-numbers","match-braces"); Prism.plugins.toolbar.registerButton("select-code", function (env) { let button = document.createElement("button"); button.textContent = "select this " + env.language; button.addEventListener("click", function () { if (document.body.createTextRange) { let range = document.body.createTextRange(); range.moveToElementText(env.element); range.select(); } else if (window.getSelection) { let selection = window.getSelection(); let range = document.createRange(); range.selectNodeContents(env.element); selection.removeAllRanges(); selection.addRange(range); } }); return button; })' ></script></div><label for="sidebar-checkbox" class="sidebar-toggle"></label>
